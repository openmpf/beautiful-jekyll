{
    "docs": [
        {
            "location": "/index.html", 
            "text": "NOTICE:\n This software (or technical data) was produced for the U.S. Government under contract, and is subject to the Rights in Data-General Clause 52.227-14, Alt. IV (DEC 2007). Copyright 2019 The MITRE Corporation. All Rights Reserved.\n\n\n\n\nOverview\n\n\nThere are numerous video and image exploitation capabilities available today. The Open Media Processing Framework (OpenMPF) provides a framework for chaining, combining, or replacing individual components for the purpose of experimentation and comparison.\n\n\nOpenMPF is a non-proprietary, scalable framework that permits practitioners and researchers to construct video, imagery, and audio exploitation capabilities using the available third-party components. Using OpenMPF, one can extract targeted entities in large-scale data environments, such as face and object detection.\n\n\nFor those developing new exploitation capabilities, OpenMPF exposes a set of Application Program Interfaces (APIs) for extending media analytics functionality. The APIs allow integrators to introduce new algorithms capable of detecting new targeted entity types. For example, a backpack detection algorithm could be integrated into an OpenMPF instance. OpenMPF does not restrict the number of algorithms that can operate on a given media file, permitting researchers, practitioners, and developers to explore arbitrarily complex composites of exploitation algorithms.\n\n\nA list of open source algorithms currently integrated into the OpenMPF as distributed processing components is shown here:\n\n\n\n\n\n\n\n\nOperation\n\n\nObject Type\n\n\nFramework\n\n\n\n\n\n\n\n\n\n\nDetection/Tracking\n\n\nFace\n\n\nLBP-Based OpenCV\n\n\n\n\n\n\nDetection/Tracking\n\n\nFace\n\n\nDlib\n\n\n\n\n\n\nDetection/Tracking\n\n\nPerson\n\n\nHOG-Based OpenCV\n\n\n\n\n\n\nDetection/Tracking\n\n\nMotion\n\n\nMOG w/ STRUCK\n\n\n\n\n\n\nDetection/Tracking\n\n\nMotion\n\n\nSuBSENSE w/ STRUCK\n\n\n\n\n\n\nDetection/Tracking\n\n\nLicense Plate\n\n\nOpenALPR\n\n\n\n\n\n\nDetection\n\n\nSpeech\n\n\nSphinx\n\n\n\n\n\n\nDetection\n\n\nScene\n\n\nOpenCV\n\n\n\n\n\n\nDetection\n\n\nClassification\n\n\nOpenCV DNN\n\n\n\n\n\n\nDetection/Tracking\n\n\nClassification\n\n\nDarknet\n\n\n\n\n\n\nDetection\n\n\nText Region\n\n\nEAST\n\n\n\n\n\n\nDetection\n\n\nText (OCR)\n\n\nApache Tika\n\n\n\n\n\n\nDetection\n\n\nText (OCR)\n\n\nTesseract OCR\n\n\n\n\n\n\nDetection\n\n\nImage (from document)\n\n\nApache Tika\n\n\n\n\n\n\n\n\nThe OpenMPF exposes data processing and job management web services via a User Interface (UI). These services allow users to upload media, create media processing jobs, determine the status of jobs, and retrieve the artifacts associated with completed jobs. The web services give application developers flexibility to use the OpenMPF in their preferred environment and programming language.", 
            "title": "Home"
        }, 
        {
            "location": "/index.html#overview", 
            "text": "There are numerous video and image exploitation capabilities available today. The Open Media Processing Framework (OpenMPF) provides a framework for chaining, combining, or replacing individual components for the purpose of experimentation and comparison.  OpenMPF is a non-proprietary, scalable framework that permits practitioners and researchers to construct video, imagery, and audio exploitation capabilities using the available third-party components. Using OpenMPF, one can extract targeted entities in large-scale data environments, such as face and object detection.  For those developing new exploitation capabilities, OpenMPF exposes a set of Application Program Interfaces (APIs) for extending media analytics functionality. The APIs allow integrators to introduce new algorithms capable of detecting new targeted entity types. For example, a backpack detection algorithm could be integrated into an OpenMPF instance. OpenMPF does not restrict the number of algorithms that can operate on a given media file, permitting researchers, practitioners, and developers to explore arbitrarily complex composites of exploitation algorithms.  A list of open source algorithms currently integrated into the OpenMPF as distributed processing components is shown here:     Operation  Object Type  Framework      Detection/Tracking  Face  LBP-Based OpenCV    Detection/Tracking  Face  Dlib    Detection/Tracking  Person  HOG-Based OpenCV    Detection/Tracking  Motion  MOG w/ STRUCK    Detection/Tracking  Motion  SuBSENSE w/ STRUCK    Detection/Tracking  License Plate  OpenALPR    Detection  Speech  Sphinx    Detection  Scene  OpenCV    Detection  Classification  OpenCV DNN    Detection/Tracking  Classification  Darknet    Detection  Text Region  EAST    Detection  Text (OCR)  Apache Tika    Detection  Text (OCR)  Tesseract OCR    Detection  Image (from document)  Apache Tika     The OpenMPF exposes data processing and job management web services via a User Interface (UI). These services allow users to upload media, create media processing jobs, determine the status of jobs, and retrieve the artifacts associated with completed jobs. The web services give application developers flexibility to use the OpenMPF in their preferred environment and programming language.", 
            "title": "Overview"
        }, 
        {
            "location": "/Release-Notes/index.html", 
            "text": "NOTICE:\n This software (or technical data) was produced for the U.S. Government under contract, and is subject to the Rights in Data-General Clause 52.227-14, Alt. IV (DEC 2007). Copyright 2019 The MITRE Corporation. All Rights Reserved.\n\n\n\n\nOpenMPF 4.1.0: July 2019\n\n\nDocumentation\n\n\n\n\n\nUpdated the \nC++ Batch Component API\n to describe the ROTATION detection property. See the \nArbitrary Rotation\n section below.\n\n\nUpdated the \nREST API\n with new component registration REST endpoints. See the \nComponent Registration REST Endpoints\n section below.\n\n\nAdded a \nREADME\n for the EAST text region detection component. See the \nEAST Text Region Detection Component\n section below.\n\n\nUpdated the Tesseract OCR text detection component \nREADME\n. See the  \nTesseract OCR Text Detection Component\n section below.\n\n\nUpdated the openmpf-docker repo \nREADME\n and \nSWARM\n guide to describe the new streamlined approach to using \ndocker-compose config\n. See the \nDocker Deployment\n section below.\n\n\nFixed the description of MIN_SEGMENT_LENGTH and associated examples in the \nUser Guide\n for issue \n#891\n.\n\n\nUpdated the \nJava Batch Component API\n with information on how to use Log4j2. Related to resolving issue \n#855\n.\n\n\nUpdated the \nInstall Guide\n to point to the Docker \nREADME\n.\n\n\nTransformed the Build Guide into a \nDevelopment Environment Guide\n.\n\n\n\n\n\n\nArbitrary Rotation\n\n\n\n\nThe C++ MPFVideoCapture and MPFImageReader tools now support ROTATION values other than 0, 90, 180, and 270 degrees. Users can now specify a clockwise ROTATION job property in the range [0, 360). Values outside that range will be normalized to that range. Floating point values are accepted.\n\n\nWhen using those tools to read frame data, they will automatically correct for rotation so that the returned frame is horizontally oriented toward the normal 3 o'clock position.\n\n\nWhen FEED_FORWARD_TYPE=REGION, these tools will look for a ROTATION detection property in the feed-forward detections and automatically correct for rotation. For example, a detection property of ROTATION=90 represents that the region is rotated 90 degrees counter clockwise, and therefore must be rotated 90 degrees clockwise to correct for it.\n\n\nWhen FEED_FORWARD_TYPE=SUPERSET_REGION, these tools will properly account for the ROTATION detection property associated with each feed-forward detection when calculating the bounding box that encapsulates all of those regions.\n\n\nWhen FEED_FORWARD_TYPE=FRAME, these tools will rotate the frame according to the ROTATION job property. It's important to note that for rotations other than 0, 90, 180, and 270 degrees the rotated frame dimensions will be larger than the original frame dimensions. This is because the  frame needs to be expanded to encapsulate the entirety of the original rotated frame region. Black pixels are used to fill the empty space near the edges of the original frame.\n\n\n\n\n\n\nThe Markup component now places a colored dot at the upper-left corner of each detection region so that users can determine the rotation of the region relative to the entire frame.\n\n\n\n\n\n\nComponent Registration REST Endpoints\n\n\n\n\nAdded a \n[POST] /rest/components/registerUnmanaged\n endpoint so that components running as separate Docker containers can self-register with the Workflow Manager.\n\n\nSince these components are not managed by the Node Manager, they are considered unmanaged OpenMPF components. These components are not displayed in Nodes web UI and are tagged as unmanaged in the Component Registration web UI where they can only be removed.\n\n\nNote that components uploaded to the Component Registration web UI as .tar.gz files are considered managed components.\n\n\n\n\n\n\nAdded a \n[DELETE] /rest/components/{componentName}\n endpoint that can be used to remove managed and unmanaged components.\n\n\n\n\nPython Component Executor Docker Image\n\n\n\n\n\nComponent developers can now use a Python component executor Docker image to write a Python component for OpenMPF that can be encapsulated\nwithin a Docker container. This isolates the build and execution environment from the rest of OpenMPF. For more information, see the \nREADME\n.\n\n\nComponents developed with this image are not managed by the Node Manager; rather, they self-register with the Workflow Manager and their lifetime is determined by their own Docker container.\n\n\n\n\n\n\nDocker Deployment\n\n\n\n\nStreamlined single-host \ndocker-compose up\n deployments and multi-host \ndocker stack deploy\n swarm deployments. Now users are instructed to create a single \ndocker-compose.yml\n file for both types of deployments.\n\n\nRemoved the \ndocker-generate-compose-files.sh\n script in favor of allowing users the flexibility of combining multiple \ndocker-compose.*.yml\n files together using \ndocker-compose config\n. See the \nGenerate docker-compose.yml\n section of the README.\n\n\nComponents based on the Python component executor Docker image can now be defined and configured directly in \ndocker-compose.yml\n.\n\n\nOpenMPF Docker images now make use of Docker labels.\n\n\n\n\n\n\nEAST Text Region Detection Component\n\n\n\n\nThis new component uses the Efficient and Accurate Scene Text (EAST) detection model to detect text regions in images and videos. It reports their location, angle of rotation, and text type (STRUCTURED or UNSTRUCTURED), and supports a variety of settings to control the behavior of merging text regions into larger regions. It does not perform OCR on the text or track detections across video frames. Thus, each video track is at most one detection long. For more information, see the \nREADME\n.\n\n\nOptionally, this component can be built as a Docker image using the Python component executor Docker image, allowing it to exist apart from the Node Manager image.\n\n\n\n\n\n\nTesseract OCR Text Detection Component\n\n\n\n\nUpdated to support reading tessdata \n*.traineddata\n files at a specified MODELS_DIR_PATH. This allows users to install new \n*.traineddata\n files post deployment.\n\n\nUpdated to optionally perform Tesseract Orientation and Script Detection (OSD). When enabled, the component will attempt to use the orientation results of OSD to automatically rotate the image, as well as perform OCR using the scripts detected by OSD.\n\n\nUpdated to optionally rotate a feed-forward text region 180 degrees to account for upside-down text.\n\n\nNow supports the following preprocessing properties for both structured and unstructured text:\n\n\nText sharpening\n\n\nText rescaling\n\n\nOtsu image thresholding\n\n\nAdaptive thresholding\n\n\nHistogram equalization\n\n\nAdaptive histogram equalization (also known as Contrast Limited Adaptive Histogram Equalization (CLAHE))\n\n\n\n\n\n\nWill use the TEXT_TYPE detection property in feed-forward regions provided by the EAST component to determine which preprocessing steps to perform.\n\n\nFor more information on these new features, see the \nREADME\n.\n\n\nRemoved gibberish and string filters since they only worked on English text.\n\n\n\n\nActiveMQ Profiles\n\n\n\n\n\nThe ActiveMQ Docker image now supports custom profiles. The container selects an \nactivemq.xml\n and \nenv\n file to use at runtime based on the value of the \nACTIVE_MQ_PROFILE\n environment variable. Among others, these files contain configuration settings for Java heap space and component queue memory limits.\n\n\nThis release only supports a \ndefault\n profile setting, as defined by \nactivemq-default.xml\n and \nenv.default\n; however, developers are free to add other \nactivemq-\nprofile\n.xml\n and \nenv.\nprofile\n files to the ActiveMQ Docker image to suit their needs.\n\n\n\n\nDisabled ActiveMQ Prefetch\n\n\n\n\n\nDisabled ActiveMQ prefetching on all component queues. Previously, a prefetch value of one was resulting in situations where one component service could be dispatched two sub-jobs, thereby starving other available component services which could process one of those sub-jobs in parallel.\n\n\n\n\nSearch Region Percentages\n\n\n\n\n\nIn addition to using exact pixel values, users can now use percentages for the following properties when specifying search regions for C++ and Python components:\n\n\nSEARCH_REGION_TOP_LEFT_X_DETECTION\n\n\nSEARCH_REGION_TOP_LEFT_Y_DETECTION\n\n\nSEARCH_REGION_BOTTOM_RIGHT_X_DETECTION\n\n\nSEARCH_REGION_BOTTOM_RIGHT_Y_DETECTION\n\n\n\n\n\n\nFor example, setting SEARCH_REGION_TOP_LEFT_X_DETECTION=50% will result in components only processing the right half of an image or video.\n\n\nOptionally, users can specify exact pixel values of some of these properties and percentages for others.\n\n\n\n\nOther Improvements\n\n\n\n\n\nIncreased the number of ActiveMQ maxConcurrentConsumers for the MPF.COMPLETED_DETECTIONS queue from 30 to 60.\n\n\nThe Create Job web UI now only displays the content of the \n$MPF_HOME/share/remote-media\n directory instead of all of \n$MPF_HOME/share\n, which prevents the Workflow Manager from indexing generated JSON output files, artifacts, and markup. Indexing the latter resulted in Java heap space issues for large scale production systems. This is a mitigation for issue \n#897\n.\n\n\nThe Job Status web UI now makes proper use of pagination in SQL/Hibernate through the Workflow Manager to avoid retrieving the entire jobs table, which was inefficient.\n\n\nThe Workflow Manager will now silently discard all duplicate messages in the ActiveMQ Dead Letter Queue (DLQ), regardless of destination. Previously, only messages destined for component sub-job request queues were discarded.\n\n\n\n\nBug Fixes\n\n\n\n\n\n[\n#891\n] Fixed a bug where the Workflow Manager media segmenter generated short segments that were minimally MIN_SEGMENT_LENGTH+1 in size instead of MIN_SEGMENT_LENGTH.\n\n\n[\n#745\n] In environments where thousands of jobs are processed, users have observed that, on occasion, pending sub-job messages in ActiveMQ queues are not processed until a new job is created. This seems to have been resolved by disabling ActiveMQ prefetch behavior on component queues.\n\n\n[\n#855\n] A logback circular reference suppressed exception no longer throws a StackOverflowError. This was resolved by transitioning the Workflow Manager and Java components from the Logback framework to Log4j2.\n\n\n\n\nKnown Issues\n\n\n\n\n\n[\n#897\n] OpenMPF will attempt to index files located in \n$MPF_HOME/share\n as soon as the webapp is started by Tomcat. This is so that those files can be listed in a directory tree in the Create Job web UI. The main problem is that once a file gets indexed it's never removed from the cache, even if the file is manually deleted, resulting in a memory leak.\n\n\n\n\nOpenMPF 4.0.0: February 2019\n\n\nDocumentation\n\n\n\n\n\nAdded an \nObject Storage Guide\n with information on how to configure OpenMPF to work with a custom NGINX object storage server, and how to run jobs that use an S3 object storage server. Note that the system properties for the custom NGINX object storage server have changed since the last release.\n\n\n\n\nUpgrade to Tesseract 4.0\n\n\n\n\n\nBoth the Tesseract OCR Text Detection Component and OpenALPR License Plate Detection Components have been updated to use the new version of Tesseract.\n\n\nAdditionally, Leptonica has been upgraded from 1.72 to 1.75.\n\n\n\n\nDocker Deployment\n\n\n\n\n\nThe Docker images now use the yum package manager to install ImageMagick6 from a public RPM repository instead of downloading the RPMs directly from imagemagick.org. This resolves an issue with the OpenMPF Docker build where RPMs on \nimagemagick.org\n were no longer available.\n\n\n\n\nTesseract OCR Text Detection Component\n\n\n\n\n\nUpdated to allow the user to set a TESSERACT_OEM property in order to select an OCR engine mode (OEM).\n\n\n\"script/Latin\" can now be specified as the TESSERACT_LANGUAGE. When selected, Tesseract will select all Latin characters, which can be from different Latin languages.\n\n\n\n\nCeph S3 Object Storage\n\n\n\n\n\nAdded support for downloading files from, and uploading files to, an S3 object storage server. The following job properties can be provided: S3_ACCESS_KEY, S3_SECRET_KEY, S3_RESULTS_BUCKET, S3_UPLOAD_ONLY.\n\n\nAt this time, only support for Ceph object storage has been tested. However, the Workflow Manager uses the AWS SDK for Java to communicate with the object store, so it is possible that other S3-compatible storage solutions may work as well.\n\n\n\n\nISO-8601 Timestamps\n\n\n\n\n\nAll timestamps in the JSON output object, and streaming video callbacks, are now in the ISO-8601 format (e.g. \"2018-12-19T12:12:59.995-05:00\"). This new format includes the time zone, which makes it possible to compare timestamps generated between systems in different time zones.\n\n\nThis change does not affect the track and detection start and stop offset times, which are still reported in milliseconds since the start of the video.\n\n\n\n\nReduced Redis Usage\n\n\n\n\n\nThe Workflow Manager has been refactored to reduce usage of the Redis in-memory database. In general, Redis is not necessary for storing job information and only resulted in introducing potential delays in accessing that data over the network stack.\n\n\nNow, only track and detection data is stored in Redis for batch jobs. This reduces the amount of memory the Workflow Manager requires of the Java Virtual Machine. Compared to the other job information, track and detection data can potentially be relatively much larger. In the future, we plan to store frame data in Redis for streaming jobs as well.\n\n\n\n\nCaffe Vehicle Color Estimation\n\n\n\n\n\nThe Caffe Component \nmodels.ini\n file has been updated with a \"vehicle_color\" section with links for downloading the \nReza Fuad Rachmadi's Vehicle Color Recognition Using Convolutional Neural Network\n model files.\n\n\nThe following pipelines have been added. These require the above model files to be placed in \n$MPF_HOME/share/models/CaffeDetection\n:\n\n\nCAFFE REZAFUAD VEHICLE COLOR DETECTION PIPELINE,\n\n\nCAFFE REZAFUAD VEHICLE COLOR DETECTION (WITH FF REGION FROM TINY YOLO VEHICLE DETECTOR) PIPELINE\n\n\nCAFFE REZAFUAD VEHICLE COLOR DETECTION (WITH FF REGION FROM YOLO VEHICLE DETECTOR) PIPELINE\n\n\n\n\n\n\n\n\nTrack Merging and Minimum Track Length\n\n\n\n\n\nThe following system properties now have \"video\" in their names:\n\n\ndetection.video.track.merging.enabled\n\n\ndetection.video.track.min.gap\n\n\ndetection.video.track.min.length\n\n\ndetection.video.track.overlap.threshold\n\n\n\n\n\n\nThe above properties can be overridden by the following job properties, respectively. These have not been renamed since the last release:\n\n\nMERGE_TRACKS\n\n\nMIN_GAP_BETWEEN_TRACKS\n\n\nMIN_TRACK_LENGTH\n\n\nMIN_OVERLAP\n\n\n\n\n\n\nThese system and job properties now only apply to video media. This resolves an issue where users had set \ndetection.track.min.length=5\n, which resulted in dropping all image media tracks. By design, each image track can only contain a single detection.\n\n\n\n\nBug Fixes\n\n\n\n\n\nFixed a bug where the Docker entrypoint scripts appended properties to the end of \n$MPF_HOME/share/config/mpf-custom.properties\n every time the Docker deployment was restarted, resulting in entries like \ndetection.segment.target.length=5000,5000,5000\n.\n\n\nUpgrading to Tesseract 4 fixes a bug where, when specifying \nTESSERACT_LANGUAGE\n, if one of the languages is Arabic, then Arabic must be specified last. Arabic can now be specified first, for example: \nara+eng\n.\n\n\nFixed a bug where the minimum track length property was being applied to image tracks. Now it's only applied to video tracks.\n\n\nFixed a bug where ImageMagick6 installation failed while building Docker images.\n\n\n\n\nOpenMPF 3.0.0: December 2018\n\n\n\n\nNOTE:\n The \nBuild Guide\n and \nInstall Guide\n are outdated. The old process for manually configuring a Build VM, using it to build an OpenMPF package, and installing that package, is deprecated in favor of Docker containers. Please refer to the openmpf-docker \nREADME\n.\n\n\nNOTE:\n Do not attempt to register or unregister a component through the Nodes UI in a Docker deployment. It may appear to succeed, but the changes will not affect the child Node Manager containers, only the Workflow Manager container. Also, do not attempt to use the \nmpf\n command line tools in a Docker deployment.\n\n\n\n\nDocumentation\n\n\n\n\n\nAdded a \nREADME\n, \nSWARM\n guide, and \nCONTRIBUTING\n guide for Docker deployment.\n\n\nUpdated the \nUser Guide\n with information on how track properties and track confidence are handled when merging tracks.\n\n\nAdded README files for new components. Refer to the component sections below.\n\n\n\n\nDocker Support\n\n\n\n\n\nOpenMPF can now be built and distributed as 5 Docker images: openmpf_workflow_manager, openmpf_node_manager, openmpf_active_mq, mysql_database, and redis.\n\n\nThese images can be deployed on a single host using \ndocker-compose up\n.\n\n\nThey can also be deployed across multiple hosts in a Docker swarm cluster using \ndocker stack deploy\n.\n\n\nGPU support is enabled through the NVIDIA Docker runtime.\n\n\nBoth HTTP and HTTPS deployments are supported.\n\n\n\n\n\n\nJSON Output Object\n\n\n\n\nAdded a \ntrackProperties\n field at the track level that works in much the same way as the \ndetectionProperties\n field at the detection level. Both are maps that contain zero or more key-value pairs. The component APIs have always supported the ability to return track-level properties, but they were never represented in the JSON output object, until now.\n\n\nSimilarly, added a track \nconfidence\n field. The component APIs always supported setting it, but the value was never used in the JSON output object, until now.\n\n\nAdded \njobErrors\n and\njobWarnings\n fields. The \njobErrors\n field will mention that there are items in \ndetectionProcessingErrors\n fields.\n\n\nThe \noffset\n, \nstartOffset\n, and \nstopOffset\n fields have been removed in favor of the existing \noffsetFrame\n, \nstartOffsetFrame\n, and \nstopOffsetFrame\n fields, respectively. They were redundant and deprecated.\n\n\nAdded a \nmpf.output.objects.exemplars.only\n system property, and \nOUTPUT_EXEMPLARS_ONLY\n job property, that can be set to reduce the size of the JSON output object by only recording the track exemplars instead of all of the detections in each track.\n\n\nAdded a \nmpf.output.objects.last.stage.only\n system property, and \nOUTPUT_LAST_STAGE_ONLY\n job property, that can be set to reduce the size of the JSON output object by only recording the detections for the last non-markup stage of a pipeline.\n\n\n\n\nDarknet Component\n\n\n\n\n\nThe Darknet component can now support processing streaming video.\n\n\nIn batch mode, video frames are prefetched, decoded, and stored in a buffer using a separate thread from the one that performs the detection. The size of the prefetch buffer can be configured by setting \nFRAME_QUEUE_CAPACITY\n.\n\n\nThe Darknet component can now perform basic tracking and generate video tracks with multiple detections. Both the default detection mode and preprocessor detection mode are supported.\n\n\nThe Darknet component has been updated to support the full and tiny YOLOv3 models. The YOLOv2 models are no longer supported.\n\n\n\n\nTesseract OCR Text Detection Component\n\n\n\n\n\nThis new component extracts text found in an image and reports it as a single-detection track.\n\n\nPDF documents can also be processed with one track detection per page.\n\n\nUsers may set the language of each track using the \nTESSERACT_LANGUAGE\n property as well as adjust other image preprocessing properties for text extraction.\n\n\nRefer to the \nREADME\n.\n\n\n\n\nOpenCV Scene Change Detection Component\n\n\n\n\n\nThis new component detects and segments a given video by scenes. Each scene change is detected using histogram comparison, edge comparison, brightness (fade outs), and overall hue/saturation/value differences between adjacent frames.\n\n\nUsers can toggle each type of of scene change detection technique as well as threshold properties for each detection method.\n\n\nRefer to the \nREADME\n.\n\n\n\n\nTika Text Detection Component\n\n\n\n\n\nThis new component extracts text contained in documents and performs language detection. 71 languages and most document formats (.txt, .pptx, .docx, .doc, .pdf, etc.) are supported.\n\n\nRefer to the \nREADME\n.\n\n\n\n\nTika Image Detection Component\n\n\n\n\n\nThis new component extracts images embedded in document formats (.pdf, .ppt, .doc) and stores them on disk in a specified directory.\n\n\nRefer to the \nREADME\n.\n\n\n\n\nTrack-Level Properties and Confidence\n\n\n\n\n\nRefer to the addition of track-level properties and confidence in the \nJSON Output Object\n section.\n\n\nComponents have been updated to return meaningful track-level properties. Caffe and Darknet include \nCLASSIFICATION\n, OALPR includes the exemplar \nTEXT\n, and Sphinx includes the \nTRANSCRIPTION\n.\n\n\nThe Workflow Manager will now populate the track-level confidence. It is the same as the exemplar confidence, which is the max of all of the track detections.\n\n\n\n\nCustom NGINX HTTP Object Storage\n\n\n\n\n\nAdded \nhttp.object.storage.*\n system properties for configuring an optional custom NGINX object storage server on which to store generated detection artifacts, JSON output objects, and markup files.\n\n\nWhen a file cannot be uploaded to the server, the Workflow Manager will fall back to storing it in \n$MPF_HOME/share\n, which is the default behavior when an object storage server is not specified.\n\n\nIf and when a failure occurs, the JSON output object will contain a descriptive message in the \njobWarnings\n field, and, if appropriate, the \nmarkupResult.message\n field. If the job completes without other issues, the final status will be \nCOMPLETE_WITH_WARNINGS\n.\n\n\nThe NGINX storage server runs custom server-side code which we can make available upon request. In the future, we plan to support more common storage server solutions, such as Amazon S3.\n\n\n\n\n\n\nActiveMQ\n\n\n\n\nThe \nMPF_OUTPUT\n queue is no longer supported and has been removed. Job producers can specify a callback URL when creating a job so that they are alerted when the job is complete. Users observed heap space issues with ActiveMQ after running thousands of jobs without consuming messages from the \nMPF_OUTPUT\n queue.\n\n\nThe Workflow Manager will now silently discard duplicate sub-job request messages in the ActiveMQ Dead Letter Queue (DLQ). This fixes a bug where the Workflow Manager would prematurely terminate jobs corresponding to the duplicate messages. It's assumed that ActiveMQ will only place a duplicate message in the DLQ if the original message, or another duplicate, can be delivered.\n\n\n\n\nNode Auto-Configuration\n\n\n\n\n\nAdded the \nnode.auto.config.enabled\n, \nnode.auto.unconfig.enabled\n, and \nnode.auto.config.num.services.per.component\n system properties for automatically managing the configuration of services when nodes join and leave the OpenMPF cluster.\n\n\nDocker will assign a a hostname with a randomly-generated id to containers in a swarm deployment. The above properties allow the Workflow Manager to automatically discover and configure services on child Node Manager components, which is convenient since the hostname of those containers cannot be known in advance, and new containers with new hostnames are created when the swarm is restarted.\n\n\n\n\nJob Status Web UI\n\n\n\n\n\nAdded the \nweb.broadcast.job.status.enabled\n and \nweb.job.polling.interval\n system properties that can be used to configure if the Workflow Manager automatically broadcasts updates to the Job Status web UI. By default, the broadcasts are enabled.\n\n\nIn a production environment that processes hundreds of jobs or more at the same time, this behavior can result in overloading the web UI, causing it to slow down and freeze up. To prevent this, set \nweb.broadcast.job.status.enabled\n to \nfalse\n. If \nweb.job.polling.interval\n is set to a non-zero value, the web UI will poll for updates at that interval (specified in milliseconds).\n\n\nTo disable broadcasts and polling, set \nweb.broadcast.job.status.enabled\n to \nfalse\n and \nweb.job.polling.interval\n to a zero or negative value. Users will then need to manually refresh the Job Status web page using their web browser.\n\n\n\n\nOther Improvements\n\n\n\n\n\nNow using variable-length text fields in the mySQL database for string data that may exceed 255 characters.\n\n\nUpdated the \nMPFImageReader\n tool to use OpenCV video capture behind the scenes to support reading data from HTTP URLs.\n\n\nPython components can now include pre-built wheel files in the plugin package.\n\n\nWe now use a \nJenkinsfile\n Groovy script for our Jenkins build process. This allows us to use revision control for our continuous integration process and share that process with the open source community.\n\n\nAdded \nremote.media.download.retries\n and \nremote.media.download.sleep\n system properties that can be used to configure how the Workflow Manager will attempt to retry downloading remote media if it encounters a problem.\n\n\nArtifact extraction now uses MPFVideoCapture, which employs various fallback strategies for extracting frames in cases where a video is not well-formed or corrupted. For components that use MPFVideoCapture, this enables better consistency between the frames they process and the artifacts that are later extracted.\n\n\n\n\nBug Fixes\n\n\n\n\n\nJobs now properly end in \nERROR\n if an invalid media URL is provided or there is a problem accessing remote media.\n\n\nJobs now end in \nCOMPLETE_WITH_ERRORS\n when a detection splitter error occurs due to missing system properties.\n\n\nComponents can now include their own version of the Google Protobuf library. It will not conflict with the version used by the rest of OpenMPF.\n\n\nThe Java component executor now sets the proper job id in the job name instead of using the ActiveMQ message request id.\n\n\nThe Java component executor now sets the run directory using \nsetRunDirectory()\n.\n\n\nActions can now be properly added using an \"extras\" component. An extras component only includes a \ndescriptor.json\n file and declares Actions, Tasks, and Pipelines using other component algorithms.\n\n\nRefer to the items listed in the \nActiveMQ\n section.\n\n\nRefer to the addition of track-level properties and confidence in the \nJSON Output Object\n section.\n\n\n\n\nKnown Issues\n\n\n\n\n\n[\n#745\n] In environments where thousands of jobs are processed, users have observed that, on occasion, pending sub-job messages in ActiveMQ queues are not processed until a new job is created. The reason is currently unknown.\n\n\n[\n#544\n] Image artifacts retain some permissions from source files available on the local host. This can result in some of the image artifacts having executable permissions.\n\n\n[\n#604\n] The Sphinx component cannot be unregistered because \n$MPF_HOME/plugins/SphinxSpeechDetection/lib\n is owned by root on a deployment machine.\n\n\n[\n#623\n] The Nodes UI does not work correctly when \n[POST] /rest/nodes/config\n is used at the same time. This is because the UI's state is not automatically updated to reflect changes made through the REST endpoint.\n\n\n[\n#783\n] The Tesseract OCR Text Detection Component has a \nknown issue\n because it uses Tesseract 3. If a combination of languages is specified using \nTESSERACT_LANGUAGE\n, and one of the languages is Arabic, then Arabic must be specified last. For example, for English and Arabic, \neng+ara\n will work, but \nara+eng\n will not.\n\n\n[\n#784\n] Sometimes services do not start on OpenMPF nodes, and those services cannot be started through the Nodes web UI. This is not a Docker-specific problem, but it has been observed in a Docker swarm deployment when auto-configuration is enabled. The workaround is to restart the Docker swarm deployment, or remove the entire node in the Nodes UI and add it again.\n\n\n\n\nOpenMPF 2.1.0: June 2018\n\n\n\n\nNOTE:\n If building this release on a machine used to build a previous version of OpenMPF, then please run \nsudo pip install --upgrade pip\n to update to at least pip 10.0.1. If not, the OpenMPF build script will fail to properly download .whl files for Python modules.\n\n\n\n\nDocumentation\n\n\n\n\n\nAdded the \nPython Batch Component API\n.\n\n\nAdded the \nNode Guide\n.\n\n\nAdded the \nGPU Support Guide\n.\n\n\nUpdated the \nInstall Guide\n with an \"(Optional) Install the NVIDIA CUDA Toolkit\" section.\n\n\nRenamed Admin Manual to Admin Guide for consistency.\n\n\n\n\nPython Batch Component API\n\n\n\n\n\nDevelopers can now write batch components in Python using the mpf_component_api module.\n\n\nDependencies can be specified in a setup.py file. OpenMPF will automatically download the .whl files using pip at build time.\n\n\nWhen deployed, a virtualenv is created for the Python component so that it runs in a sandbox isolated from the rest of the system.\n\n\nOpenMPF ImageReader and VideoCapture tools are provided in the mpf_component_util module.\n\n\nExample Python components are provided for reference.\n\n\n\n\nSpare Nodes\n\n\n\n\n\nSpare nodes can join and leave an OpenMPF cluster while the Workflow Manager is running. You can create a spare node by cloning an existing OpenMPF child node. Refer to the \nNode Guide\n.\n\n\nNote that changes made using the Component Registration web page only affect core nodes, not spare nodes. Core nodes are those configured during the OpenMPF installation process.\n\n\nAdded \nmpf list-nodes\n command to list the core nodes and available spare nodes.\n\n\nOpenMPF now uses the JGroups FILE_PING protocol for peer discovery instead of TCPPING. This means that the list of OpenMPF nodes no longer needs to be fully specified when the Workflow Manager starts. Instead, the Workflow Manager, and Node Manager process on each node, use the files in \n$MPF_HOME/share/nodes\n to determine which nodes are currently available.\n\n\nUpdated JGroups from 3.6.4. to 4.0.11.\n\n\nThe environment variables specified in \n/etc/profile.d/mpf.sh\n have been simplified. Of note, ALL_MPF_NODES has been replaced by CORE_MPF_NODES.\n\n\n\n\nDefault Detection System Properties\n\n\n\n\n\nThe detection properties that specify the default values when creating new jobs can now be updated at runtime without restarting the Workflow Manager. Changing these properties will only have an effect on new jobs, not jobs that are currently running.\n\n\nThese default detection system properties are separated from the general system properties in the Properties web page. The latter still require the Workflow Manager to be restarted for changes to take effect.\n\n\nThe Apache Commons Configuration library is now used to read and write properties files. When defining a property value using an environment variable in the Properties web page, or \n$MPF_HOME/config/mpf-custom.properties\n, be sure to prepend the variable name with \nenv:\n. For example:\n\n\n\n\ndetection.models.dir.path=${env:MPF_HOME}/models/\n\n\n\n\n\n\nAlternatively, you can define system properties using other system properties:\n\n\n\n\ndetection.models.dir.path=${mpf.share.path}/models/\n\n\n\n\nAdaptive Frame Interval\n\n\n\n\n\nThe FRAME_RATE_CAP property can be used to set a threshold on the maximum number of frames to process within one second of the native video time. This property takes precedence over the user-provided / pipeline-provided value for FRAME_INTERVAL. When the FRAME_RATE_CAP property is specified, an internal frame interval value is calculated as follows:\n\n\n\n\ncalcFrameInterval = max(1, floor(mediaNativeFPS / frameRateCapProp));\n\n\n\n\n\n\nFRAME_RATE_CAP may be disabled by setting it \n= 0. FRAME_INTERVAL can be disabled in the same way.\n\n\nIf FRAME_RATE_CAP is disabled, then FRAME_INTERVAL will be used instead.\n\n\nIf both FRAME_RATE_CAP and FRAME_INTERVAL are disabled, then a value of 1 will be used for FRAME_INTERVAL.\n\n\n\n\nDarknet Component\n\n\n\n\n\nThis release includes a component that uses the \nDarknet neural network framework\n to perform detection and classification of objects using trained models.\n\n\nPipelines for the Tiny YOLO and YOLOv2 models are provided. Due to its large size, the YOLOv2 weights file must be downloaded separately and placed in \n$MPF_HOME/share/models/DarknetDetection\n in order to use the YOLOv2 pipelines. Refer to \nDarknetDetection/plugin-files/models/models.ini\n for more information.\n\n\nThis component supports a preprocessor mode and default mode of operation. If preprocessor mode is enabled, and multiple Darknet detections in a frame share the same classification, then those are merged into a single detection where the region corresponds to the superset region that encapsulates all of the original detections, and the confidence value is the probability that at least one of the original detections is a true positive. If disabled, multiple Darknet detections in a frame are not merged together.\n\n\nDetections are not tracked across frames. One track is generated per detection.\n\n\nThis component supports an optional CLASS_WHITELIST_FILE property. When provided, only detections with class names listed in the file will be generated.\n\n\nThis component can be compiled with GPU support if the NVIDIA CUDA Toolkit is installed on the build machine. Refer to the \nGPU Support Guide\n. If the toolkit is not found, then the component will compile with CPU support only.\n\n\nTo run on a GPU, set the CUDA_DEVICE_ID job property, or set the detection.cuda.device.id system property, \n= 0.\n\n\nWhen CUDA_DEVICE_ID \n= 0, you can set the FALLBACK_TO_CPU_WHEN_GPU_PROBLEM job property, or the detection.use.cpu.when.gpu.problem system property, to \nTRUE\n if you want to run the component logic on the CPU instead of the GPU when a GPU problem is detected.\n\n\n\n\nModels Directory\n\n\n\n\n\nThe\n$MPF_HOME/share/models\n directory is now used by the Darknet and Caffe components to store model files and associated files, such as classification names files, weights files, etc. This allows users to more easily add model files post-deployment. Instead of copying the model files to \n$MPF_HOME/plugins/\ncomponent-name\n/models\n directory on each node in the OpenMPF cluster, they only need to copy them to the shared directory once.\n\n\nTo add new models to the Darknet and Caffe component, add an entry to the respective \ncomponent-name\n/plugin-files/models/models.ini\n file.\n\n\n\n\nPackaging and Deployment\n\n\n\n\n\nPython components are packaged with their respective dependencies as .whl files. This can be automated by providing a setup.py file. An example OpenCV Python component is provided that demonstrates how the component is packaged and deployed with the opencv-python module. When deployed, a virtualenv is created for the component with the .whl files installed in it.\n\n\nWhen deploying OpenMPF, LD_LIBRARY_PATH is no longer set system-wide. Refer to Known Issues.\n\n\n\n\nWeb User Interface\n\n\n\n\n\nUpdated the Nodes page to distinguish between core nodes and spare nodes, and to show when a node is online or offline.\n\n\nUpdated the Component Registration page to list the core nodes as a reminder that changes will not affect spare nodes.\n\n\nUpdated the Properties page to separate the default detection properties from the general system properties.\n\n\n\n\nBug Fixes\n\n\n\n\n\nCustom Action, task, and pipeline names can now contain \"(\" and \")\" characters again.\n\n\nDetection location elements for audio tracks and generic tracks in a JSON output object will now have a y value of \n0\n instead of \n1\n.\n\n\nStreaming health report and summary report timestamps have been corrected to represent hours in the 0-23 range instead of 1-24.\n\n\nSingle-frame .gif files are now segmented properly and no longer result in a NullPointerException.\n\n\nLD_LIBRARY_PATH is now set at the process level for Tomcat, the Node Manager, and component services, instead of at the system level in \n/etc/profile.d/mpf.sh\n. Also, deployments no longer create \n/etc/ld.so.conf.d/mpf.conf\n. This better isolates OpenMPF from the rest of the system and prevents issues, such as being unable to use SSH, when system libraries are not compatible with OpenMPF libraries. The latter situation may occur when running \nyum update\n on the system, which can make OpenMPF unusable until a new deployment package with compatible libraries is installed.\n\n\nThe Workflow Manager will no longer generate an \"Error retrieving the SingleJobInfo model\" line in the log if someone is viewing the Job Status page when a job submitted through the REST API is in progress.\n\n\n\n\nKnown Issues\n\n\n\n\n\nWhen multiple component services of the same type on the same node log to the same file at the same time, sometimes log lines will not be captured in the log file. The logging frameworks (log4j and log4cxx) do not support that usage. This problem happens more frequently on systems running many component services at the same time.\n\n\nThe following exception was observed:\n\n\n\n\ncom.google.protobuf.InvalidProtocolBufferException: Message missing required fields: data_uri\n\n\n\n\n\n\n\nFurther debugging is necessary to determine the reason why that message was missing that field. The situation is not easily reproducible. It may occur when ActiveMQ and / or the system is under heavy load and sends duplicate messages in attempt to ensure message delivery. Some of those messages seem to end up in the dead letter queue (DLQ). For now, we've improved the way we handle messages in the DLQ. If OpenMPF can process a message successfully, the job is marked as COMPLETED_WITH_ERRORS, and the message is moved from ActiveMQ.DLQ to MPF.DLQ_PROCESSED_MESSAGES. If OpenMPF cannot process a message successfully, it is moved from ActiveMQ.DLQ to MPF.DLQ_INVALID_MESSAGES.\n\n\n\n\n\n\nThe \nmpf stop\n command will stop the Workflow Manager, which will in turn send commands to all of the available nodes to stop all running component services. If a service is processing a sub-job when the quit command is received, that service process will not terminate until that sub-job is completely processed. Thus, the service may put a sub-job response on the ActiveMQ response queue after the Workflow Manager has terminated. That will not cause a problem because the queues are flushed the next time the Workflow Manager starts; however, there will be a problem if the service finishes processing the sub-job after the Workflow Manager is restarted. At that time, the Workflow Manager will have no knowledge of the old job and will in turn generate warnings in the log about how the job id is \"not known to the system\" and/or \"not found as a batch or a streaming job\". These can be safely ignored. Often, if these messages appear in the log, then C++ services were running after stopping the Workflow Manager. To address this, you may wish to run \nsudo killall amq_detection_component\n after running \nmpf stop\n.\n\n\n\n\nOpenMPF 2.0.0: February 2018\n\n\n\n\nNOTE:\n Components built for previous releases of OpenMPF are not compatible with OpenMPF 2.0.0 due to Batch Component API changes to support generic detections, and changes made to the format of the descriptor.json file to support stream processing.\n\n\nNOTE:\n This release contains basic support for processing video streams. Currently, the only way to make use of that functionality is through the REST API. Streaming jobs and services cannot be created or monitored through the web UI. Only the SuBSENSE component has been updated to support streaming. Only single-stage pipelines are supported at this time.\n\n\n\n\nDocumentation\n\n\n\n\n\nUpdated documents to distinguish the batch component APIs from the streaming component API.\n\n\nAdded the \nC++ Streaming Component API\n.\n\n\nUpdated the \nC++ Batch Component API\n to describe support for generic detections.\n\n\nUpdated the \nREST API\n with endpoints for streaming jobs.\n\n\n\n\nSupport for Generic Detections\n\n\n\n\n\nC++ and Java components can now declare support for the UNKNOWN data type. The respective batch APIs have been updated with a function that will enable a component to process an \nMPFGenericJob\n, which represents a piece of media that is not a video, image, or audio file.\n\n\nNote that these API changes make OpenMPF R2.0.0 incompatible with components built for previous releases of OpenMPF. Specifically, the new component executor will not be able to load the component logic library.\n\n\n\n\nC++ Batch Component API\n\n\n\n\n\nAdded the following function to support generic detections:\n\n\nMPFDetectionError GetDetections(const MPFGenericJob \njob, vector\nMPFGenericTrack\n \ntracks)\n\n\n\n\n\n\n\n\nJava Batch Component API\n\n\n\n\n\nAdded the following method to support generic detections:\n\n\nList\nMPFGenericTrack\n getDetections(MPFGenericJob job)\n\n\n\n\n\n\n\n\nStreaming REST API\n\n\n\n\n\nAdded the following REST endpoints for streaming jobs:\n\n\n[GET] /rest/streaming/jobs\n: Returns a list of streaming job ids.\n\n\n[POST] /rest/streaming/jobs\n: Creates and submits a streaming job. Users can register for health report and summary report callbacks.\n\n\n[GET] /rest/streaming/jobs/{id}\n: Gets information about a streaming job.\n\n\n[POST] /rest/streaming/jobs/{id}/cancel\n: Cancels a streaming job.\n\n\n\n\n\n\n\n\nWorkflow Manager\n\n\n\n\n\nUpdated to support generic detections.\n\n\nUpdated Redis to store information about streaming jobs.\n\n\nAdded controllers for streaming job REST endpoints.\n\n\nAdded ability to generate health reports and segment summary reports for streaming jobs.\n\n\nImproved code flow between the Workflow Manager and master Node Manager to support streaming jobs.\n\n\nAdded ActiveMQ queues to enable the C++ Streaming Component Executor to send reports and job status to the Workflow Manager.\n\n\n\n\nNode Manager\n\n\n\n\n\nUpdated the master Node Manager and child Node Managers to spawn component services on demand to handle streaming jobs, cancel those jobs, and to monitor the status of those processes.\n\n\nUsing .ini files to represent streaming job properties and enable better communication between a child Node Manager and C++ Streaming Component Executor.\n\n\n\n\nC++ Streaming Component API\n\n\n\n\n\nDeveloped the C++ Streaming Component API with the following functions:\n\n\nMPFStreamingDetectionComponent(const MPFStreamingVideoJob \njob)\n: Constructor that takes a streaming video job.\n\n\nstring GetDetectionType()\n: Returns the type of detection (i.e. \"FACE\").\n\n\nvoid BeginSegment(const VideoSegmentInfo \nsegment_info)\n: Indicates the beginning of a new video segment.\n\n\nbool ProcessFrame(const cv::Mat \nframe, int frame_number)\n: Processes a single frame for the current video segment.\n\n\nvector\nMPFVideoTrack\n EndSegment()\n: Indicates the end of the current video segment.\n\n\n\n\n\n\nUpdated the C++ Hello World component to support streaming jobs.\n\n\n\n\nC++ Streaming Component Executor\n\n\n\n\n\nDeveloped the C++ Streaming Component Executor to load a streaming component logic library, read frames from a video stream, and exercise the component logic through the C++ Streaming Component API.\n\n\nWhen the C++ Streaming Component Executor cannot read a frame from the stream, it will sleep for at least 1 millisecond, doubling the amount of sleep time per attempt until it reaches the  \nstallTimeout\n value specified when the job was created. While stalled, the job status will be STALLED. After the timeout is exceeded, the job will be TERMINATED.\n\n\nThe C++ Streaming Component Executor supports FRAME_INTERVAL, as well as rotation, horizontal flipping, and cropping (region of interest) properties. Does not support USE_KEY_FRAMES.\n\n\n\n\nInteroperability Package\n\n\n\n\n\nAdded the following Java classes to the interoperability package to simplify third party integration:\n\n\nJsonHealthReportCollection\n: Represents the JSON content of a health report callback. Contains one or more \nJsonHealthReport\n objects.\n\n\nJsonSegmentSummaryReport\n: Represents the JSON content of a summary report callback. Content is similar to the JSON output object used for batch processing.\n\n\n\n\n\n\n\n\nSuBSENSE Component\n\n\n\n\n\nThe SuBSENSE component now supports both batch processing and stream processing.\n\n\nEach video segment will be processed independently of the rest. In other words, tracks will be generated on a segment-by-segment basis and tracks will not carry over between segments.\n\n\nNote that the last frame in the previous segment will be used to determine if there is motion in the first frame of the next segment.\n\n\n\n\nPackaging and Deployment\n\n\n\n\n\nUpdated descriptor.json fields to allow components to support batch and/or streaming jobs. Components that use the old descriptor.json file format cannot be registered through the web UI.  \n\n\nBatch component logic and streaming component logic are compiled into separate libraries.\n\n\nThe mySQL \nstreaming_job_request\n table has been updated with the following fields, which are used to populate the JSON health reports:\n\n\nstatus_detail\n: (Optional) A user-friendly description of the current job status.\n\n\nactivity_frame_id\n: The frame id associated with the last job activity. Activity is defined as the start of a new track for the current segment.\n\n\nactivity_timestamp\n: The timestamp associated with the last job activity.\n\n\n\n\n\n\n\n\nWeb User Interface\n\n\n\n\n\nAdded column names to the table that appears when the user clicks in the Media button associated with a job on the Job Status page. Now descriptive comments are provided when table cells are empty.\n\n\n\n\nBug Fixes\n\n\n\n\n\nUpgraded Tika to 1.17 to resolve an issue with improper indentation in a Python file (rotation.py) that resulted in generating at least one error message per image processed. When processing a large number of images, this would generate may error messages, causing the Automatic Bug Reporting Tool daemon (abrtd) process to run at 100% CPU. Once in that state, that process would stay there, essentially wasting on CPU core. This resulted in some of the Jenkins virtual machines we used for testing to become unresponsive.\n\n\n\n\nKnown Issues\n\n\n\n\n\n\n\nOpenCV 3.3.0 \ncv::imread()\n does not properly decode some TIFF images that have EXIF orientation metadata. It can handle images that are flipped horizontally, but not vertically. It also has issues with rotated images. Since most components rely on that function to read image data, those components may silently fail to generate detections for those kinds of images.\n\n\n\n\n\n\nUsing single quotes, apsotrophes, or double quotes in the name of an algorithm, action, task, or pipeline configured on an existing OpenMPF system will result in a failure to perform an OpenMPF upgrade on that system. Specifically, the step where pre-existing custom actions, tasks, and pipelines are carried over to the upgraded version of OpenMPF will fail. Please do not use those special characters while naming those elements. If this has been done already, then those elements should be manually renamed in the XML files prior to an upgrade attempt.\n\n\n\n\n\n\nOpenMPF uses OpenCV, which  uses FFmpeg, to connect to video streams. If a proxy and/or firewall prevents the network connection from succeeding, then OpenCV, or the underlying FFmpeg library, will segfault. This causes the C++ Streaming Component Executor process to fail. In turn, the job status will be set to ERROR with a status detail message of \"Unexpected error. See logs for details\". In this case, the logs will not contain any useful information. You can identify a segfault by the following line in the node-manager log:\n\n\n\n\n\n\n2018-02-15 16:01:21,814 INFO [pool-3-thread-4] o.m.m.nms.streaming.StreamingProcess - Process: Component exited with exit code 139\u00a0\n\n\n\n\n\n\nTo determine if FFmpeg can connect to the stream or not, run \nffmpeg -i \nstream-uri\n in a terminal window. Here's an example when it's successful:\n\n\n\n\n[mpf@localhost bin]$ ffmpeg -i rtsp://184.72.239.149/vod/mp4:BigBuckBunny_115k.mov\nffmpeg version n3.3.3-1-ge51e07c Copyright (c) 2000-2017 the FFmpeg developers\n  built with gcc 4.8.5 (GCC) 20150623 (Red Hat 4.8.5-4)\n  configuration: --prefix=/apps/install --extra-cflags=-I/apps/install/include --extra-ldflags=-L/apps/install/lib --bindir=/apps/install/bin --enable-gpl --enable-nonfree --enable-libtheora --enable-libfreetype --enable-libmp3lame --enable-libvorbis --enable-libx264 --enable-libopencore-amrnb --enable-libopencore-amrwb --enable-version3 --enable-shared --disable-libsoxr --enable-avresample\n  libavutil      55. 58.100 / 55. 58.100\n  libavcodec     57. 89.100 / 57. 89.100\n  libavformat    57. 71.100 / 57. 71.100\n  libavdevice    57.  6.100 / 57.  6.100\n  libavfilter     6. 82.100 /  6. 82.100\n  libavresample   3.  5.  0 /  3.  5.  0\n  libswscale      4.  6.100 /  4.  6.100\n  libswresample   2.  7.100 /  2.  7.100\n  libpostproc    54.  5.100 / 54.  5.100\n[rtsp @ 0x1924240] UDP timeout, retrying with TCP\nInput #0, rtsp, from 'rtsp://184.72.239.149/vod/mp4:BigBuckBunny_115k.mov':\n  Metadata:\n    title           : BigBuckBunny_115k.mov\n  Duration: 00:09:56.48, start: 0.000000, bitrate: N/A\n    Stream #0:0: Audio: aac (LC), 12000 Hz, stereo, fltp\n    Stream #0:1: Video: h264 (Constrained Baseline), yuv420p(progressive), 240x160, 24 fps, 24 tbr, 90k tbn, 48 tbc\nAt least one output file must be specified\n\n\n\n\n\n\nHere's an example when it's not successful, so there may be network issues:\n\n\n\n\n[mpf@localhost bin]$ ffmpeg -i rtsp://184.72.239.149/vod/mp4:BigBuckBunny_115k.mov\nffmpeg version n3.3.3-1-ge51e07c Copyright (c) 2000-2017 the FFmpeg developers\n  built with gcc 4.8.5 (GCC) 20150623 (Red Hat 4.8.5-4)\n  configuration: --prefix=/apps/install --extra-cflags=-I/apps/install/include --extra-ldflags=-L/apps/install/lib --bindir=/apps/install/bin --enable-gpl --enable-nonfree --enable-libtheora --enable-libfreetype --enable-libmp3lame --enable-libvorbis --enable-libx264 --enable-libopencore-amrnb --enable-libopencore-amrwb --enable-version3 --enable-shared --disable-libsoxr --enable-avresample\n  libavutil      55. 58.100 / 55. 58.100\n  libavcodec     57. 89.100 / 57. 89.100\n  libavformat    57. 71.100 / 57. 71.100\n  libavdevice    57.  6.100 / 57.  6.100\n  libavfilter     6. 82.100 /  6. 82.100\n  libavresample   3.  5.  0 /  3.  5.  0\n  libswscale      4.  6.100 /  4.  6.100\n  libswresample   2.  7.100 /  2.  7.100\n  libpostproc    54.  5.100 / 54.  5.100\n[tcp @ 0x171c300] Connection to tcp://184.72.239.149:554?timeout=0 failed: Invalid argument\nrtsp://184.72.239.149/vod/mp4:BigBuckBunny_115k.mov: Invalid argument\n\n\n\n\n\n\nTika 1.17 does not come pre-packaged with support for some embedded image formats in PDF files, possibly to avoid patent issues. OpenMPF does not handle embedded images in PDFs, so that's not a problem. Tika will print out the following warnings, which can be safely ignored:\n\n\n\n\nJan 22, 2018 11:02:15 AM org.apache.tika.config.InitializableProblemHandler$3 handleInitializableProblem\nWARNING: JBIG2ImageReader not loaded. jbig2 files will be ignored\nSee https://pdfbox.apache.org/2.0/dependencies.html#jai-image-io\nfor optional dependencies.\nTIFFImageWriter not loaded. tiff files will not be processed\nSee https://pdfbox.apache.org/2.0/dependencies.html#jai-image-io\nfor optional dependencies.\nJ2KImageReader not loaded. JPEG2000 files will not be processed.\nSee https://pdfbox.apache.org/2.0/dependencies.html#jai-image-io\nfor optional dependencies.\n\n\n\n\n\nOpenMPF 1.0.0: October 2017\n\n\nDocumentation\n\n\n\n\n\nUpdated the \nBuild Guide\n with instructions for installing the latest JDK, latest JRE, FFmpeg 3.3.3, new codecs, and OpenCV 3.3.\n\n\nAdded an \nAcknowledgements\n section that provides information on third party dependencies leveraged by the OpenMPF.\n\n\nAdded a \nFeed Forward Guide\n that explains feed forward processing and how to use it.\n\n\nAdded missing requirements checklist content to the \nInstall Guide\n.\n\n\nUpdated the README at the top level of each of the primary repositories to help with user navigation and provide general information.\n\n\n\n\nUpgrade to FFmpeg 3.3.3 and OpenCV 3.3\n\n\n\n\n\nUpdated core framework from FFmpeg 2.6.3 to FFmpeg 3.3.3.\n\n\nAdded the following FFmpeg codecs: x256, VP9, AAC, Opus, Speex.\n\n\nUpdated core framework and components from OpenCV 3.2 to OpenCV 3.3. No longer building with opencv_contrib.\n\n\n\n\nFeed Forward Behavior\n\n\n\n\n\nUpdated the workflow manager (WFM) and all video components to optionally perform feed forward processing for batch jobs. This allows tracks to be passed forward from one pipeline stage to the next. Components in the next stage will only process the frames associated with the detections in those tracks. This differs from the default segmenting behavior, which does not preserve detection regions or track information between stages.\n\n\nTo enable this behavior, the optional FEED_FORWARD_TYPE property must be set to \"FRAME\", \"SUPERSET_REGION\", or \"REGION\". If set to \"FRAME\" then the components in the next stage will process the whole frame region associated with each detection in the track passed forward. If set to \"SUPERSET_REGION\" then the components in the next stage will determine the bounding box that encapsulates all of the detection regions in the track, and only process the pixel data within that superset region. If set to \"REGION\" then the components in the next stage will process the region associated with each detection in the track passed forward, which may vary in size and position from frame to frame.\n\n\nThe optional FEED_FORWARD_TOP_CONFIDENCE_COUNT property can be set to a number to limit the number of detections passed forward in a track. For example, if set to \"5\", then only the top 5 detections in the track will be passed forward and processed by the next stage. The top detections are defined as those with the highest confidence values, or if the confidence values are the same, those with the lowest frame index.\n\n\nNote that setting the feed forward properties has no effect on the first pipeline stage because there is no prior stage that can pass tracks to it.\n\n\n\n\nCaffe Component\n\n\n\n\n\nUpdated the Caffe component to process images in the BGR color space instead of the RGB color space. This addresses a bug found in OpenCV. Refer to the Bug Fixes section below.\n\n\nAdded support for processing videos.\n\n\nAdded support for an optional ACTIVATION_LAYER_LIST property. For each network layer specified in the list, the \ndetectionProperties\n map in the JSON output object will contain one entry. The value is an encoded string of the JSON representation of an OpenCV matrix of the activation values for that layer. The activation values are obtained after the Caffe network has processed the frame data.\n\n\nAdded support for an optional SPECTRAL_HASH_FILE_LIST property. For each JSON file specified in the list, the \ndetectionProperties\n map in the JSON output object will contain one entry. The value is a string of 0's and 1's representing the spectral hash calculated using the information in the spectral hash JSON file. The spectral hash is calculated using activation values after the Caffe network has processed the frame data.\n\n\nAdded a pipeline to showcase the above two features for the GoogLeNet Caffe model.\n\n\nRemoved the TRANSPOSE property from the Caffe component since it was not necessary.\n\n\nAdded red, green, and blue mean subtraction values to the GoogLeNet pipeline.\n\n\n\n\nUse Key Frames\n\n\n\n\n\nAdded support for an optional USE_KEY_FRAMES property to each video component. When true the component will only look at key frames (I-frames) from the input video. Can be used in conjunction with FRAME_INTERVAL. For example, when USE_KEY_FRAMES is true, and FRAME_INTERVAL is set to \"2\", then every other key frame will be processed.\n\n\n\n\nMPFVideoCapture and MPFImageReader Tools\n\n\n\n\n\nUpdated the MPFVideoCapture and MPFImageReader tools to handle feed forward properties.\n\n\nUpdated the MPFVideoCapture tool to handle FRAME_INTERVAL and USE_KEY_FRAMES properties.\n\n\nUpdated all existing components to leverage these tools as much as possible.\n\n\nWe encourage component developers to use these tools to automatically take care of common frame grabbing and frame manipulation behaviors, and not to reinvent the wheel.\n\n\n\n\nDead Letter Queue\n\n\n\n\n\nIf for some reason a sub-job request that should have gone to a component ends up on the ActiveMQ Dead Letter Queue (DLQ), then the WFM will now process that failed request so that the job can complete. The ActiveMQ management page will now show that ActiveMQ.DLQ has 1 consumer. It will also show unconsumed messages in MPF.PROCESSED_DLQ_MESSAGES. Those are left for auditing purposes. The \"Message Detail\" for these shows the string representation of the original job request protobuf message.\n\n\n\n\nUpgrade Path\n\n\n\n\n\nRemoved the Release 0.8 to Release 0.9 upgrade path in the deployment scripts.\n\n\nAdded support for a Release 0.9 to Release 1.0.0 upgrade path, and a Release 0.10.0 to Release 1.0.0 upgrade path.\n\n\n\n\nMarkup\n\n\n\n\n\nBounding boxes are now drawn along the interpolated path between detection regions whenever there are one or more frames in a track which do not have detections associated with them.\n\n\nFor each track, the color of the bounding box is now a randomly selected hue in the HSV color space. The colors are evenly distributed using the golden ratio.\n\n\n\n\nBug Fixes\n\n\n\n\n\nFixed a \nbug in OpenCV\n where the Caffe example code was processing images in the RGB color space instead of the BGR color space. Updated the OpenMPF Caffe component accordingly.\n\n\nFixed a bug in the OpenCV person detection component that caused bounding boxes to be too large for detections near the edge of a frame.\n\n\nResubmitting jobs now properly carries over configured job properties.\n\n\nFixed a bug in the build order of the OpenMPF project so that test modules that the WFM depends on are built before the WFM itself.\n\n\nThe Markup component draws bounding boxes between detections when a FRAME_INTERVAL is specified. This is so that the bounding box in the marked-up video appears in every frame. Fixed a bug where the bounding boxes drawn on non-detection frames appeared to stand still rather than move along the interpolated path between detection regions.\n\n\nFixed a bug on the OALPR license plate detection component where it was not properly handling the SEARCH_REGION_* properties.\n\n\nSupport for the MIN_GAP_BETWEEN_SEGMENTS property was not implemented properly. When the gap between two segments is less than this property value then the segments should be merged; otherwise, the segments should remain separate. In some cases, the exact opposite was happening. This bug has been fixed.\n\n\n\n\nKnown Issues\n\n\n\n\n\nBecause of the number of additional ActiveMQ messages involved, enabling feed forward for low resolution video may take longer than the non-feed-forward behavior.\n\n\n\n\nOpenMPF 0.10.0: July 2017\n\n\n\n\nWARNING:\n There is no longer a \u201cDEFAULT CAFFE ACTION\u201d, \u201cDEFAULT CAFFE TASK\u201d, or \u201cDEFAULT CAFFE PIPELINE\u201d. There is now a \u201cCAFFE GOOGLENET DETECTION PIPELINE\u201d and \u201cCAFFE YAHOO NSFW DETECTION PIPELINE\u201d, which each have a respective action and task.\n\n\nNOTE:\n MPFImageReader has been re-enabled in this version of OpenMPF since we upgraded to OpenCV 3.2, which addressed the known issues with imread(), auto-orientation, and jpeg files in OpenCV 3.1.\n\n\n\n\nDocumentation\n\n\n\n\n\nAdded a \nContributor Guide\n that provides guidelines for contributing to the OpenMPF codebase.\n\n\nUpdated the \nJava Batch Component API\n with links to the example Java components.\n\n\nUpdated the \nBuild Guide\n with instructions for OpenCV 3.2.\n\n\n\n\nUpgrade to OpenCV 3.2\n\n\n\n\n\nUpdated core framework and components from OpenCV 3.1 to OpenCV 3.2.\n\n\n\n\nSupport for Animated gifs\n\n\n\n\n\nAll gifs are now treated as videos. Each gif will be handled as an MPFVideoJob.\n\n\nUnanimated gifs are treated as 1-frame videos.\n\n\nThe WFM Media Inspector now populates the \nmedia_properties\n map with a \"FRAME_COUNT\" entry (in addition to the \"DURATION, and \"FPS\" entries).\n\n\n\n\nCaffe Component\n\n\n\n\n\nAdded support for the Yahoo Not Suitable for Work (NSFW) Caffe model for explicit material detection.\n\n\nUpdated the Caffe component to support the OpenCV 3.2 Deep Neural Network (DNN) module.\n\n\n\n\nFuture Support for Streaming Video\n\n\n\n\n\nNOTE:\n At this time, OpenMPF does not support streaming video. This section details what's being / has been done so far to prepare for that feature.\n\n\n\n\n\n\nThe codebase is being updated / refactored to support both the current \"batch\" job functionality and new \"streaming\" job functionality.\n\n\nbatch job: complete video files are written to disk before they are processed\n\n\nstreaming job: video frames are read from a streaming endpoint (such as RTSP) and processed in near real time\n\n\n\n\n\n\nThe REST API is being updated with endpoints for streaming jobs:\n\n\n[POST] /rest/streaming/jobs\n: Creates and submits a streaming job\n\n\n[POST] /rest/streaming/jobs/{id}/cancel\n: Cancels a streaming job\n\n\n[GET] /rest/streaming/jobs/{id}\n: Gets information about a streaming job\n\n\n\n\n\n\nThe Redis and mySQL databases are being updated to support streaming video jobs.\n\n\nA batch job will never have the same id as a streaming job. The integer ids will always be unique.\n\n\n\n\n\n\n\n\nBug Fixes\n\n\n\n\n\nThe MOG and SuBSENSE component services could segfault and terminate if the \u201cUSE_MOTION_TRACKING\u201d property was set to \u201c1\u201d and a detection was found close to the edge of the frame. Specifically, this would only happen if the video had a width and/or height dimension that was not an exact power of two.\n\n\nThe reason was because the code downsamples each frame by a power of two and rounds the value of the width and height up to the nearest integer. Later on when upscaling detection rectangles back to a size that\u2019s relative to the original image, the resized rectangle sometimes extended beyond the bounds of the original frame.\n\n\n\n\n\n\n\n\nKnown Issues\n\n\n\n\n\nIf a job is submitted through the REST API, and a user to logged into the web UI and looking at the job status page, the WFM may generate \"Error retrieving the SingleJobInfo model for the job with id\" messages.\n\n\nThis is because the job status is only added to the HTTP session object if the job is submitted through the web UI. When the UI queries the job status it inspects this object.\n\n\nThis message does not appear if job status is obtained using the \n[GET] /rest/jobs/{id}\n endpoint.\n\n\n\n\n\n\nThe \n[GET] /rest/jobs/stats\n endpoint aggregates information about all of the jobs ever run on the system. If thousands of jobs have been run, this call could take minutes to complete. The code should be improved to execute a direct mySQL query.\n\n\n\n\nOpenMPF 0.9.0: April 2017\n\n\n\n\nWARNING:\n MPFImageReader has been disabled in this version of OpenMPF. Component developers should use MPFVideoCapture instead. This affects components developed against previous versions of OpenMPF and components developed against this version of OpenMPF. Please refer to the Known Issues section for more information.\n\n\nWARNING:\n The OALPR Text Detection Component has been renamed to OALPR \nLicense Plate\n Text Detection Component. This affects the name of the component package and the name of the actions, tasks, and pipelines. When upgrading from R0.8 to R0.9, if the old OALPR Text Detection Component is installed in R0.8 then you will be prompted to install it again at the end of the upgrade path script. We recommend declining this prompt because the old component will conflict with the new component.\n\n\nWARNING:\n Action, task, and pipeline names that started with \"MOTION DETECTION PREPROCESSOR\" have been renamed \"MOG MOTION DETECTION PREPROCESSOR\". Similarly, \"WITH MOTION PREPROCESSOR\" has changed to \"WITH MOG MOTION PREPROCESSOR\".\n\n\n\n\nDocumentation\n\n\n\n\n\nUpdated the \nREST API\n to reflect job properties, algorithm-specific properties, and media-specific properties.\n\n\nStreamlined the \nC++ Batch Component API\n document for clarity and simplicity.\n\n\nCompleted the \nJava Batch Component API\n document.\n\n\nUpdated the \nAdmin Guide\n and \nUser Guide\n to reflect web UI changes.\n\n\nUpdated the \nBuild Guide\n with instructions for GitHub repositories.\n\n\n\n\nWorkflow Manager\n\n\n\n\n\nAdded support for job properties, which will override pre-defined pipeline properties.\n\n\nAdded support for algorithm-specific properties, which will apply to a single stage of the pipeline and will override job properties and pre-defined pipeline properties.\n\n\nAdded support for media-specific properties, which will apply to a single piece and media and will override job properties, algorithm-specific properties, and pre-defined pipeline properties.\n\n\nComponents can now be automatically registered and installed when the web application starts in Tomcat.\n\n\n\n\nWeb User Interface\n\n\n\n\n\nThe \"Close All\" button on pop-up notifications now dismisses all notifications from the queue, not just the visible ones.\n\n\nJob completion notifications now only appear for jobs created during the current login session instead of all jobs.\n\n\nThe ROTATION, HORIZONTAL_FLIP, and SEARCH_REGION_* properties can be set using the web interface when creating a job. Once files are selected for a job, these properties can be set individually or by groups of files.\n\n\nThe Node and Process Status page has been merged into the Node Configuration page for simplicity and ease of use.\n\n\nThe Media Markup results page has been merged into the Job Status page for simplicity and ease of use.\n\n\nThe File Manager UI has been improved to handle large numbers of files and symbolic links.\n\n\nThe side navigation menu is now replaced by a top navigation bar.\n\n\n\n\nREST API\n\n\n\n\n\nAdded an optional jobProperties object to the /rest/jobs/ request which contains String key-value pairs which override the pipeline's pre-configured job properties.\n\n\nAdded an optional algorithmProperties object to the /rest/jobs/ request which can be used to configure properties for specific algorithms in the pipeline. These properties override the pipeline's pre-configured job properties. They also override the values in the jobProperties object.\n\n\nUpdated the /rest/jobs/ request to add more detail to media, replacing a list of mediaUri Strings with a list of media objects, each of which contains a mediaUri and an optional mediaProperties map. The mediaProperties map can be used to configure properties for the specific piece of media. These properties override the pipeline's pre-configured job properties, values in the jobProperties object, and values in the algorithmProperties object.\n\n\nStreamlined the actions, tasks, and pipelines endpoints that are used by the web UI.\n\n\n\n\nFlipping, Rotation, and Region of Interest\n\n\n\n\n\nThe ROTATION, HORIZONTAL_FLIP, and SEARCH_REGION_* properties will no longer appear in the detectionProperties map in the JSON detection output object. When applied to an algorithm these properties now appear in the pipeline.stages.actions.properties element. When applied to a piece of media these properties will now appear in the the media.mediaProperties element.\n\n\nThe OpenMPF now supports multiple regions of interest in a single media file.  Each region will produce tracks separately, and the tracks for each region will be listed in the JSON output as if from a separate media file.\n\n\n\n\nComponent API\n\n\n\n\n\nJava Batch Component API is functionally complete for third-party development, with the exception of Component Adapter and frame transformation utilities classes.\n\n\nRe-architected the Java Batch Component API to use a more traditional Java method structure of returning track lists and throwing exceptions (rather than modifying input track lists and returning statuses), and encapsulating job properties into MPFJob objects:\n\n\nList\nMPFVideoTrack\n getDetections(MPFVideoJob job) throws MPFComponentDetectionError\n\n\nList\nMPFAudioTrack\n getDetections(MPFAudioJob job) throws MPFComponentDetectionError\n\n\nList\nMPFImageLocation\n getDetections(MPFImageJob job) throws MPFComponentDetectionError\n\n\n\n\n\n\nCreated examples for the Java Batch Component API.\n\n\nReorganized the Java and C++ component source code to enable component development without the OpenMPF core, which will simplify component development and streamline the code base.\n\n\n\n\nJSON Output Objects\n\n\n\n\n\nThe JSON output object for the job now contains a jobProperties map which contains all properties defined for the job in the job request.  For example, if the job request specifies a CONFIDENCE_THRESHOLD of then the jobProperties map in the output will also list a CONFIDENCE_THRESHOLD of 5.\n\n\nThe JSON output object for the job now contains a algorithmProperties element which contains all algorithm-specific properties defined for the job in the job request.  For example, if the job request specifies a FRAME_INTERVAL of 2 for FACECV then the algorithmProperties element in the output will contain an entry for \"FACECV\" and that entry will list a FRAME_INTERVAL of 2.\n\n\nEach JSON media output object now contains a mediaProperties map which contains all media-specific properties defined by the job request.  For example, if the job request specifies a ROTATION of 90 degrees for a single piece of media then the mediaProperties map for that piece of piece will list a ROTATION of 90.\n\n\nThe content of JSON output objects are now organized by detection type (e.g. MOTION, FACE, PERSON, TEXT, etc.) rather than action type.\n\n\n\n\nCaffe Component\n\n\n\n\n\nAdded support for flip, rotation, and cropping to regions of interest.\n\n\nAdded support for returning multiple classifications per detection based on user-defined settings. The classification list is in order of decreasing confidence value.\n\n\n\n\nNew Pipelines\n\n\n\n\n\nNew SuBSENSE motion preprocessor pipelines have been added to components that perform detection on video.\n\n\n\n\nPackaging and Deployment\n\n\n\n\n\nActions.xml, Algorithms.xml, nodeManagerConfig.xml, nodeServicesPalette.json, Pipelines.xml, and Tasks.xml are no longer stored within the Workflow Manager WAR file. They are now stored under \n$MPF_HOME/data\n. This makes it easier to upgrade the Workflow Manager and makes it easier for users to access these files.\n\n\nEach component can now be optionally installed and registered during deployment. Components not registered are set to the UPLOADED state. They can then be removed or registered through the Component Registration page.\n\n\nJava components are now packaged as tar.gz files instead of RPMs, bringing them into alignment with C++ components.\n\n\nOpenMPF R0.9 can be installed over OpenMPF R0.8. The deployment scripts will determine that an upgrade should take place.\n\n\nAfter the upgrade, user-defined actions, tasks, and pipelines will have \"CUSTOM\" prepended to their name.\n\n\nThe job_request table in the mySQL database will have a new \"output_object_version\" column. This column will have \"1.0\" for jobs created using OpenMPF R0.8 and \"2.0\" for jobs created using OpenMPF R0.9. The JSON output object schema has changed between these versions.\n\n\n\n\n\n\nReorganized source code repositories so that component SDKs can be downloaded separately from the OpenMPF core and so that components are grouped by license and maturity. Build scripts have been created to streamline and simplify the build process across the various repositories.\n\n\n\n\nUpgrade to OpenCV 3.1\n\n\n\n\n\nThe OpenMPF software has been ported to use OpenCV 3.1, including all of the C++ detection components and the markup component. For the OpenALPR license plate detection component, the versions of the openalpr, tesseract, and leptonica libraries were also upgraded to openalpr-2.3.0, tesseract-3.0.4, and leptonica-1.7.2.  For the SuBSENSE motion component, the version of the SuBSENSE library was upgraded to use the code found at this location: \nhttps://bitbucket.org/pierre_luc_st_charles/subsense/src\n.\n\n\n\n\nBug Fixes\n\n\n\n\n\nMOG motion detection always detected motion in frame 0 of a video. Because motion can only be detected between two adjacent frames, frame 1 is now the first frame in which motion can be detected.\n\n\nMOG motion detection never detected motion in the first frame of a video segment (other than the first video segment because of the frame 0 bug described above). Now, motion is detected using the first frame before the start of a segment, rather than the first frame of the segment.\n\n\nThe above bugs were also present in SuBSENSE motion detection and have been fixed.\n\n\nSuBSENSE motion detection generated tracks where the frame numbers were off by one. Corrected the frame index logic.\n\n\nVery large video files caused an out of memory error in the system during Workflow Manager media inspection.\n\n\nA job would fail when processing images with an invalid metadata tag for the camera flash setting.\n\n\nUsers were permitted to select invalid file types using the File Manager UI.\n\n\n\n\nKnown Issues\n\n\n\n\n\nMPFImageReader does not work reliably with the current release version of OpenCV 3.1\n: In OpenCV 3.1, new functionality was introduced to interpret EXIF information when reading jpeg files.\n\n\nThere are two issues with this new functionality that impact our ability to use the OpenCV \nimread()\n function with MPFImageReader:\n\n\nFirst, because of a bug in the OpenCV code, reading a jpeg file that contains exif information could cause it to hang. (See \nhttps://github.com/opencv/opencv/issues/6665\n.)\n\n\nSecond, it is not possible to tell the \nimread()\nfunction to ignore the EXIF data, so the image it returns is automatically rotated. (See \nhttps://github.com/opencv/opencv/issues/6348\n.) This results in the MPFImageReader applying a second rotation to the image due to the EXIF information.\n\n\n\n\n\n\nTo address these issues, we developed the following workarounds:\n\n\nCreated a version of the MPFVideoCapture that works with an MPFImageJob. The new MPFVideoCapture can pull frames from both video files and images. MPFVideoCapture leverages cv::VideoCapture, which does not have the two issues described above.\n\n\nDisabled the use of MPFImageReader to prevent new users from trying to develop code leveraging this previous functionality.", 
            "title": "Release Notes"
        }, 
        {
            "location": "/Release-Notes/index.html#openmpf-410-july-2019", 
            "text": "", 
            "title": "OpenMPF 4.1.0: July 2019"
        }, 
        {
            "location": "/Release-Notes/index.html#openmpf-400-february-2019", 
            "text": "", 
            "title": "OpenMPF 4.0.0: February 2019"
        }, 
        {
            "location": "/Release-Notes/index.html#openmpf-300-december-2018", 
            "text": "NOTE:  The  Build Guide  and  Install Guide  are outdated. The old process for manually configuring a Build VM, using it to build an OpenMPF package, and installing that package, is deprecated in favor of Docker containers. Please refer to the openmpf-docker  README .  NOTE:  Do not attempt to register or unregister a component through the Nodes UI in a Docker deployment. It may appear to succeed, but the changes will not affect the child Node Manager containers, only the Workflow Manager container. Also, do not attempt to use the  mpf  command line tools in a Docker deployment.", 
            "title": "OpenMPF 3.0.0: December 2018"
        }, 
        {
            "location": "/Release-Notes/index.html#openmpf-210-june-2018", 
            "text": "NOTE:  If building this release on a machine used to build a previous version of OpenMPF, then please run  sudo pip install --upgrade pip  to update to at least pip 10.0.1. If not, the OpenMPF build script will fail to properly download .whl files for Python modules.", 
            "title": "OpenMPF 2.1.0: June 2018"
        }, 
        {
            "location": "/Release-Notes/index.html#openmpf-200-february-2018", 
            "text": "NOTE:  Components built for previous releases of OpenMPF are not compatible with OpenMPF 2.0.0 due to Batch Component API changes to support generic detections, and changes made to the format of the descriptor.json file to support stream processing.  NOTE:  This release contains basic support for processing video streams. Currently, the only way to make use of that functionality is through the REST API. Streaming jobs and services cannot be created or monitored through the web UI. Only the SuBSENSE component has been updated to support streaming. Only single-stage pipelines are supported at this time.", 
            "title": "OpenMPF 2.0.0: February 2018"
        }, 
        {
            "location": "/Release-Notes/index.html#openmpf-100-october-2017", 
            "text": "", 
            "title": "OpenMPF 1.0.0: October 2017"
        }, 
        {
            "location": "/Release-Notes/index.html#openmpf-0100-july-2017", 
            "text": "WARNING:  There is no longer a \u201cDEFAULT CAFFE ACTION\u201d, \u201cDEFAULT CAFFE TASK\u201d, or \u201cDEFAULT CAFFE PIPELINE\u201d. There is now a \u201cCAFFE GOOGLENET DETECTION PIPELINE\u201d and \u201cCAFFE YAHOO NSFW DETECTION PIPELINE\u201d, which each have a respective action and task.  NOTE:  MPFImageReader has been re-enabled in this version of OpenMPF since we upgraded to OpenCV 3.2, which addressed the known issues with imread(), auto-orientation, and jpeg files in OpenCV 3.1.", 
            "title": "OpenMPF 0.10.0: July 2017"
        }, 
        {
            "location": "/Release-Notes/index.html#openmpf-090-april-2017", 
            "text": "WARNING:  MPFImageReader has been disabled in this version of OpenMPF. Component developers should use MPFVideoCapture instead. This affects components developed against previous versions of OpenMPF and components developed against this version of OpenMPF. Please refer to the Known Issues section for more information.  WARNING:  The OALPR Text Detection Component has been renamed to OALPR  License Plate  Text Detection Component. This affects the name of the component package and the name of the actions, tasks, and pipelines. When upgrading from R0.8 to R0.9, if the old OALPR Text Detection Component is installed in R0.8 then you will be prompted to install it again at the end of the upgrade path script. We recommend declining this prompt because the old component will conflict with the new component.  WARNING:  Action, task, and pipeline names that started with \"MOTION DETECTION PREPROCESSOR\" have been renamed \"MOG MOTION DETECTION PREPROCESSOR\". Similarly, \"WITH MOTION PREPROCESSOR\" has changed to \"WITH MOG MOTION PREPROCESSOR\".", 
            "title": "OpenMPF 0.9.0: April 2017"
        }, 
        {
            "location": "/Install-Guide/index.html", 
            "text": "NOTICE:\n This software (or technical data) was produced for the U.S. Government under contract, and is subject to the Rights in Data-General Clause 52.227-14, Alt. IV (DEC 2007). Copyright 2019 The MITRE Corporation. All Rights Reserved.\n\n\n\n\nDocker\n\n\nOpenMPF is installed using the Docker container platform. Please read the \ndisclaimer\n below.\n\n\nTo use prebuilt Docker images, refer to the \"Quick Start\" section of the documentation for the OpenMPF Workflow Manager image on \nDockerHub\n.\n\n\nFor more information, including how to setup Docker, and build and deploy OpenMPF Docker images, refer to the openmpf-docker \nREADME\n.\n\n\nAdditionally, if you would like to install OpenMPF across multiple physical or virtual machines, then refer to the openmpf-docker \nSwarm Deployment Guide\n. \n\n\nDisclaimer\n\n\nThe Open Media Processing Framework (OpenMPF) software is provided as raw source code. This is to avoid any potential licensing issues that may arise from distributing a pre-compiled executable that is linked to dependencies that are licensed under a copyleft license or have patent restrictions. Generally, it is acceptable to build and execute software with these dependencies for non-commercial in-house use.\n\n\nBy distributing the OpenMPF software as raw source code the development team is able to keep most of the software clean from copyleft and patent issues so that it can be published under a more open Apache license and freely distributed to interested parties.\n\n\n\n\nIMPORTANT:\n It is the responsibility of the end users who build the OpenMPF software to abide by all of the non-commercial and re-distribution restrictions imposed by the dependencies that the OpenMPF software uses. Building OpenMPF and linking in these dependencies at build time or run time may result in creating a derivative work under the terms of the GNU General Public License. Refer to \nAcknowledgements\n for more information about these dependencies.\n\n\n\n\nIn general, it is only acceptable to use and distribute the executable form of the OpenMPF, which includes generated Docker images, \"in house\", which is loosely defined as internally with an organization. The OpenMPF should only be distributed to third parties in raw source code form and those parties will be responsible for creating their own executables and Docker images.", 
            "title": "Install Guide"
        }, 
        {
            "location": "/Install-Guide/index.html#docker", 
            "text": "OpenMPF is installed using the Docker container platform. Please read the  disclaimer  below.  To use prebuilt Docker images, refer to the \"Quick Start\" section of the documentation for the OpenMPF Workflow Manager image on  DockerHub .  For more information, including how to setup Docker, and build and deploy OpenMPF Docker images, refer to the openmpf-docker  README .  Additionally, if you would like to install OpenMPF across multiple physical or virtual machines, then refer to the openmpf-docker  Swarm Deployment Guide .", 
            "title": "Docker"
        }, 
        {
            "location": "/Install-Guide/index.html#disclaimer", 
            "text": "The Open Media Processing Framework (OpenMPF) software is provided as raw source code. This is to avoid any potential licensing issues that may arise from distributing a pre-compiled executable that is linked to dependencies that are licensed under a copyleft license or have patent restrictions. Generally, it is acceptable to build and execute software with these dependencies for non-commercial in-house use.  By distributing the OpenMPF software as raw source code the development team is able to keep most of the software clean from copyleft and patent issues so that it can be published under a more open Apache license and freely distributed to interested parties.   IMPORTANT:  It is the responsibility of the end users who build the OpenMPF software to abide by all of the non-commercial and re-distribution restrictions imposed by the dependencies that the OpenMPF software uses. Building OpenMPF and linking in these dependencies at build time or run time may result in creating a derivative work under the terms of the GNU General Public License. Refer to  Acknowledgements  for more information about these dependencies.   In general, it is only acceptable to use and distribute the executable form of the OpenMPF, which includes generated Docker images, \"in house\", which is loosely defined as internally with an organization. The OpenMPF should only be distributed to third parties in raw source code form and those parties will be responsible for creating their own executables and Docker images.", 
            "title": "Disclaimer"
        }, 
        {
            "location": "/User-Guide/index.html", 
            "text": "NOTICE:\n This software (or technical data) was produced for the U.S. Government under contract, and is subject to the Rights in Data-General Clause 52.227-14, Alt. IV (DEC 2007).\nCopyright 2019 The MITRE Corporation. All Rights Reserved.\n\n\nNOTE:\n This release contains basic support for processing video streams. Currently, the only way to make use of that functionality is through the REST API. Streaming jobs and services cannot be created or monitored through the web UI.\n\n\n\n\nGeneral\n\n\nAccessing the Web UI\n\n\nOn the server hosting the Open Media Processing Framework (OpenMPF), the Web UI is accessible at http://localhost:8080/workflow-manager. To access it from other machines, substitute the hostname or IP address of the master node server in place of \"localhost\".\n\n\nThe OpenMPF user interface (UI) was designed and tested for use with Chrome and FireFox. It has not been tested with other browsers. Attempting to use an unsupported browser will result in a warning.\n\n\nLogging In\n\n\nThe OpenMPF Web UI requires user authentication and provides two default accounts: \"mpf\" and \"admin\". The password for the \"mpf\" user is \"mpf123\". These accounts are used to assign user or admin roles for OpenMPF cluster management. Note that an administrator can remove these accounts and/or add new ones using a command line tool. Refer to the \nAdmin Guide\n for features available to an admin user.\n\n\n\n\nThe landing page for a user is the Job Status page:\n\n\n\n\nLogging in starts a user session. By default, after 30 minutes of inactivity the user will automatically be logged out of the system. Within one minute of logging out the user will be prompted to extend or end their session. Note that the timeout period can be configured by any admin user with the admin role.\n\n\nA given user can only be logged into the OpenMPF from one machine using one browser at a time. If the same user attempts to log in from another machine, or another browser on the same machine, then the first user session will be terminated immediately and redirected back to the login page. This feature ensures that the user will be able to immediately log in again if the user accidentally closes the browser window or shuts down their machine without properly logging out first.\n\n\nA user may have multiple browser tabs or windows open for the same session, for example, to view the Jobs Status page and Logs page at the same time. It is not recommended that two users login using the same browser at the same time in different tabs or windows. Technically, the second user to login will take precedence, but the first user session will not appear to be terminated. Instead the first user will appear to share recently uploaded media, recent jobs, and other information with the second user. Also, when one of the users logs out in this scenario, they will both be logged out.\n\n\nOnce logged in, only the \"admin\" user has access to the full capabilities within the application, including node configuration, resetting system properties, and stopping and starting processes across the cluster. A non-admin user can view the status of processes in the Admin Console but cannot alter the node configuration or stop and start processes.\n\n\nLogging out\n\n\nTo log out a user can click the down arrow associated with the user icon at the top right hand corner of the page and then select \"Logout\":\n\n\n\n\nUser (Non-Admin) Features\n\n\nThe remainder of this document will describe the features available to a non-admin user.\n\n\nCreating Workflow Manager Jobs\n\n\nA \"job\" consists of a set of image, video, or audio files and a set of exploitation algorithms that will operate on those files.  A job is created by assigning input media file(s) to a pipeline.  A pipeline specifies the order in which processing steps are performed. Each step consists of a single task and each task consists of one or more actions which may be performed in parallel. The following sections describe the UI views associated with the different aspects of job creation and job execution.\n\n\nCreate Job\n\n\nThis is the primary page for creating jobs. Creating a job consists of uploading and selecting files as well as a pipeline and job priority.\n\n\n\n\nUploading Files\n\n\nSelecting a directory in the File Manager will display all files in that directory.  The user can use previously uploaded files, or to choose from the icon bar at the bottom of the panel:\n\n\n Create New Folder\n\n Add Local Files\n\n Upload from URL\n\n Refresh\n\n\nNote that the first three options are only available if the \"remote-media\" directory or one of its subdirectories is selected. That directory resides in the OpenMPF share directory. The full path is shown in the footer of the File Manager section.\n\n\nClicking the \"Add Local Files\" icon will display a file browser dialog so that the user can select and upload one or more files from their local machine. The files will be uploaded to the selected directory. The upload progress dialog will display a preview of each file (if possible) and whether or not each file is uploaded successfully.\n\n\nClicking the \"Create New Folder\" icon will allow the user to create a new directory within the one currently selected. If the user has selected \"remote-media\", then adding a directory called \"Test Data\" will place it within \"remote-media\". \"Test Data\" will appear as a subdirectory in the directory tree shown in the web UI. If the user then clicks on \"Test Data\" and then the \"Add Local Files\" button the user can upload files to that specific directory. In the screenshot below, \"lena.png\" has been uploaded to the parent \"remote-media\" directory.\n\n\n\n\nClicking the \"Upload from URL\" icon enables the user to specify URLs pointing to remote media. Each URL must appear on a new line. Note that if a URL to a video is submitted then it must be a direct link to the video file. Specifying a URL to a YouTube HTML page, for example, will not work.\n\n\n\n\nClicking the \"Refresh\" icon updates the displayed file tree from the file system. Use this if an external process has added or removed files to or from the underlying file system.\n\n\nCreating Jobs\n\n\nCreating a job consists of selecting files as well as a pipeline and job priority.\n\n\n\n\nFiles are selected by first clicking the name of a directory to populate the files table in the center of the UI and then clicking the checkbox next to the file. Multiple files can be selected, including files from different directories. Also, the contents of an entire directory, and its subdirectories, can be selected by clicking the checkbox next to the parent directory name. To review which files have been selected, click the \"View\" button shown to the right of the \"# Files\" indicator.  If there are many files in a directory, you may need to page through the directory using the page number buttons at the bottom of the center pane.\n\n\nYou can remove a file from the selected files by clicking on the red \"X\" for the individual file.  You can also remove multiple files by first selecting the files using the checkboxes and then clicking on the \"Remove Checked\" button.\n\n\n\n\nThe media properties can be adjusted for individual files by clicking on the \"Set Properties\" button for that file. You can modify the properties of a group of files by clicking on the \"Set properties for Checked\" after selecting multiple files.\n\n\n\n\nAfter files have been selected it's time to assign a pipeline and job priority. The \"Select a pipeline and job priority\" section is located on the right side of the screen.  Clicking on the down-arrow on the far right of the \"Select a pipeline\" area displays a drop-down menu containing the available pipelines.  Click on the desired pipeline to select it. Existing pipelines provided with the system are listed in the Default Pipelines section of this document.\n\n\n\"Select job priority\" is immediately below \"Select a pipeline\" and has a similar drop-down menu.  Clicking on the down-arrow on the right hand side of the \"Select job priority\" area displays the drop-down menu of available priorities.  Clicking on the desired priority selects it.  Priority 4 is the default value used if no priority is selected by the user. Priority 0 is the lowest priority, and priority 9 is the highest priority. When a job is executed it's divided into tasks that are each executed by a component service running on one of the nodes in the OpenMPF cluster. Each service executes tasks with the highest priority first. Note that a service will first complete the task it's currently processing before moving on to the next task. Thus, a long-running low-priority task may delay the execution of a high-priority task.\n\n\nAfter files have been selected and a pipeline and priority are assigned, clicking on the \"Create Job\" icon will start the job.  When the job starts, the user will be shown the \"Job Status\" view.\n\n\nJob Status\n\n\nThe Job Status page displays a summary of the status for all jobs run by any user in the past. The current status and progress of any running job can be monitored from this view, which is updated automatically.\n\n\n\n\nWhen a job is COMPLETE a user can view the generated JSON output object data by clicking the \"Output Objects\" button for that job. A new tab/window will open with the detection output. The detection object output displays a formatted JSON representation of the detection results.\n\n\n\n\nA user can click the \"Cancel\" button to attempt to cancel the execution of a job before it completes. Note that if a service is currently processing part of a job, for example, a video segment that's part of a larger video file, then it will continue to process that part of the job until it completes or there is an error. The act of cancelling a job will prevent other parts of that job from being processed. Thus, if the \"Cancel\" button is clicked late into the job execution, or if each part of the job is already being processed by services executing in parallel, it may have no effect. Also, if the video segment size is set to a very large number, and the detection being performed is slow, then cancelling a job could take awhile.\n\n\nA user can click the \"Resubmit\" button to execute a job again. The new job execution will retain the same job id and all generated artifacts, marked up media, and detection objects will be replaced with the new results. The results of the previous job execution will no longer be available. Note that the user has the option to change the job priority when resubmitting a job.\n\n\nYou can view the results of any Media Markup by clicking on the \"Media\" button for that job. This view will display the path of the source medium and the marked up output path of any media processed using a pipeline that contains a markup action. Clicking an image will display a popup with the marked up image. You cannot view a preview for marked up videos. In any case, the marked up data can be downloaded to the machine running the web browser by clicking the \"Download\" button.\n\n\n\n\nCreate Custom Pipelines\n\n\nA pipeline consists of a series of tasks executed sequentially. A task consists of a single action or a set of two or more actions performed in parallel. An action is the execution of an algorithm. The ability to arrange tasks and actions in various ways provides a great deal of flexibility when creating pipelines. Users may combine pre-existing tasks in different ways, or create new tasks based on the pre-existing actions.\n\n\nSelecting \"Pipelines\" from the \"Configuration\" dropdown menu in the top menu bar brings up the Pipeline Creation View, which enables users to create new pipelines. To create a new action, the user can scroll to the \"Create A New Action\" section of the page and select the desired algorithm from the \"Select an Algorithm\" dropdown menu:\n\n\n\n\nSelecting an algorithm will bring up a scrollable table of properties associated with the algorithm, including each property's name, description, data type, and an editable field allowing the user to set a custom value. The user may enter values for only those properties that they wish to change; any property value fields left blank will result in default values being used for those properties. For example, a custom action may be created based on the OpenCV face detection component to scan for faces equal to or exceeding a size of 100x100 pixels.\n\n\nWhen done editing the property values, the user can click the \"Create Action\" button, enter a name and description for the action (both are required), and then click the \"Create\" button. The action will then be listed in the \"Available Actions\" table and also in the \"Select an Action\" dropdown menu used for task creation.\n\n\n\n\nTo create a new task, the user can scroll to the \"Create A New Task\" section of the page:\n\n\n\n\nThe user can use the \"Select an Action\" dropdown menu to select the desired action and then click \"Add Action to Task\". The user can follow this procedure to add additional actions to the task, if desired. Clicking on the \"Remove\" button next to an added action will remove it from the task. When the user is finished adding actions the user can click \"Create Task\", enter a name and description for the task (both are required), and then click the \"Create\" button. The task will be listed in the \"Available Tasks\" table as well as in the \"Select a Task\" dropdown menu used for pipeline creation.\n\n\n\n\nTo build a new pipeline, the user can scroll down to the \"Create A New Pipeline\" section of the page:\n\n\n\n\nThe user can use the \"Select a Task\" dropdown menu to select the first task and then click \"Add Task to Pipeline\". The user can follow this procedure to add additional tasks to the pipeline, if desired. Clicking on the \"Remove\" button next to an added task will remove it from the pipeline. When the user is finished adding tasks the user can click \"Create Pipeline\", enter a name and description for the pipeline (both are required), and then click the \"Create\" button. The pipeline will be listed in the \"Available Pipelines\" table.\n\n\n\n\nAll pipelines successfully created in this view will also appear in the pipeline drop down selection menus on any job creation page:\n\n\n\n\n\n\nNOTE: Pipeline, task, and action names are case-insensitive. All letters will be converted to uppercase.\n\n\n\n\nLogs\n\n\nThis page allows a user to view the various log files that are generated by system processes running on the various nodes in the OpenMPF cluster. A log file can be selected by first selecting a host from the \"Available Hosts\" drop-down and then selecting a log file from the \"Available Logs\" drop-down. The information in the log can be filtered for display based on the following log levels:  ALL, TRACE, DEBUG, INFO, WARN, ERROR, or FATAL.  Choosing a successive log level displays all information at that level and levels below (e.g., choosing WARN will cause all WARN, INFO, DEBUG, and TRACE information to be displayed, but will filter out ERROR and FATAL information).\n\n\n\n\nIn general, all services of the same component type running on the same node write log messages to the same file. For example, all OCV face detection services on somehost-7-mpfd2 write log messages to the same \"ocv-face-detection\" log file. All OCV face detection services on somehost-7-mpfd3 write log messages to a different \"ocv-face-detection\" log file.\n\n\nNote that only the master node will have the \"workflow-manager\" log. This is because the workflow manager only runs on the master node. The same is true for the \"activemq\" and \"tomcat\" logs.\n\n\nThe \"node-manager-startup\" and \"node-manager\" logs will appear for every node in the OpenMPF cluster. The \"node-manager-startup\" log captures information about the nodemanager startup process, such as if any errors occurred. The \"node-manager\" log captures information about node manager execution, such as starting and stopping services.\n\n\nThe \"detection\" log captures information about initializing C++ detection components and how they handle job request and response messages.\n\n\nNode Configuration and Status\n\n\nThis page allows a user to view the various service processes running on each node in the OpenMPF cluster. Each node shows information about the current status of each service, if it is unlaunchable due to an underlying error, and how many services are running for each node. If a service is unlaunchable, it will be indicated using a red status icon (not shown). Note that services are grouped by component type. Click the chevron \"\n\" to expand a service group to view the individual services.\n\n\n\n\nEach service is given a unique number to distinguish between multiple instances of the same service running on the same node. For example, if there are two instances of the OpenCV face detection service running on somehost-7-mpfd2 then the first one will have a name of Service 1 and the second one will be Service 2 (not shown).\n\n\nProperties Settings\n\n\nThis page allows a user to view the various OpenMPF properties configured automatically or by an admin user:\n\n\n\n\nStatistics\n\n\nThe \"Jobs\" tab on this page allows a user to view a bar graph representing the time it took to execute the longest running job for a given pipeline. Pipelines that do not have bars have not been used to run any jobs yet. Job statistics are preserved when the workflow manager is restarted.\n\n\n\n\nFor example, the DLIB FACE DETECTION PIPELINE was run twice. Note that the Y-axis in the bar graph has a logarithmic scale. Hovering the mouse over any bar in the graph will show more information. Information about each pipeline is listed below the graph.\n\n\nThe \"Processes\" tab on this page allows a user to view a table with information about the runtime of various internal workflow manager operations. The \"Count\" field represents the number of times each operation was run. The min, max, and mean are calculated over the set of times each operation was performed. Runtime information is reset when the workflow manager is restarted.\n\n\n\n\nREST API\n\n\nThis page allows a user to try out the various REST API endpoints provided by the workflow manager. It is intended to serve as a learning tool for technical users who wish to design and build systems that interact with the OpenMPF.\n\n\nAfter selecting a functional category, such as \"meta\", \"jobs\", \"statistics\", \"nodes\", \"pipelines\", or \"system-message\", each REST endpoint for that category is shown in a list. Selecting one of them will cause it to expand and reveal more information about the request and response structures. If the request takes any parameters then a section will appear that allows the user to manually specify them.\n\n\n\n\nIn the example above, the \"/rest/jobs/{id}\" endpoint was selected. It takes a required \"id\" parameter that corresponds to a previously run job and returns a JSON representation of that job's information. The screenshot below shows the result of specifying an \"id\" of \"1\", providing the \"mpf\" user credentials when prompted, and then clicking the \"Try it out!\" button:\n\n\n\n\nThe HTTP response information is shown below the \"Try it out!\" button. Note that the structure of the \"Response Body\" is the same as the response model shown in the \"Response Class\" directly underneath the \"/rest/jobs/{id}\" label.\n\n\nDetection Chaining\n\n\nThe OpenMPF has the ability to chain detection tasks together in a detection pipeline. As each detection stage in the pipeline completes, the volume of data to be processed in the next stage may be reduced. Generally, any detection tasks executed prior to the final detection task in the pipeline are referred to as preprocessors or filters. For example, consider the following pipeline which demonstrates the use of a motion preprocessor:\n\n\n\n\nIn the pipeline above, the motion preprocessor reduces the volume of data which is passed to the face detector. This is particularly useful when the input media collection contains videos captured by a fixed-location camera.  For example, a camera targeting a chokepoint such as a hallway door. The motion preprocessor will filter the input media so that only regions of video containing motion are passed on to the face detector.\n\n\nDetection pipelines may be created with, or without, preprocessors and filters using the Create Custom Pipelines view.\n\n\n\n\nWARNING: Preprocessors and filters may ultimately eliminate the entirety of a media file. When an entire media file is eliminated, none of the subsequent stages in the pipeline will operate on that file. Therefore, it is important to consider the consequences of using preprocessors/filters. For example, when the motion detection receives an image or audio file, its default behavior is to return a response indicating that the file did not contain any motion tracks. If the pipeline continued to face detection then none of the image files would be eligible for that kind of detection.\n\n\n\n\n\"USE_PREPROCESSOR\" Property\n\n\nIn order to mitigate the risk of eliminating useful media files simply because they are not supported by a detector using its default settings, some algorithms expose a \"USE_PREPROCESSOR\" property. When a user creates an action based on a detector with this property, the user may assign this property a nonzero value in order to indicate that the detector should behave as a preprocessor as opposed to a filter. When acting as a preprocessor, a detector will not emit an empty detection set when provided with an unsupported media type, rather it will return a single track spanning the duration of the media file. Thus, when configured with the \"USE_PREPROCESSOR\" setting, the motion detector will not prevent images from passing on to the next stage in the pipeline, for example.\n\n\nSegmenting Media\n\n\nThe OpenMPF allows users to configure video segmenting properties for actions in a pipeline. Audio files (which do not have the concept of \"frames\") and image files (which are treated like single-frame videos) are not affected by these properties.\n\n\nSegmenting is performed before a detection action in order to split work across the available detection services running on the various nodes in the OpenMPF cluster. In general, each instance of a detection service can process one video segment at a time. Multiple services can process separate segments at the same time, thus enabling parallel processing. There are two fundamental segmenting scenarios:\n\n\n\n\nSegmenting must be performed on a video which has not passed through a preprocessor or filter.\n\n\nSegmenting must be performed on a video which has passed through a preprocessor or filter.\n\n\n\n\nIn the first scenario the segmenting logic is less complex. The segmenter will create a supersegment corresponding to the entire length of the video (in frames), and it will then divide the supersegment into segments which respect to the provided \"TARGET_SEGMENT_LENGTH\" and \"MIN_SEGMENT_LENGTH\" properties.\n\n\nIn the second scenario the segmenting logic is more complex. The segmenter first examines the start and stop times associated with all of the overlapping tracks produced by the previous detection action in the pipeline and proceeds to merge those intervals and segment the result. The goal is to generate a minimum number of segments that don't include unnecessary frames (frames that don't belong to any tracks). For example:\n\n\n\n\n\"TARGET_SEGMENT_LENGTH\" Property\n\n\nThis property indicates the preferred number of frames which will be provided to the detection component. For example, a value of \"100\" indicates that the input video should be split into 100-frame segments. Note that the properties \"MIN_SEGMENT_LENGTH\" and \"MIN_GAP_BETWEEN_SEGMENTS\" may ultimately cause segments to vary from the preferred segment size.\n\n\n\"MIN_SEGMENT_LENGTH\" Property\n\n\nIf a segment length is less than this value, the segment will be merged into the segment that precedes it. If no segment precedes it, the short segment will stand on its own. Short segments are not discarded.\n\n\nExample 1: Adjacent Segment Present\n\n\n\n\n\n\nIn this example, a preprocessor has completed and produced a single track.\n\n\nThe next detection action specifies the following parameters:\n\n\n\"TARGET_SEGMENT_LENGTH\" = 100\n\n\n\"MIN_SEGMENT_LENGTH\" = 75\n\n\n\n\n\n\nThree segments are initially produced from the input track with lengths corresponding to 100 frames, 100 frames, and 50 frames.\n\n\nSince segment 3 is not at least the minimum specified segment length, it is merged with segment 2.\n\n\nUltimately, two segments are produced.\n\n\n\n\nExample 2: No Adjacent Segment\n\n\n\n\n\n\nIn this example, a preprocessor has completed and produced two non-overlapping tracks.\n\n\nThe next detection action specifies the following parameters:\n\n\n\"TARGET_SEGMENT_LENGTH\" = 100\n\n\n\"MIN_SEGMENT_LENGTH\" = 75\n\n\n\"MIN_GAP_BETWEEN_SEGMENTS\" = 50\n\n\n\n\n\n\nThe segmenter begins by merging any segments which are less than \"MIN_GAP_BETWEEN_SEGMENTS\" apart. There are none.\n\n\nThe segmenter then splits the existing segments using the \"MIN_SEGMENT_LENGTH\" and \"TARGET_SEGMENT_LENGTH\" values.\n\n\nThe segmenter iterates through each segment produced. If the segment satisfies the minimum length constraint, it moves to the next segment.\n\n\nWhen it reaches the third segment and finds the length of 50 frames is not at least the minimum length, it merges that segment with the previous adjacent segment.\n\n\nWhen it reaches the final segment and finds that the length of 25 frames is not at least the minimum length, it creates a short segment since there is no adjacent preceding segment to merge it with.\n\n\n\n\n\n\nUltimately, three segments are produced.\n\n\n\n\n\"MIN_GAP_BETWEEN_SEGMENTS\" Property\n\n\nThis property is important to pipelines which contain preprocessors or filters and controls the minimum gap which must appear between consecutive segments. The purpose of this property is to prevent scenarios where a preprocessor or filter produces a large number of short segments separated by only a few frames. By merging the segments together prior to performing further segmentation, the number of work units produced by the segmenting plan can be reduced, thereby reducing pipeline execution time.\n\n\nConsider the following diagram, which further illustrates the purpose of this property:\n\n\n\n\n\n\nThe user submits a video to a pipeline containing a motion preprocessor followed by another extractor (e.g., face).\n\n\nThe video is initially split into segments using the properties provided by the motion preprocessor. Specifically, the preprocessor action specifies the following parameters and four segments are produced:\n\n\n\"TARGET_SEGMENT_LENGTH\" = \"250\"\n\n\n\"MIN_SEGMENT_LENGTH\" = \"150\"\n\n\n\"MERGE_TRACKS\" = \"true\"\n\n\n\n\n\n\nThe segments are submitted to the motion preprocessor, and five distinct and non-overlapping tracks are returned based on the frames of the segments in which motion is detected.\n\n\nBecause the \"MERGE_TRACKS\" property is set to \"true\", tracks are merged across segment boundaries if applicable.\n   This rule is applied to each pair of tracks that are only one frame apart (adjacent). Consequently, only three\n   tracks are ultimately derived from the video. (The number of tracks is reduced from five to three between the\n   \"Preprocessor\" and \"Track Merger\" phases of the diagram.) When two tracks are merged, the confidence value will be\n   set to the maximum confidence value of the two tracks and their track properties will be merged. If the two tracks\n   both have a track property with the same name but different values, the values will be concatenated with a\n   semicolon as the separator.\n\n\nThe non-overlapping tracks are then used to form the video segments for the next detection action. This action specifies the following parameters:\n\n\n\"TARGET_SEGMENT_LENGTH\" = \"75\"\n\n\n\"MIN_SEGMENT_LENGTH\" = \"26\"\n\n\n\"MIN_GAP_BETWEEN_SEGMENTS\" = \"100\"\n\n\n\n\n\n\nThe segmenting logic merges tracks which are less than \"MIN_GAP_BETWEEN_SEGMENTS\" frames apart into one long segment. Once all tracks have been merged, each track is segmented with respect to the provided \"TARGET_SEGMENT_LENGTH\" and \"MIN_SEGMENT_LENGTH\" properties. Ultimately, ten segments are produced. (Track #1 and Track #2 in the \"Track Merger\" phase of the diagram are combined, which is why Segment #3 in the \"Segmenter\" phase of the diagram includes the 25 frames that span the gap between those two tracks.)", 
            "title": "User Guide"
        }, 
        {
            "location": "/User-Guide/index.html#general", 
            "text": "", 
            "title": "General"
        }, 
        {
            "location": "/User-Guide/index.html#accessing-the-web-ui", 
            "text": "On the server hosting the Open Media Processing Framework (OpenMPF), the Web UI is accessible at http://localhost:8080/workflow-manager. To access it from other machines, substitute the hostname or IP address of the master node server in place of \"localhost\".  The OpenMPF user interface (UI) was designed and tested for use with Chrome and FireFox. It has not been tested with other browsers. Attempting to use an unsupported browser will result in a warning.", 
            "title": "Accessing the Web UI"
        }, 
        {
            "location": "/User-Guide/index.html#logging-in", 
            "text": "The OpenMPF Web UI requires user authentication and provides two default accounts: \"mpf\" and \"admin\". The password for the \"mpf\" user is \"mpf123\". These accounts are used to assign user or admin roles for OpenMPF cluster management. Note that an administrator can remove these accounts and/or add new ones using a command line tool. Refer to the  Admin Guide  for features available to an admin user.   The landing page for a user is the Job Status page:   Logging in starts a user session. By default, after 30 minutes of inactivity the user will automatically be logged out of the system. Within one minute of logging out the user will be prompted to extend or end their session. Note that the timeout period can be configured by any admin user with the admin role.  A given user can only be logged into the OpenMPF from one machine using one browser at a time. If the same user attempts to log in from another machine, or another browser on the same machine, then the first user session will be terminated immediately and redirected back to the login page. This feature ensures that the user will be able to immediately log in again if the user accidentally closes the browser window or shuts down their machine without properly logging out first.  A user may have multiple browser tabs or windows open for the same session, for example, to view the Jobs Status page and Logs page at the same time. It is not recommended that two users login using the same browser at the same time in different tabs or windows. Technically, the second user to login will take precedence, but the first user session will not appear to be terminated. Instead the first user will appear to share recently uploaded media, recent jobs, and other information with the second user. Also, when one of the users logs out in this scenario, they will both be logged out.  Once logged in, only the \"admin\" user has access to the full capabilities within the application, including node configuration, resetting system properties, and stopping and starting processes across the cluster. A non-admin user can view the status of processes in the Admin Console but cannot alter the node configuration or stop and start processes.", 
            "title": "Logging In"
        }, 
        {
            "location": "/User-Guide/index.html#logging-out", 
            "text": "To log out a user can click the down arrow associated with the user icon at the top right hand corner of the page and then select \"Logout\":", 
            "title": "Logging out"
        }, 
        {
            "location": "/User-Guide/index.html#user-non-admin-features", 
            "text": "The remainder of this document will describe the features available to a non-admin user.", 
            "title": "User (Non-Admin) Features"
        }, 
        {
            "location": "/User-Guide/index.html#creating-workflow-manager-jobs", 
            "text": "A \"job\" consists of a set of image, video, or audio files and a set of exploitation algorithms that will operate on those files.  A job is created by assigning input media file(s) to a pipeline.  A pipeline specifies the order in which processing steps are performed. Each step consists of a single task and each task consists of one or more actions which may be performed in parallel. The following sections describe the UI views associated with the different aspects of job creation and job execution.", 
            "title": "Creating Workflow Manager Jobs"
        }, 
        {
            "location": "/User-Guide/index.html#create-job", 
            "text": "This is the primary page for creating jobs. Creating a job consists of uploading and selecting files as well as a pipeline and job priority.", 
            "title": "Create Job"
        }, 
        {
            "location": "/User-Guide/index.html#uploading-files", 
            "text": "Selecting a directory in the File Manager will display all files in that directory.  The user can use previously uploaded files, or to choose from the icon bar at the bottom of the panel:   Create New Folder  Add Local Files  Upload from URL  Refresh  Note that the first three options are only available if the \"remote-media\" directory or one of its subdirectories is selected. That directory resides in the OpenMPF share directory. The full path is shown in the footer of the File Manager section.  Clicking the \"Add Local Files\" icon will display a file browser dialog so that the user can select and upload one or more files from their local machine. The files will be uploaded to the selected directory. The upload progress dialog will display a preview of each file (if possible) and whether or not each file is uploaded successfully.  Clicking the \"Create New Folder\" icon will allow the user to create a new directory within the one currently selected. If the user has selected \"remote-media\", then adding a directory called \"Test Data\" will place it within \"remote-media\". \"Test Data\" will appear as a subdirectory in the directory tree shown in the web UI. If the user then clicks on \"Test Data\" and then the \"Add Local Files\" button the user can upload files to that specific directory. In the screenshot below, \"lena.png\" has been uploaded to the parent \"remote-media\" directory.   Clicking the \"Upload from URL\" icon enables the user to specify URLs pointing to remote media. Each URL must appear on a new line. Note that if a URL to a video is submitted then it must be a direct link to the video file. Specifying a URL to a YouTube HTML page, for example, will not work.   Clicking the \"Refresh\" icon updates the displayed file tree from the file system. Use this if an external process has added or removed files to or from the underlying file system.", 
            "title": "Uploading Files"
        }, 
        {
            "location": "/User-Guide/index.html#creating-jobs", 
            "text": "Creating a job consists of selecting files as well as a pipeline and job priority.   Files are selected by first clicking the name of a directory to populate the files table in the center of the UI and then clicking the checkbox next to the file. Multiple files can be selected, including files from different directories. Also, the contents of an entire directory, and its subdirectories, can be selected by clicking the checkbox next to the parent directory name. To review which files have been selected, click the \"View\" button shown to the right of the \"# Files\" indicator.  If there are many files in a directory, you may need to page through the directory using the page number buttons at the bottom of the center pane.  You can remove a file from the selected files by clicking on the red \"X\" for the individual file.  You can also remove multiple files by first selecting the files using the checkboxes and then clicking on the \"Remove Checked\" button.   The media properties can be adjusted for individual files by clicking on the \"Set Properties\" button for that file. You can modify the properties of a group of files by clicking on the \"Set properties for Checked\" after selecting multiple files.   After files have been selected it's time to assign a pipeline and job priority. The \"Select a pipeline and job priority\" section is located on the right side of the screen.  Clicking on the down-arrow on the far right of the \"Select a pipeline\" area displays a drop-down menu containing the available pipelines.  Click on the desired pipeline to select it. Existing pipelines provided with the system are listed in the Default Pipelines section of this document.  \"Select job priority\" is immediately below \"Select a pipeline\" and has a similar drop-down menu.  Clicking on the down-arrow on the right hand side of the \"Select job priority\" area displays the drop-down menu of available priorities.  Clicking on the desired priority selects it.  Priority 4 is the default value used if no priority is selected by the user. Priority 0 is the lowest priority, and priority 9 is the highest priority. When a job is executed it's divided into tasks that are each executed by a component service running on one of the nodes in the OpenMPF cluster. Each service executes tasks with the highest priority first. Note that a service will first complete the task it's currently processing before moving on to the next task. Thus, a long-running low-priority task may delay the execution of a high-priority task.  After files have been selected and a pipeline and priority are assigned, clicking on the \"Create Job\" icon will start the job.  When the job starts, the user will be shown the \"Job Status\" view.", 
            "title": "Creating Jobs"
        }, 
        {
            "location": "/User-Guide/index.html#job-status", 
            "text": "The Job Status page displays a summary of the status for all jobs run by any user in the past. The current status and progress of any running job can be monitored from this view, which is updated automatically.   When a job is COMPLETE a user can view the generated JSON output object data by clicking the \"Output Objects\" button for that job. A new tab/window will open with the detection output. The detection object output displays a formatted JSON representation of the detection results.   A user can click the \"Cancel\" button to attempt to cancel the execution of a job before it completes. Note that if a service is currently processing part of a job, for example, a video segment that's part of a larger video file, then it will continue to process that part of the job until it completes or there is an error. The act of cancelling a job will prevent other parts of that job from being processed. Thus, if the \"Cancel\" button is clicked late into the job execution, or if each part of the job is already being processed by services executing in parallel, it may have no effect. Also, if the video segment size is set to a very large number, and the detection being performed is slow, then cancelling a job could take awhile.  A user can click the \"Resubmit\" button to execute a job again. The new job execution will retain the same job id and all generated artifacts, marked up media, and detection objects will be replaced with the new results. The results of the previous job execution will no longer be available. Note that the user has the option to change the job priority when resubmitting a job.  You can view the results of any Media Markup by clicking on the \"Media\" button for that job. This view will display the path of the source medium and the marked up output path of any media processed using a pipeline that contains a markup action. Clicking an image will display a popup with the marked up image. You cannot view a preview for marked up videos. In any case, the marked up data can be downloaded to the machine running the web browser by clicking the \"Download\" button.", 
            "title": "Job Status"
        }, 
        {
            "location": "/User-Guide/index.html#create-custom-pipelines", 
            "text": "A pipeline consists of a series of tasks executed sequentially. A task consists of a single action or a set of two or more actions performed in parallel. An action is the execution of an algorithm. The ability to arrange tasks and actions in various ways provides a great deal of flexibility when creating pipelines. Users may combine pre-existing tasks in different ways, or create new tasks based on the pre-existing actions.  Selecting \"Pipelines\" from the \"Configuration\" dropdown menu in the top menu bar brings up the Pipeline Creation View, which enables users to create new pipelines. To create a new action, the user can scroll to the \"Create A New Action\" section of the page and select the desired algorithm from the \"Select an Algorithm\" dropdown menu:   Selecting an algorithm will bring up a scrollable table of properties associated with the algorithm, including each property's name, description, data type, and an editable field allowing the user to set a custom value. The user may enter values for only those properties that they wish to change; any property value fields left blank will result in default values being used for those properties. For example, a custom action may be created based on the OpenCV face detection component to scan for faces equal to or exceeding a size of 100x100 pixels.  When done editing the property values, the user can click the \"Create Action\" button, enter a name and description for the action (both are required), and then click the \"Create\" button. The action will then be listed in the \"Available Actions\" table and also in the \"Select an Action\" dropdown menu used for task creation.   To create a new task, the user can scroll to the \"Create A New Task\" section of the page:   The user can use the \"Select an Action\" dropdown menu to select the desired action and then click \"Add Action to Task\". The user can follow this procedure to add additional actions to the task, if desired. Clicking on the \"Remove\" button next to an added action will remove it from the task. When the user is finished adding actions the user can click \"Create Task\", enter a name and description for the task (both are required), and then click the \"Create\" button. The task will be listed in the \"Available Tasks\" table as well as in the \"Select a Task\" dropdown menu used for pipeline creation.   To build a new pipeline, the user can scroll down to the \"Create A New Pipeline\" section of the page:   The user can use the \"Select a Task\" dropdown menu to select the first task and then click \"Add Task to Pipeline\". The user can follow this procedure to add additional tasks to the pipeline, if desired. Clicking on the \"Remove\" button next to an added task will remove it from the pipeline. When the user is finished adding tasks the user can click \"Create Pipeline\", enter a name and description for the pipeline (both are required), and then click the \"Create\" button. The pipeline will be listed in the \"Available Pipelines\" table.   All pipelines successfully created in this view will also appear in the pipeline drop down selection menus on any job creation page:    NOTE: Pipeline, task, and action names are case-insensitive. All letters will be converted to uppercase.", 
            "title": "Create Custom Pipelines"
        }, 
        {
            "location": "/User-Guide/index.html#logs", 
            "text": "This page allows a user to view the various log files that are generated by system processes running on the various nodes in the OpenMPF cluster. A log file can be selected by first selecting a host from the \"Available Hosts\" drop-down and then selecting a log file from the \"Available Logs\" drop-down. The information in the log can be filtered for display based on the following log levels:  ALL, TRACE, DEBUG, INFO, WARN, ERROR, or FATAL.  Choosing a successive log level displays all information at that level and levels below (e.g., choosing WARN will cause all WARN, INFO, DEBUG, and TRACE information to be displayed, but will filter out ERROR and FATAL information).   In general, all services of the same component type running on the same node write log messages to the same file. For example, all OCV face detection services on somehost-7-mpfd2 write log messages to the same \"ocv-face-detection\" log file. All OCV face detection services on somehost-7-mpfd3 write log messages to a different \"ocv-face-detection\" log file.  Note that only the master node will have the \"workflow-manager\" log. This is because the workflow manager only runs on the master node. The same is true for the \"activemq\" and \"tomcat\" logs.  The \"node-manager-startup\" and \"node-manager\" logs will appear for every node in the OpenMPF cluster. The \"node-manager-startup\" log captures information about the nodemanager startup process, such as if any errors occurred. The \"node-manager\" log captures information about node manager execution, such as starting and stopping services.  The \"detection\" log captures information about initializing C++ detection components and how they handle job request and response messages.", 
            "title": "Logs"
        }, 
        {
            "location": "/User-Guide/index.html#node-configuration-and-status", 
            "text": "This page allows a user to view the various service processes running on each node in the OpenMPF cluster. Each node shows information about the current status of each service, if it is unlaunchable due to an underlying error, and how many services are running for each node. If a service is unlaunchable, it will be indicated using a red status icon (not shown). Note that services are grouped by component type. Click the chevron \" \" to expand a service group to view the individual services.   Each service is given a unique number to distinguish between multiple instances of the same service running on the same node. For example, if there are two instances of the OpenCV face detection service running on somehost-7-mpfd2 then the first one will have a name of Service 1 and the second one will be Service 2 (not shown).", 
            "title": "Node Configuration and Status"
        }, 
        {
            "location": "/User-Guide/index.html#properties-settings", 
            "text": "This page allows a user to view the various OpenMPF properties configured automatically or by an admin user:", 
            "title": "Properties Settings"
        }, 
        {
            "location": "/User-Guide/index.html#statistics", 
            "text": "The \"Jobs\" tab on this page allows a user to view a bar graph representing the time it took to execute the longest running job for a given pipeline. Pipelines that do not have bars have not been used to run any jobs yet. Job statistics are preserved when the workflow manager is restarted.   For example, the DLIB FACE DETECTION PIPELINE was run twice. Note that the Y-axis in the bar graph has a logarithmic scale. Hovering the mouse over any bar in the graph will show more information. Information about each pipeline is listed below the graph.  The \"Processes\" tab on this page allows a user to view a table with information about the runtime of various internal workflow manager operations. The \"Count\" field represents the number of times each operation was run. The min, max, and mean are calculated over the set of times each operation was performed. Runtime information is reset when the workflow manager is restarted.", 
            "title": "Statistics"
        }, 
        {
            "location": "/User-Guide/index.html#rest-api", 
            "text": "This page allows a user to try out the various REST API endpoints provided by the workflow manager. It is intended to serve as a learning tool for technical users who wish to design and build systems that interact with the OpenMPF.  After selecting a functional category, such as \"meta\", \"jobs\", \"statistics\", \"nodes\", \"pipelines\", or \"system-message\", each REST endpoint for that category is shown in a list. Selecting one of them will cause it to expand and reveal more information about the request and response structures. If the request takes any parameters then a section will appear that allows the user to manually specify them.   In the example above, the \"/rest/jobs/{id}\" endpoint was selected. It takes a required \"id\" parameter that corresponds to a previously run job and returns a JSON representation of that job's information. The screenshot below shows the result of specifying an \"id\" of \"1\", providing the \"mpf\" user credentials when prompted, and then clicking the \"Try it out!\" button:   The HTTP response information is shown below the \"Try it out!\" button. Note that the structure of the \"Response Body\" is the same as the response model shown in the \"Response Class\" directly underneath the \"/rest/jobs/{id}\" label.", 
            "title": "REST API"
        }, 
        {
            "location": "/User-Guide/index.html#detection-chaining", 
            "text": "The OpenMPF has the ability to chain detection tasks together in a detection pipeline. As each detection stage in the pipeline completes, the volume of data to be processed in the next stage may be reduced. Generally, any detection tasks executed prior to the final detection task in the pipeline are referred to as preprocessors or filters. For example, consider the following pipeline which demonstrates the use of a motion preprocessor:   In the pipeline above, the motion preprocessor reduces the volume of data which is passed to the face detector. This is particularly useful when the input media collection contains videos captured by a fixed-location camera.  For example, a camera targeting a chokepoint such as a hallway door. The motion preprocessor will filter the input media so that only regions of video containing motion are passed on to the face detector.  Detection pipelines may be created with, or without, preprocessors and filters using the Create Custom Pipelines view.   WARNING: Preprocessors and filters may ultimately eliminate the entirety of a media file. When an entire media file is eliminated, none of the subsequent stages in the pipeline will operate on that file. Therefore, it is important to consider the consequences of using preprocessors/filters. For example, when the motion detection receives an image or audio file, its default behavior is to return a response indicating that the file did not contain any motion tracks. If the pipeline continued to face detection then none of the image files would be eligible for that kind of detection.", 
            "title": "Detection Chaining"
        }, 
        {
            "location": "/User-Guide/index.html#use_preprocessor-property", 
            "text": "In order to mitigate the risk of eliminating useful media files simply because they are not supported by a detector using its default settings, some algorithms expose a \"USE_PREPROCESSOR\" property. When a user creates an action based on a detector with this property, the user may assign this property a nonzero value in order to indicate that the detector should behave as a preprocessor as opposed to a filter. When acting as a preprocessor, a detector will not emit an empty detection set when provided with an unsupported media type, rather it will return a single track spanning the duration of the media file. Thus, when configured with the \"USE_PREPROCESSOR\" setting, the motion detector will not prevent images from passing on to the next stage in the pipeline, for example.", 
            "title": "\"USE_PREPROCESSOR\" Property"
        }, 
        {
            "location": "/User-Guide/index.html#segmenting-media", 
            "text": "The OpenMPF allows users to configure video segmenting properties for actions in a pipeline. Audio files (which do not have the concept of \"frames\") and image files (which are treated like single-frame videos) are not affected by these properties.  Segmenting is performed before a detection action in order to split work across the available detection services running on the various nodes in the OpenMPF cluster. In general, each instance of a detection service can process one video segment at a time. Multiple services can process separate segments at the same time, thus enabling parallel processing. There are two fundamental segmenting scenarios:   Segmenting must be performed on a video which has not passed through a preprocessor or filter.  Segmenting must be performed on a video which has passed through a preprocessor or filter.   In the first scenario the segmenting logic is less complex. The segmenter will create a supersegment corresponding to the entire length of the video (in frames), and it will then divide the supersegment into segments which respect to the provided \"TARGET_SEGMENT_LENGTH\" and \"MIN_SEGMENT_LENGTH\" properties.  In the second scenario the segmenting logic is more complex. The segmenter first examines the start and stop times associated with all of the overlapping tracks produced by the previous detection action in the pipeline and proceeds to merge those intervals and segment the result. The goal is to generate a minimum number of segments that don't include unnecessary frames (frames that don't belong to any tracks). For example:", 
            "title": "Segmenting Media"
        }, 
        {
            "location": "/User-Guide/index.html#target_segment_length-property", 
            "text": "This property indicates the preferred number of frames which will be provided to the detection component. For example, a value of \"100\" indicates that the input video should be split into 100-frame segments. Note that the properties \"MIN_SEGMENT_LENGTH\" and \"MIN_GAP_BETWEEN_SEGMENTS\" may ultimately cause segments to vary from the preferred segment size.", 
            "title": "\"TARGET_SEGMENT_LENGTH\" Property"
        }, 
        {
            "location": "/User-Guide/index.html#min_segment_length-property", 
            "text": "If a segment length is less than this value, the segment will be merged into the segment that precedes it. If no segment precedes it, the short segment will stand on its own. Short segments are not discarded.", 
            "title": "\"MIN_SEGMENT_LENGTH\" Property"
        }, 
        {
            "location": "/User-Guide/index.html#example-1-adjacent-segment-present", 
            "text": "In this example, a preprocessor has completed and produced a single track.  The next detection action specifies the following parameters:  \"TARGET_SEGMENT_LENGTH\" = 100  \"MIN_SEGMENT_LENGTH\" = 75    Three segments are initially produced from the input track with lengths corresponding to 100 frames, 100 frames, and 50 frames.  Since segment 3 is not at least the minimum specified segment length, it is merged with segment 2.  Ultimately, two segments are produced.", 
            "title": "Example 1: Adjacent Segment Present"
        }, 
        {
            "location": "/User-Guide/index.html#example-2-no-adjacent-segment", 
            "text": "In this example, a preprocessor has completed and produced two non-overlapping tracks.  The next detection action specifies the following parameters:  \"TARGET_SEGMENT_LENGTH\" = 100  \"MIN_SEGMENT_LENGTH\" = 75  \"MIN_GAP_BETWEEN_SEGMENTS\" = 50    The segmenter begins by merging any segments which are less than \"MIN_GAP_BETWEEN_SEGMENTS\" apart. There are none.  The segmenter then splits the existing segments using the \"MIN_SEGMENT_LENGTH\" and \"TARGET_SEGMENT_LENGTH\" values.  The segmenter iterates through each segment produced. If the segment satisfies the minimum length constraint, it moves to the next segment.  When it reaches the third segment and finds the length of 50 frames is not at least the minimum length, it merges that segment with the previous adjacent segment.  When it reaches the final segment and finds that the length of 25 frames is not at least the minimum length, it creates a short segment since there is no adjacent preceding segment to merge it with.    Ultimately, three segments are produced.", 
            "title": "Example 2: No Adjacent Segment"
        }, 
        {
            "location": "/User-Guide/index.html#min_gap_between_segments-property", 
            "text": "This property is important to pipelines which contain preprocessors or filters and controls the minimum gap which must appear between consecutive segments. The purpose of this property is to prevent scenarios where a preprocessor or filter produces a large number of short segments separated by only a few frames. By merging the segments together prior to performing further segmentation, the number of work units produced by the segmenting plan can be reduced, thereby reducing pipeline execution time.  Consider the following diagram, which further illustrates the purpose of this property:    The user submits a video to a pipeline containing a motion preprocessor followed by another extractor (e.g., face).  The video is initially split into segments using the properties provided by the motion preprocessor. Specifically, the preprocessor action specifies the following parameters and four segments are produced:  \"TARGET_SEGMENT_LENGTH\" = \"250\"  \"MIN_SEGMENT_LENGTH\" = \"150\"  \"MERGE_TRACKS\" = \"true\"    The segments are submitted to the motion preprocessor, and five distinct and non-overlapping tracks are returned based on the frames of the segments in which motion is detected.  Because the \"MERGE_TRACKS\" property is set to \"true\", tracks are merged across segment boundaries if applicable.\n   This rule is applied to each pair of tracks that are only one frame apart (adjacent). Consequently, only three\n   tracks are ultimately derived from the video. (The number of tracks is reduced from five to three between the\n   \"Preprocessor\" and \"Track Merger\" phases of the diagram.) When two tracks are merged, the confidence value will be\n   set to the maximum confidence value of the two tracks and their track properties will be merged. If the two tracks\n   both have a track property with the same name but different values, the values will be concatenated with a\n   semicolon as the separator.  The non-overlapping tracks are then used to form the video segments for the next detection action. This action specifies the following parameters:  \"TARGET_SEGMENT_LENGTH\" = \"75\"  \"MIN_SEGMENT_LENGTH\" = \"26\"  \"MIN_GAP_BETWEEN_SEGMENTS\" = \"100\"    The segmenting logic merges tracks which are less than \"MIN_GAP_BETWEEN_SEGMENTS\" frames apart into one long segment. Once all tracks have been merged, each track is segmented with respect to the provided \"TARGET_SEGMENT_LENGTH\" and \"MIN_SEGMENT_LENGTH\" properties. Ultimately, ten segments are produced. (Track #1 and Track #2 in the \"Track Merger\" phase of the diagram are combined, which is why Segment #3 in the \"Segmenter\" phase of the diagram includes the 25 frames that span the gap between those two tracks.)", 
            "title": "\"MIN_GAP_BETWEEN_SEGMENTS\" Property"
        }, 
        {
            "location": "/Admin-Guide/index.html", 
            "text": "NOTICE:\n This software (or technical data) was produced for the U.S. Government under contract, and is subject to the Rights in Data-General Clause 52.227-14, Alt. IV (DEC 2007).\nCopyright 2019 The MITRE Corporation. All Rights Reserved.\n\n\nIMPORTANT:\n Please refer to the \nUser Configuration\n section for changing the default user passwords.\n\n\nNOTE:\n This release contains basic support for processing video streams. Currently, the only way to make use of that functionality is through the REST API. Streaming jobs and services cannot be created or monitored through the web UI.\n\n\n\n\nWeb UI\n\n\nThe login procedure, as well as all of the pages accessible through the Workflow Manager sidebar, are the same for admin and non-admin users. Refer to the \nUser Guide\n for more information. The default account for an admin user has the username \"admin\" and password \"mpfadm\". \n\n\nWe highly recommend changing the default username and password settings for any environment which is exposed on a network, especially production environments. The default settings are public knowledge, which could be a security risk. Please refer to the \nUser Configuration\n section below.\n\n\nThis document will cover the additional functionality permitted to admin users through the Admin Console pages.\n\n\nDashboard\n\n\nThe landing page for an admin user is the Job Status page:\n\n\n\n\nThe Job Status page displays a summary of the status for all jobs run by any user in the past. The current status and progress of any running job can be monitored from this view, which is updated automatically.\n\n\nNode Configuration and Status\n\n\nThis page provides a list of all of the services that are configured to run on the OpenMPF cluster, and enables an admin user to start, stop, or restart them on an individual basis. Only an admin user can perform these actions. If a non-admin user views this page, the \"Action(s)\" column is not displayed. This page also enables an admin user to edit the configuration for all nodes in the OpenMPF cluster. A non-admin user can only view the existing configuration.\n\n\n\n\nAn admin user can add a node by using the \"Add Node\" button and selecting a node in the OpenMPF cluster from the drop-down list. You an also select to add all services at this time. A node and all if its configured services can be removed by clicking the trash can to the right of the node's hostname.\n\n\nAn admin user can add services individually by selecting the node edit button at the bottom of the node. The number of service instances can be increased or decreased by using the drop-down. Click the \"Submit\" button to save the changes.\n\n\nAny node or service changes take effect immediately, no saving is required, except for adding services.\n\n\nWhen making changes, please be aware of the following:\n\n\n\n\nIt may take a minute for the configuration to take effect on the server.\n\n\nIf you remove an existing service from a node, any job that service is processing will be stopped, and you will need to resubmit that job.\n\n\nIf you create a new node, its configuration will not take effect until the OpenMPF software is properly installed and started on the associated host.\n\n\nIf you delete a node, you will need to manually turn off the hardware running that node (deleting a node does not shut down the machine).\n\n\n\n\nProperties Settings\n\n\nThis page allows an admin user to view and edit various OpenMPF properties:\n\n\n\n\nAn admin user can click inside of the \"Value\" field for any of the properties and type a new value. Doing so will change the color of the property to orange and display an orange icon to the right of the property name.\n\n\nNote that if the admin user types in the original value of the property, or clicks the \"Reset\" button, then it will return back to the normal coloration.\n\n\nWARNING: Changing the value of these properties can prevent the workflow manager from running after the web server is restarted. Also, no validation checks are performed on the user-provided values. Proceed with caution!\n\n\nAt the bottom of the properties table is the \"Save Properties\" button. The number of modified properties is shown in parentheses. Clicking the button will make the necessary changes to the properties file on the file system, but the changes will not take effect until the workflow manager is restarted. The saved properties will be colored blue and a blue icon will be displayed to the right of the property name. Additionally, a notification will appear at the top of the page alerting all system users that a restart is required:\n\n\n\n\nComponent Registration\n\n\nThis page allows an admin user to add and remove non-default components to and from the system:\n\n\n\n\nA component package takes the form of a tar.gz file. An admin user can either drag and drop the file onto the \"Upload a new component\" dropzone area or click the dropzone area to open a file browser and select the file that way. In either case, the component will begin to be uploaded to the system. If the admin user dragged and dropped the file onto the dropzone area then the upload progress will be shown in that area. Once uploaded, the workflow manager will automatically attempt to register the component. Notification messages will appear in the upper right side of the screen to indicate success or failure if an error occurs. The \"Current Components\" table will display the component status.\n\n\n\n\nIf for some reason the component package upload succeeded but the component registration failed then the admin user will be able to click the \"Register\" button again to try to another registration attempt. For example, the admin user may do this after reviewing the workflow manager logs and resolving any issues that prevented the component from successfully registering the first time. One reason may be that a component with the same name already exists on the system. Note that an error will also occur if the top-level directory of the component package, once extracted, already exists in the /opt/mpf/plugins directory on the system.\n\n\nOnce registered, an admin user has the option to remove the component. This will unregister it and completely remove any configured services, as well as the uploaded file and its extracted contents, from the system. Also, the component algorithm as well as any actions, tasks, and pipelines specified in the component's descriptor file will be removed when the component is removed.\n\n\nWARNING: Any actions, tasks, or pipelines created through the Create Custom Pipelines page that make use of the algorithm, actions, or tasks specified in the descriptor file of the component being removed will also be removed. This is to prevent pipelines from not working properly once the component is removed.\n\n\nUser Configuration\n\n\nEvery time the Workflow Manager starts it will attempt to create accounts for the users listed in the \nuser.properties\n file. At runtime this file is extracted to \n$MPF_HOME/config\n on the machine running the Workflow Manager. For every user listed in that file, the Workflow Manager will create that user account if a user with the same name doesn't already exists in the SQL database. By default, that file contains two entries, one for the \"admin\" user with the \"mpfadm\" password, and one for a non-admin \"mpf\" user with the \"mpf123\" password.\n\n\nWe highly recommend modifying the \nuser.properties\n file with your own user entries before attempting to start the Workflow Manager for the first time. This will ensure that the default user accounts are not created. \n\n\nThe official way to deploy OpenMPF is to use the Docker container platform. If you are using Docker, please follow the instructions in the openmpf-docker \nREADME\n that explain how to use a \ndocker secret\n for your custom \nuser.properties\n file.", 
            "title": "Admin Guide"
        }, 
        {
            "location": "/Admin-Guide/index.html#web-ui", 
            "text": "The login procedure, as well as all of the pages accessible through the Workflow Manager sidebar, are the same for admin and non-admin users. Refer to the  User Guide  for more information. The default account for an admin user has the username \"admin\" and password \"mpfadm\".   We highly recommend changing the default username and password settings for any environment which is exposed on a network, especially production environments. The default settings are public knowledge, which could be a security risk. Please refer to the  User Configuration  section below.  This document will cover the additional functionality permitted to admin users through the Admin Console pages.", 
            "title": "Web UI"
        }, 
        {
            "location": "/Admin-Guide/index.html#dashboard", 
            "text": "The landing page for an admin user is the Job Status page:   The Job Status page displays a summary of the status for all jobs run by any user in the past. The current status and progress of any running job can be monitored from this view, which is updated automatically.", 
            "title": "Dashboard"
        }, 
        {
            "location": "/Admin-Guide/index.html#node-configuration-and-status", 
            "text": "This page provides a list of all of the services that are configured to run on the OpenMPF cluster, and enables an admin user to start, stop, or restart them on an individual basis. Only an admin user can perform these actions. If a non-admin user views this page, the \"Action(s)\" column is not displayed. This page also enables an admin user to edit the configuration for all nodes in the OpenMPF cluster. A non-admin user can only view the existing configuration.   An admin user can add a node by using the \"Add Node\" button and selecting a node in the OpenMPF cluster from the drop-down list. You an also select to add all services at this time. A node and all if its configured services can be removed by clicking the trash can to the right of the node's hostname.  An admin user can add services individually by selecting the node edit button at the bottom of the node. The number of service instances can be increased or decreased by using the drop-down. Click the \"Submit\" button to save the changes.  Any node or service changes take effect immediately, no saving is required, except for adding services.  When making changes, please be aware of the following:   It may take a minute for the configuration to take effect on the server.  If you remove an existing service from a node, any job that service is processing will be stopped, and you will need to resubmit that job.  If you create a new node, its configuration will not take effect until the OpenMPF software is properly installed and started on the associated host.  If you delete a node, you will need to manually turn off the hardware running that node (deleting a node does not shut down the machine).", 
            "title": "Node Configuration and Status"
        }, 
        {
            "location": "/Admin-Guide/index.html#properties-settings", 
            "text": "This page allows an admin user to view and edit various OpenMPF properties:   An admin user can click inside of the \"Value\" field for any of the properties and type a new value. Doing so will change the color of the property to orange and display an orange icon to the right of the property name.  Note that if the admin user types in the original value of the property, or clicks the \"Reset\" button, then it will return back to the normal coloration.  WARNING: Changing the value of these properties can prevent the workflow manager from running after the web server is restarted. Also, no validation checks are performed on the user-provided values. Proceed with caution!  At the bottom of the properties table is the \"Save Properties\" button. The number of modified properties is shown in parentheses. Clicking the button will make the necessary changes to the properties file on the file system, but the changes will not take effect until the workflow manager is restarted. The saved properties will be colored blue and a blue icon will be displayed to the right of the property name. Additionally, a notification will appear at the top of the page alerting all system users that a restart is required:", 
            "title": "Properties Settings"
        }, 
        {
            "location": "/Admin-Guide/index.html#component-registration", 
            "text": "This page allows an admin user to add and remove non-default components to and from the system:   A component package takes the form of a tar.gz file. An admin user can either drag and drop the file onto the \"Upload a new component\" dropzone area or click the dropzone area to open a file browser and select the file that way. In either case, the component will begin to be uploaded to the system. If the admin user dragged and dropped the file onto the dropzone area then the upload progress will be shown in that area. Once uploaded, the workflow manager will automatically attempt to register the component. Notification messages will appear in the upper right side of the screen to indicate success or failure if an error occurs. The \"Current Components\" table will display the component status.   If for some reason the component package upload succeeded but the component registration failed then the admin user will be able to click the \"Register\" button again to try to another registration attempt. For example, the admin user may do this after reviewing the workflow manager logs and resolving any issues that prevented the component from successfully registering the first time. One reason may be that a component with the same name already exists on the system. Note that an error will also occur if the top-level directory of the component package, once extracted, already exists in the /opt/mpf/plugins directory on the system.  Once registered, an admin user has the option to remove the component. This will unregister it and completely remove any configured services, as well as the uploaded file and its extracted contents, from the system. Also, the component algorithm as well as any actions, tasks, and pipelines specified in the component's descriptor file will be removed when the component is removed.  WARNING: Any actions, tasks, or pipelines created through the Create Custom Pipelines page that make use of the algorithm, actions, or tasks specified in the descriptor file of the component being removed will also be removed. This is to prevent pipelines from not working properly once the component is removed.", 
            "title": "Component Registration"
        }, 
        {
            "location": "/Admin-Guide/index.html#user-configuration", 
            "text": "Every time the Workflow Manager starts it will attempt to create accounts for the users listed in the  user.properties  file. At runtime this file is extracted to  $MPF_HOME/config  on the machine running the Workflow Manager. For every user listed in that file, the Workflow Manager will create that user account if a user with the same name doesn't already exists in the SQL database. By default, that file contains two entries, one for the \"admin\" user with the \"mpfadm\" password, and one for a non-admin \"mpf\" user with the \"mpf123\" password.  We highly recommend modifying the  user.properties  file with your own user entries before attempting to start the Workflow Manager for the first time. This will ensure that the default user accounts are not created.   The official way to deploy OpenMPF is to use the Docker container platform. If you are using Docker, please follow the instructions in the openmpf-docker  README  that explain how to use a  docker secret  for your custom  user.properties  file.", 
            "title": "User Configuration"
        }, 
        {
            "location": "/Node-Guide/index.html", 
            "text": "NOTICE:\n This software (or technical data) was produced for the U.S. Government under contract, and is subject to the Rights in Data-General Clause 52.227-14, Alt. IV (DEC 2007).\nCopyright 2019 The MITRE Corporation. All Rights Reserved.\n\n\n\n\nJGroups Communication\n\n\nOpenMPF uses the \nJGroups\n toolkit for passing messages between node-manager processes. One process runs on each OpenMPF Node Manager Docker container, and one master node-manager runs as part of the Workflow Manager web application on the Workflow Manager Docker container. Each of these containers is effectively a \"node\" in the OpenMPF cluster.\n\n\nThere are two primary aspects of JGroups that an OpenMPF administrator needs to be concerned with:\n\n\n\n\n\n\nOpenMPF uses the JGroups FILE_PING protocol for peer discovery. Each node uses files stored in \n$MPF_HOME/share/nodes/MPF_Channel\n. A node will write a file to that directory when the node-manager starts up, and read files in that directory to determine what other nodes are in the OpenMPF cluster.\n\n\n\n\n\n\nEach OpenMPF node uses network port 7800 for JGroups TCP communication. Please ensure that this port is open in the network firewall on each OpenMPF node, or the firewall is disabled.\n\n\n\n\n\n\nIf for some reason port 7800 is reserved by another process, JGroups will try the next available port, starting at 7801, then 7802, and so on. Note that within an OpenMPF \ndevelopment environment\n the Workflow Manager web application and node-manager process will run on the same machine, resulting in the use of port 7800 and 7801.\n\n\nWhen a node first starts up it will be in its own JGroups cluster. Within a minute it will be merged into the cluster with the other OpenMPF nodes. At that time it will be recognized by the Workflow Manager and become available for running services and processing jobs.", 
            "title": "Node Guide"
        }, 
        {
            "location": "/Node-Guide/index.html#jgroups-communication", 
            "text": "OpenMPF uses the  JGroups  toolkit for passing messages between node-manager processes. One process runs on each OpenMPF Node Manager Docker container, and one master node-manager runs as part of the Workflow Manager web application on the Workflow Manager Docker container. Each of these containers is effectively a \"node\" in the OpenMPF cluster.  There are two primary aspects of JGroups that an OpenMPF administrator needs to be concerned with:    OpenMPF uses the JGroups FILE_PING protocol for peer discovery. Each node uses files stored in  $MPF_HOME/share/nodes/MPF_Channel . A node will write a file to that directory when the node-manager starts up, and read files in that directory to determine what other nodes are in the OpenMPF cluster.    Each OpenMPF node uses network port 7800 for JGroups TCP communication. Please ensure that this port is open in the network firewall on each OpenMPF node, or the firewall is disabled.    If for some reason port 7800 is reserved by another process, JGroups will try the next available port, starting at 7801, then 7802, and so on. Note that within an OpenMPF  development environment  the Workflow Manager web application and node-manager process will run on the same machine, resulting in the use of port 7800 and 7801.  When a node first starts up it will be in its own JGroups cluster. Within a minute it will be merged into the cluster with the other OpenMPF nodes. At that time it will be recognized by the Workflow Manager and become available for running services and processing jobs.", 
            "title": "JGroups Communication"
        }, 
        {
            "location": "/Object-Storage-Guide/index.html", 
            "text": "NOTICE:\n This software (or technical data) was produced for the U.S. Government under contract, \nand is subject to the Rights in Data-General Clause 52.227-14, Alt. IV (DEC 2007).\nCopyright 2019 The MITRE Corporation. All Rights Reserved.\n\n\n\n\nObject Storage Overview\n\n\nBy default, OpenMPF will write markup files, JSON output objects, and extracted artifacts to directories in \n\n$MPF_HOME/share\n. For multi-node deployments, \n$MPF_HOME/share\n points to a directory on a network share. \nMost often, the share is managed by the Network File System (NFS) protocol, although using NFS is not a requirement.\n\n\nAlternatively, OpenMPF supports writing these files to an object storage server. That may be desirable in cloud \ndeployments to better support integration between systems, and/or to consolidate file storage as a cost-saving measure.\n\n\nWhen a file cannot be uploaded to the server, the Workflow Manager will fall back to storing it in \n$MPF_HOME/share\n. \nIf and when a failure occurs, the JSON output object will contain a descriptive message in the \njobWarnings\n field. \nIf the job completes without other issues, the final status will be \nCOMPLETE_WITH_WARNINGS\n.\n\n\nCommon Object Storage Properties\n\n\nThe following system properties are common to the various types of object storage solutions that OpenMPF supports:\n\n\n\n\nhttp.object.storage.upload.retry.count\n\n\nThe number of times OpenMPF will attempt to upload an object to the storage server after the first failed attempt.\n\n\nWhen using NGINX, exponential back off is used between retry attempts. There is a 500ms delay before the \n  first retry. The delay doubles for each subsequent retry.\n\n\nWhen using S3, the AWS SDK's default retry strategy is used.\n\n\n\n\n\n\n\n\nCustom NGINX HTTP Object Storage\n\n\nOpenMPF supports a custom NGINX object storage server solution. If you're interested, please contact us. \nWe can make the server-side code available upon request.\n\n\nFor those who choose to run their own custom NGINX object storage server, please configure OpenMPF by setting \nthe \nhttp.object.storage.nginx.service.uri\n property to the URI of the NGINX server. \nThe following system properties are unique to the custom NGINX object storage solution:\n\n\n\n\nhttp.object.storage.nginx.service.uri\n\n\nEnables use of NGINX when provided.\n\n\nThe URI to the custom NGINX object storage server. For example:  \nhttps://somehost:123543/somepath\n.\n\n\nYou must provide a valid value.\n\n\n\n\n\n\nhttp.object.storage.nginx.upload.thread.count\n\n\nThe number of threads used to upload objects to the storage server.\n\n\nIn general, the default value is sufficient.\n\n\n\n\n\n\nhttp.object.storage.nginx.upload.segment.size\n\n\nThe chunk size, in bytes, that is used to upload objects to the storage server.\n\n\nIn general, the default value is sufficient.\n\n\n\n\n\n\n\n\nThe NGINX object storage server will determine the sha256 hash for the file once it's been uploaded. \nIt then uses that hash to name the file and returns the file URI to OpenMPF.\n\n\nS3 Object Storage\n\n\nOpenMPF supports downloading media and uploading results to an S3 compatible server such as Ceph. \nThe use of S3 is controlled through the following job properties:\n\n\n\n\nS3_RESULTS_BUCKET\n\n\nURI to bucket where result objects should be stored. For example: \nhttp://s3host/results_bucket\n\n\nTo disable the upload of result objects, do not provide a value for this property.\n\n\n\n\n\n\nS3_UPLOAD_ONLY\n\n\nWhen true, media will not be downloaded using S3 authentication. \n  If \nS3_RESULTS_BUCKET\n is set, S3 authentication will be used to upload result objects.\n\n\nWhen false or not provided, S3 authentication will be used to download remote media. \n  S3 authentication will also be used to upload result objects if \nS3_RESULTS_BUCKET\n is set.\n\n\nIf you want to run a job where some media is in S3 and some is hosted elsewhere, \n  you can set \nS3_UPLOAD_ONLY\n to \ntrue\n as a media specific property on the media that is hosted elsewhere.\n\n\n\n\n\n\nS3_ACCESS_KEY\n\n\nThe access key that will be used when downloading and uploading to S3.\n\n\n\n\n\n\nS3_SECRET_KEY\n\n\nThe secret key that will be used when downloading and uploading to S3.", 
            "title": "Object Storage Guide"
        }, 
        {
            "location": "/Object-Storage-Guide/index.html#object-storage-overview", 
            "text": "By default, OpenMPF will write markup files, JSON output objects, and extracted artifacts to directories in  $MPF_HOME/share . For multi-node deployments,  $MPF_HOME/share  points to a directory on a network share. \nMost often, the share is managed by the Network File System (NFS) protocol, although using NFS is not a requirement.  Alternatively, OpenMPF supports writing these files to an object storage server. That may be desirable in cloud \ndeployments to better support integration between systems, and/or to consolidate file storage as a cost-saving measure.  When a file cannot be uploaded to the server, the Workflow Manager will fall back to storing it in  $MPF_HOME/share . \nIf and when a failure occurs, the JSON output object will contain a descriptive message in the  jobWarnings  field. \nIf the job completes without other issues, the final status will be  COMPLETE_WITH_WARNINGS .", 
            "title": "Object Storage Overview"
        }, 
        {
            "location": "/Object-Storage-Guide/index.html#common-object-storage-properties", 
            "text": "The following system properties are common to the various types of object storage solutions that OpenMPF supports:   http.object.storage.upload.retry.count  The number of times OpenMPF will attempt to upload an object to the storage server after the first failed attempt.  When using NGINX, exponential back off is used between retry attempts. There is a 500ms delay before the \n  first retry. The delay doubles for each subsequent retry.  When using S3, the AWS SDK's default retry strategy is used.", 
            "title": "Common Object Storage Properties"
        }, 
        {
            "location": "/Object-Storage-Guide/index.html#custom-nginx-http-object-storage", 
            "text": "OpenMPF supports a custom NGINX object storage server solution. If you're interested, please contact us. \nWe can make the server-side code available upon request.  For those who choose to run their own custom NGINX object storage server, please configure OpenMPF by setting \nthe  http.object.storage.nginx.service.uri  property to the URI of the NGINX server. \nThe following system properties are unique to the custom NGINX object storage solution:   http.object.storage.nginx.service.uri  Enables use of NGINX when provided.  The URI to the custom NGINX object storage server. For example:   https://somehost:123543/somepath .  You must provide a valid value.    http.object.storage.nginx.upload.thread.count  The number of threads used to upload objects to the storage server.  In general, the default value is sufficient.    http.object.storage.nginx.upload.segment.size  The chunk size, in bytes, that is used to upload objects to the storage server.  In general, the default value is sufficient.     The NGINX object storage server will determine the sha256 hash for the file once it's been uploaded. \nIt then uses that hash to name the file and returns the file URI to OpenMPF.", 
            "title": "Custom NGINX HTTP Object Storage"
        }, 
        {
            "location": "/Object-Storage-Guide/index.html#s3-object-storage", 
            "text": "OpenMPF supports downloading media and uploading results to an S3 compatible server such as Ceph. \nThe use of S3 is controlled through the following job properties:   S3_RESULTS_BUCKET  URI to bucket where result objects should be stored. For example:  http://s3host/results_bucket  To disable the upload of result objects, do not provide a value for this property.    S3_UPLOAD_ONLY  When true, media will not be downloaded using S3 authentication. \n  If  S3_RESULTS_BUCKET  is set, S3 authentication will be used to upload result objects.  When false or not provided, S3 authentication will be used to download remote media. \n  S3 authentication will also be used to upload result objects if  S3_RESULTS_BUCKET  is set.  If you want to run a job where some media is in S3 and some is hosted elsewhere, \n  you can set  S3_UPLOAD_ONLY  to  true  as a media specific property on the media that is hosted elsewhere.    S3_ACCESS_KEY  The access key that will be used when downloading and uploading to S3.    S3_SECRET_KEY  The secret key that will be used when downloading and uploading to S3.", 
            "title": "S3 Object Storage"
        }, 
        {
            "location": "/Contributor-Guide/index.html", 
            "text": "NOTICE:\n This software (or technical data) was produced for the U.S. Government under contract, and is subject to the Rights in Data-General Clause 52.227-14, Alt. IV (DEC 2007). Copyright 2019 The MITRE Corporation. All Rights Reserved.\n\n\n\n\nHigh-level Overview\n\n\nWe're excited that you're considering contributing to the OpenMPF project! If you have any questions about the process or how to get involved, please feel free to send us an \ne-mail\n with your question.\n\n\nWe encourage you to read the remainder of the guide as well as review the project's \nLicense\n and other \nDocumentation\n.\n\n\nThe OpenMPF project consists of the following repositories:\n\n\n\n\nopenmpf/openmpf\n\n\nopenmpf/openmpf-components\n\n\nopenmpf/openmpf-contrib-components\n\n\nopenmpf/openmpf-build-tools\n\n\nopenmpf/openmpf-cpp-component-sdk\n\n\nopenmpf/openmpf-java-component-sdk\n\n\nopenmpf/openmpf-python-component-sdk\n\n\nopenmpf/openmpf-projects\n\n\nopenmpf/openmpf-docker\n\n\n\n\nWork across the project is tracked using our \nworkboard\n.\n\n\nContribution Guidelines\n\n\nWe welcome all contributions that are made in a good faith effort to meet the following criteria:\n\n\n\n\nIn line with the spirit of the project. Refer to the \nOpenMPF Overview\n.\n\n\nAddresses an issue in the issue tracker. If an issue doesn't exist yet, create one so that it can be discussed among the OpenMPF community.\n\n\nFunctionally correct and logically sound. All code must pass a code review and round of regression tests.\n\n\nDesigned to use existing interfaces, super classes, and utilities\n\n\nMakes use of well-known design patterns, polymorphism, and encapsulation where possible\n\n\nEmploys best practices for integrating with the OpenMPF architecture. Refer to the \nC++ Batch Component API\n, \nC++ Streaming Component API\n, and \nJava Batch Component API\n.\n\n\nEmploys \nstandard coding style\n that is consistent with the rest of the project\n\n\nSufficiently commented and, if necessary, comes with appropriate documentation\n\n\nComes with sufficient test cases\n\n\nDoes not introduce software vulnerabilities\n\n\n\n\nCode Merging Workflow\n\n\nContributor Instructions\n\n\nPerform the following instructions to create a feature branch off of develop, commit your changes, push your branch, and create a pull request. Before the pull request is accepted a Jenkins build must pass and an OpenMPF project administrator must review the changes. We do not have a public Jenkins server so a project administrator will have to start the build for you.\n\n\n\n\nCreate a feature branch off of the latest version of develop\n\n\n\n\ncd /path/to/repo\ngit checkout develop\ngit pull\ngit checkout -b \nmy-new-feature\n\n\n\n\n\n\n\nMake Commits\n\n\n\n\ngit add .\ngit commit\n\n\n\n\n\n\nPush your feature branch.\n\n\n\n\ngit push -u origin \nmy-new-feature\n\n\n\n\n\n\n\nCreate a pull request.\n\n\nGo to GitHub page for repo.\n\n\nClick \"New pull request\"\n\n\nChange the dropdown that says \"base: master\" to develop\n\n\nChange the dropdown that says \"compare: master\" to your feature branch\n\n\nIf a message saying \"Can\u2019t automatically merge.\" appears to the right of the dropdowns, pull the latest version of develop, merge your feature branch with develop, and push it again:\n\n\n\n\ngit checkout develop\ngit pull\ngit checkout \nmy-new-feature\n\ngit merge develop\n\n# Fix conflicts\ngit add .\ngit commit\ngit push\n\n\n\n\n\n\nClick on the gear next to \"Reviewers\" and select a reviewer\n\n\nClick \"Create pull request\"\n\n\nGet approval\n\n\nAfter creating the pull request you will see that the pull request says \"Review required\" and \"Some checks haven't completed yet.\"\n\n\nAn OpenMPF project administrator will start a Jenkins build. Once the build completes, Jenkins will post a status check to the pull request.\n\n\nIf the Jenkins build passes, the pull request page will say \"All checks have passed\"\n\n\nIf the Jenkins build fails, a project administrator will provide further guidance.\n\n\n\n\n\n\nAn OpenMPF project administrator will review the pull request.\n\n\nIf the reviewer approves the changes, the reviewer will merge the change in to develop and close the pull request.\n\n\nIf the reviewer requests changes, you will need to make changes to your feature branch and push them. After you push your changes, the Jenkins status check will be reset. A project administrator will run another Jenkins build that will contain your most recent changes.\n\n\n\n\n\n\n\n\nIn order to be accepted and merged, pull requests need to comply with the \nContribution Guidelines\n. In cases where an issue is found, please refer to the reviewer's comments for more information on how to update your code. This review and acceptance process applies to all of the OpenMPF repositories, including the OpenMPF core and all of the OpenMPF components.\n\n\nLarge pull requests should be split up into smaller pull requests where possible. This will make it easier to review the code. In general, each pull request should add new functionality, update an existing feature, or fix a bug. We strive to keep the develop branch stable. If merging a smaller pull request will break the system before additional pull requests can be merged, then it's generally a better idea to merge one larger pull request.\n\n\nNote that GitHub has a 100 MB file size limitation. There is currently no way to push files to any of the OpenMPF repositories that are larger than this size.\n\n\nReviewer Instructions\n\n\n\n\nGo to the GitHub page for the pull request\n\n\nClick on the \"Files changed\"\n\n\nReview the code before you start a Jenkins build. You don't need to post your review comments immediately, but the Jenkins machine is on an internal network so for security you must review the code before you start the Jenkins build.\n\n\nAfter you have looked at the code, start an instance of the openmpf-github-with-pull-request Jenkins build.\n\n\nIf the Jenkins build fails, you will need to work with the developer to get the tests to pass.\n\n\nCheckout their branch locally to test it\n\n\n\n\ngit fetch\ngit checkout \nnew-feature\n\n\n\n\n\n\n\nOn the pull request page click \"Add your review\"\n\n\nAdd comments\n\n\nClick the green \"Review changes\" dropdown\n\n\nIf changes are necessary, click the radio button to \"Request changes\"\n\n\nAfter the developer makes the necessary changes, go back to the pull request page\n\n\nReview the changes\n\n\nStart another instance of the openmpf-github-with-pull-request Jenkins build.\n\n\nIf you are satisfied with the changes, click the \"Review changes\" dropdown\n\n\nSelect the \"Approve\" radio button, and click \"Submit review\"\n\n\nClick \"Squash and merge\" on the pull request page\n\n\nIf you don't see a \"Squash and merge\" button, find the button that says \"Merge pull request\", click the upside down triangle on the right side of the button, select \"Squash and merge\"\n\n\nA text box showing the commit message will appear above the \"Squash and merge button\". Edit message if necessary.\n\n\nClick \"Confirm squash and merge\"\n\n\nA message will pop up saying \"Pull request successfully merged and closed. You\u2019re all set\u2014the \n branch can be safely deleted.\"\n\n\nClick \"Delete branch\"\n\n\nUpdate the openmpf-projects' develop branch with the new changes:\n\n\n\n\ncd openmpf-projects\ngit checkout develop\ngit pull\ngit submodule foreach 'git checkout develop'\ngit submodule foreach 'git pull'\ngit add .\ngit commit\ngit push\n\n\n\n\nVersioning a New Release\n\n\nThe decision to version a new release is based on the following factors:\n\n\n\n\nChanges have been made to the API which break backwards compatibility. Refer to the \nSemantic Versioning Guide\n.\n\n\nThe system has been updated with major features and/or enhancements.\n\n\nThe system has been updated to work with new versions of critical system dependencies, such as OpenCV and Spring.\n\n\nThe packaging and/or deployment process has changed significantly.\n\n\nIt's been a long time since the last release and many small updates have been made to the system.\n\n\n\n\nWhen the OpenMPF team agrees that it's time to version a new release of the system, a project administrator will create a release branch in each repository off of the develop branch. The name of a release branch takes the form \nr\nmajor\n.\nminor\n.\nbugfix\n. For example, \nr0.10.0\n. Also, the first commit in the release branch will be tagged as release candidate 1. For example, \nr0.10.0-rc1\n. Beta testers will then have the opportunity to test the release candidate 1 code.\n\n\nIf a bug is found in the release candidate code, then developers should land the bug fix to the release branch via a pull request. Once it has landed, the most recent commit will be tagged as release candidate 2. For example, \nr0.10.0-rc2\n. Beta testers will then have the opportunity to test the release candidate 2 code. The release candidate number will increase by one each time bugs are fixed. The bug fix code should be merged into the develop branch after it lands to the release branch.\n\n\nIf no bugs are found in the release candidate code for a period of time (generally, a month) then the release candidate will be finalized. The release candidate branch for each repo will be merged into the master branch for that repo. That commit on the master branch will be tagged with the release number. For example, \nr0.10.0\n.\n\n\nIf a critical bug fix needs to be made to the master branch, this is known has a \"hot fix\". Developers should land a hot fix to the master branch via a pull request. Once the code lands, the commit will be tagged by incrementing the \nbugfix\n number. For example, \nr0.10.1\n. The bug fix code should be merged into the develop branch after it lands to the master branch.\n\n\nNote that you should not use the \n--no-ff\n option when merging one branch into another. Doing so will make the commit history more verbose and difficult to follow.\n\n\nThis process is based on \nGitFlow\n.\n\n\nAdding New Components\n\n\nIn general, a new component will initially go in the \nopenmpf-contrib-components\n repository. That is a holding ground until it can be transitioned to the \nopenmpf-components\n repository. To be a candidate for transition, it must meet the following criteria:\n\n\n\n\nIs strongly in line with the spirit of the project and there is a commitment to maintain and update the code as the project evolves\n\n\nFully licensed under Apache 2.0 or a compatible license. All source code must be provided\n\n\nComes with sufficient unit, system, and/or integration tests with a strong focus on regression testing\n\n\n\n\nNote that new components should have a README.md file, LICENSE file, COPYING file, and optionally a NOTICE file. The LICENSE file should contain information about all of the licenses in the code base, including those licenses for code you didn't write.\n\n\nCoding Style\n\n\nThe following list of style guides provide a comprehensive explanation of some of the best coding practices for the programming languages used in the OpenMPF project:\n\n\n\n\nGoogle C++ Style Guide\n\n\nGoogle Java Style Guide\n\n\nGoogle JavaScript Style Guide\n\n\nGoogle Python Style Guide\n\n\n\n\nGenerally speaking, when writing new code, please refer to existing code in the repositories and match the style. Most style issues boil down to inconsistency. Not all of our code adheres to these style guidelines, but we are striving to improve it.\n\n\nUpdating Online Documentation\n\n\nOur \nopenmpf.github.io repo\n repo is forked from \nBeautiful Jekyll\n. In general, everything within \nopenmpf.github.io/docs\n is part of a Read the Docs subsite within our overall Beautiful Jekyll site.\n\n\nInstall Tools\n\n\n\nTo install \nmkdocs\n run:\n\n\npip install 'mkdocs==0.16.0'\n\n\n\n\nNote that you need version 0.16.0.\n\n\nTo install the Ruby bundler dependency management tool with the jekyll gem run:\n\n\nsudo yum install ruby-devel\ngem install bundler\nbundle install\n\n\n\n\nUpdating Read the Docs\n\n\n\nUse \nmkdocs\n to generate HTML files from Markdown (.md) files stored in \nopenmpf.github.io/docs/docs\n. When modifying the files locally, run \nmkdocs serve\n within the \nopenmpf.github.io/docs\n directory. That spawns a local webserver so that you can view changes to the docs in real time by browsing to \nhttp://localhost:8000\n.\n\n\nUpdating Beautiful Jekyll\n\n\n\nWhen making updates to our landing page, or any other page other than those that are part of Read the Docs, run \n./build-site.sh\n within the top-level \nopenmpf.github.io\n directory to generate the HTML for those changes, and then run \n./serve.sh\n to spawn a local webserver so that you can view changes by browsing to \nhttp://localhost:4000\n. Note that unlike the \nmkdocs serve\n command explained above, this site is not updated in real time as you make changes to the source code.\n\n\nCommitting Changes\n\n\n\nWhen your changes look good, make sure to run the \n./build-site.sh\n command explained above to generate the HTML site content. Commit all of the generated files and generate a pull request to merge them into the develop branch.\n\n\nWhen a commit is made to the master branch on GitHub, the \nhttps://openmpf.github.io/docs/site/\n page will automatically update (often within 5 minutes).", 
            "title": "Contributor Guide"
        }, 
        {
            "location": "/Contributor-Guide/index.html#high-level-overview", 
            "text": "We're excited that you're considering contributing to the OpenMPF project! If you have any questions about the process or how to get involved, please feel free to send us an  e-mail  with your question.  We encourage you to read the remainder of the guide as well as review the project's  License  and other  Documentation .  The OpenMPF project consists of the following repositories:   openmpf/openmpf  openmpf/openmpf-components  openmpf/openmpf-contrib-components  openmpf/openmpf-build-tools  openmpf/openmpf-cpp-component-sdk  openmpf/openmpf-java-component-sdk  openmpf/openmpf-python-component-sdk  openmpf/openmpf-projects  openmpf/openmpf-docker   Work across the project is tracked using our  workboard .", 
            "title": "High-level Overview"
        }, 
        {
            "location": "/Contributor-Guide/index.html#contribution-guidelines", 
            "text": "We welcome all contributions that are made in a good faith effort to meet the following criteria:   In line with the spirit of the project. Refer to the  OpenMPF Overview .  Addresses an issue in the issue tracker. If an issue doesn't exist yet, create one so that it can be discussed among the OpenMPF community.  Functionally correct and logically sound. All code must pass a code review and round of regression tests.  Designed to use existing interfaces, super classes, and utilities  Makes use of well-known design patterns, polymorphism, and encapsulation where possible  Employs best practices for integrating with the OpenMPF architecture. Refer to the  C++ Batch Component API ,  C++ Streaming Component API , and  Java Batch Component API .  Employs  standard coding style  that is consistent with the rest of the project  Sufficiently commented and, if necessary, comes with appropriate documentation  Comes with sufficient test cases  Does not introduce software vulnerabilities", 
            "title": "Contribution Guidelines"
        }, 
        {
            "location": "/Contributor-Guide/index.html#code-merging-workflow", 
            "text": "", 
            "title": "Code Merging Workflow"
        }, 
        {
            "location": "/Contributor-Guide/index.html#contributor-instructions", 
            "text": "Perform the following instructions to create a feature branch off of develop, commit your changes, push your branch, and create a pull request. Before the pull request is accepted a Jenkins build must pass and an OpenMPF project administrator must review the changes. We do not have a public Jenkins server so a project administrator will have to start the build for you.   Create a feature branch off of the latest version of develop   cd /path/to/repo\ngit checkout develop\ngit pull\ngit checkout -b  my-new-feature    Make Commits   git add .\ngit commit   Push your feature branch.   git push -u origin  my-new-feature    Create a pull request.  Go to GitHub page for repo.  Click \"New pull request\"  Change the dropdown that says \"base: master\" to develop  Change the dropdown that says \"compare: master\" to your feature branch  If a message saying \"Can\u2019t automatically merge.\" appears to the right of the dropdowns, pull the latest version of develop, merge your feature branch with develop, and push it again:   git checkout develop\ngit pull\ngit checkout  my-new-feature \ngit merge develop\n\n# Fix conflicts\ngit add .\ngit commit\ngit push   Click on the gear next to \"Reviewers\" and select a reviewer  Click \"Create pull request\"  Get approval  After creating the pull request you will see that the pull request says \"Review required\" and \"Some checks haven't completed yet.\"  An OpenMPF project administrator will start a Jenkins build. Once the build completes, Jenkins will post a status check to the pull request.  If the Jenkins build passes, the pull request page will say \"All checks have passed\"  If the Jenkins build fails, a project administrator will provide further guidance.    An OpenMPF project administrator will review the pull request.  If the reviewer approves the changes, the reviewer will merge the change in to develop and close the pull request.  If the reviewer requests changes, you will need to make changes to your feature branch and push them. After you push your changes, the Jenkins status check will be reset. A project administrator will run another Jenkins build that will contain your most recent changes.     In order to be accepted and merged, pull requests need to comply with the  Contribution Guidelines . In cases where an issue is found, please refer to the reviewer's comments for more information on how to update your code. This review and acceptance process applies to all of the OpenMPF repositories, including the OpenMPF core and all of the OpenMPF components.  Large pull requests should be split up into smaller pull requests where possible. This will make it easier to review the code. In general, each pull request should add new functionality, update an existing feature, or fix a bug. We strive to keep the develop branch stable. If merging a smaller pull request will break the system before additional pull requests can be merged, then it's generally a better idea to merge one larger pull request.  Note that GitHub has a 100 MB file size limitation. There is currently no way to push files to any of the OpenMPF repositories that are larger than this size.", 
            "title": "Contributor Instructions"
        }, 
        {
            "location": "/Contributor-Guide/index.html#reviewer-instructions", 
            "text": "Go to the GitHub page for the pull request  Click on the \"Files changed\"  Review the code before you start a Jenkins build. You don't need to post your review comments immediately, but the Jenkins machine is on an internal network so for security you must review the code before you start the Jenkins build.  After you have looked at the code, start an instance of the openmpf-github-with-pull-request Jenkins build.  If the Jenkins build fails, you will need to work with the developer to get the tests to pass.  Checkout their branch locally to test it   git fetch\ngit checkout  new-feature    On the pull request page click \"Add your review\"  Add comments  Click the green \"Review changes\" dropdown  If changes are necessary, click the radio button to \"Request changes\"  After the developer makes the necessary changes, go back to the pull request page  Review the changes  Start another instance of the openmpf-github-with-pull-request Jenkins build.  If you are satisfied with the changes, click the \"Review changes\" dropdown  Select the \"Approve\" radio button, and click \"Submit review\"  Click \"Squash and merge\" on the pull request page  If you don't see a \"Squash and merge\" button, find the button that says \"Merge pull request\", click the upside down triangle on the right side of the button, select \"Squash and merge\"  A text box showing the commit message will appear above the \"Squash and merge button\". Edit message if necessary.  Click \"Confirm squash and merge\"  A message will pop up saying \"Pull request successfully merged and closed. You\u2019re all set\u2014the   branch can be safely deleted.\"  Click \"Delete branch\"  Update the openmpf-projects' develop branch with the new changes:   cd openmpf-projects\ngit checkout develop\ngit pull\ngit submodule foreach 'git checkout develop'\ngit submodule foreach 'git pull'\ngit add .\ngit commit\ngit push", 
            "title": "Reviewer Instructions"
        }, 
        {
            "location": "/Contributor-Guide/index.html#versioning-a-new-release", 
            "text": "The decision to version a new release is based on the following factors:   Changes have been made to the API which break backwards compatibility. Refer to the  Semantic Versioning Guide .  The system has been updated with major features and/or enhancements.  The system has been updated to work with new versions of critical system dependencies, such as OpenCV and Spring.  The packaging and/or deployment process has changed significantly.  It's been a long time since the last release and many small updates have been made to the system.   When the OpenMPF team agrees that it's time to version a new release of the system, a project administrator will create a release branch in each repository off of the develop branch. The name of a release branch takes the form  r major . minor . bugfix . For example,  r0.10.0 . Also, the first commit in the release branch will be tagged as release candidate 1. For example,  r0.10.0-rc1 . Beta testers will then have the opportunity to test the release candidate 1 code.  If a bug is found in the release candidate code, then developers should land the bug fix to the release branch via a pull request. Once it has landed, the most recent commit will be tagged as release candidate 2. For example,  r0.10.0-rc2 . Beta testers will then have the opportunity to test the release candidate 2 code. The release candidate number will increase by one each time bugs are fixed. The bug fix code should be merged into the develop branch after it lands to the release branch.  If no bugs are found in the release candidate code for a period of time (generally, a month) then the release candidate will be finalized. The release candidate branch for each repo will be merged into the master branch for that repo. That commit on the master branch will be tagged with the release number. For example,  r0.10.0 .  If a critical bug fix needs to be made to the master branch, this is known has a \"hot fix\". Developers should land a hot fix to the master branch via a pull request. Once the code lands, the commit will be tagged by incrementing the  bugfix  number. For example,  r0.10.1 . The bug fix code should be merged into the develop branch after it lands to the master branch.  Note that you should not use the  --no-ff  option when merging one branch into another. Doing so will make the commit history more verbose and difficult to follow.  This process is based on  GitFlow .", 
            "title": "Versioning a New Release"
        }, 
        {
            "location": "/Contributor-Guide/index.html#adding-new-components", 
            "text": "In general, a new component will initially go in the  openmpf-contrib-components  repository. That is a holding ground until it can be transitioned to the  openmpf-components  repository. To be a candidate for transition, it must meet the following criteria:   Is strongly in line with the spirit of the project and there is a commitment to maintain and update the code as the project evolves  Fully licensed under Apache 2.0 or a compatible license. All source code must be provided  Comes with sufficient unit, system, and/or integration tests with a strong focus on regression testing   Note that new components should have a README.md file, LICENSE file, COPYING file, and optionally a NOTICE file. The LICENSE file should contain information about all of the licenses in the code base, including those licenses for code you didn't write.", 
            "title": "Adding New Components"
        }, 
        {
            "location": "/Contributor-Guide/index.html#coding-style", 
            "text": "The following list of style guides provide a comprehensive explanation of some of the best coding practices for the programming languages used in the OpenMPF project:   Google C++ Style Guide  Google Java Style Guide  Google JavaScript Style Guide  Google Python Style Guide   Generally speaking, when writing new code, please refer to existing code in the repositories and match the style. Most style issues boil down to inconsistency. Not all of our code adheres to these style guidelines, but we are striving to improve it.", 
            "title": "Coding Style"
        }, 
        {
            "location": "/Contributor-Guide/index.html#updating-online-documentation", 
            "text": "Our  openmpf.github.io repo  repo is forked from  Beautiful Jekyll . In general, everything within  openmpf.github.io/docs  is part of a Read the Docs subsite within our overall Beautiful Jekyll site.", 
            "title": "Updating Online Documentation"
        }, 
        {
            "location": "/Development-Environment-Guide/index.html", 
            "text": "NOTICE:\n This software (or technical data) was produced for the U.S. Government under contract, and is subject to the Rights in Data-General Clause 52.227-14, Alt. IV (DEC 2007). Copyright 2019 The MITRE Corporation. All Rights Reserved.\n\n\nWARNING:\n This guide is a work in progress and may not be completely accurate or comprehensive. Since transitioning to Docker deployment, this guide has not been fully tested.\n\n\n\n\nOverview\n\n\nThe following instructions are for setting up an environment for building OpenMPF outside of Docker.\n\n\nIf your environment is behind a proxy server, please refer to the \nProxy Configuration\n and \nSSL Inspection\n appendix sections before continuing. Keep them in mind throughout this guide and perform the configuration steps as necessary.\n\n\nInstall CentOS 7\n\n\nThe recommended minimum system specifications are:\n\n\n\n\nMemory\n: 8192MB\n\n\nCPU\n: 4\n\n\nDisk\n: 40GB on a SSD\n\n\n\n\nInstall \nCentOS 7\n. Most developers use a virtual machine for development.\n\n\nInstall System Dependencies Using Package Managers\n\n\nConfigure Additional Repositories\n\n\n\n\nInstall the Remi Repo for Redis:\n\n\nwget -P /home/mpf/Downloads \"http://rpms.remirepo.net/RPM-GPG-KEY-remi\"\n\n\nwget -P /home/mpf/Downloads \"http://rpms.famillecollet.com/enterprise/remi-release-7.rpm\"\n\n\nsudo rpm --import /home/mpf/Downloads/RPM-GPG-KEY-remi\n\n\nsudo rpm -Uvh /home/mpf/Downloads/remi-release-7.rpm\n\n\nsudo yum-config-manager --enable remi\n\n\n\n\n\n\nCreate an \n/apps\n directory and package subdirectories:\n\n\nsudo mkdir -p /apps/install/lib\n\n\nsudo mkdir -p /apps/bin/apache\n\n\nsudo mkdir /apps/ansible\n\n\nsudo mkdir -p /apps/source/cmake_sources\n\n\nsudo mkdir /apps/source/apache_sources\n\n\nsudo mkdir /apps/source/google_sources\n\n\nsudo mkdir /apps/source/opencv_sources\n\n\nsudo mkdir /apps/source/ffmpeg_sources\n\n\nsudo mkdir /apps/source/dlib-sources\n\n\nsudo mkdir /apps/source/openalpr_sources\n\n\nsudo mkdir /apps/source/ansible_sources\n\n\nsudo chown -R mpf:mpf /apps\n\n\nsudo chmod -R 755 /apps\n\n\n\n\n\n\nCreate the OpenMPF \nldconfig\n file:\n    \nsudo touch /etc/ld.so.conf.d/mpf-x86_64.conf\n\n\nAdd \n/apps/install/lib\n to the OpenMPF \nldconfig\n file:\n    \nsudo sh -c 'echo \"/apps/install/lib\" \n /etc/ld.so.conf.d/mpf-x86_64.conf'\n\n\nUpdate the shared library cache:\n    \nsudo ldconfig\n\n\n\n\nInstall System Dependencies via Yum\n\n\nUse \nyum\n to install packages:\n\n\nsudo yum install -y asciidoc autoconf automake boost boost-devel cmake3 curl freetype-devel gcc-c++ git graphviz gstreamer-plugins-base-devel gtk2-devel gtkglext-devel gtkglext-libs jasper jasper-devel libavc1394-devel libcurl-devel libdc1394-devel libffi-devel libICE-devel libjpeg-turbo-devel libpng-devel libSM-devel libtiff-devel libtool libv4l-devel libXinerama-devel libXmu-devel libXt-devel log4cplus log4cplus-devel log4cxx log4cxx-devel make mercurial mesa-libGL-devel mesa-libGLU-devel nasm ncurses-devel numpy openssl-devel pangox-compat pangox-compat-devel perl-CPAN-Meta-YAML perl-DBI perl-Digest-MD5 perl-File-Find-Rule perl-File-Find-Rule-Perl perl-JSON perl-JSON-PP perl-List-Compare perl-Number-Compare perl-Params-Util perl-Parse-CPAN-Meta php pkgconfig qt qt-devel qt-x11 redis rpm-build sshpass tbb tbb-devel tree unzip uuid-devel wget yasm yum-utils zlib-devel\n\n\nGet the OpenMPF Source Code\n\n\n\n\n\n\nClone the OpenMPF projects repository:\n\n\n\n\ncd /home/mpf\n\n\ngit clone https://github.com/openmpf/openmpf-projects.git --recursive\n\n\n\n\n\n\n\n\nInstall the OpenMPF command line tools:\n    \n \nsudo pip3 install /home/mpf/openmpf-projects/openmpf/trunk/bin/mpf-scripts\n\n\n\n\n\n\nCopy the mpf user profile script from the extracted source code:\n    \n \nsudo cp /home/mpf/openmpf-projects/openmpf/trunk/mpf-install/src/main/scripts/mpf-profile.sh /etc/profile.d/mpf.sh\n\n\n\n\n\n\nAdd \n/apps/install/bin\n to the system \nPATH\n variable:\n\n\n\n\nsudo sh -c 'echo \"PATH=\\$PATH:/apps/install/bin\" \n /etc/profile.d/mpf.sh'\n\n\n. /etc/profile.d/mpf.sh\n\n\n\n\n\n\n\n\nFor more information on the command line tools, please refer to the \nCommand Line Tools\n section below.\n\n\nAdd Maven Dependencies\n\n\nSome Maven dependencies needed for OpenMPF are not publicly available.\n\n\n\n\nDownload   \nmpf-maven-deps.tar.gz\n to \n/home/mpf/openmpf-projects/openmpf-build-tools/mpf-maven-deps.tar.gz\n.\n\n\nSet up the local Maven repository:\n\n\ncd /home/mpf\n\n\nmkdir -p .m2/repository\n\n\n\n\n\n\nExtract the archive to the local Maven repository:\n\ntar xvzf /home/mpf/openmpf-projects/openmpf-build-tools/mpf-maven-deps.tar.gz -C /home/mpf/.m2/repository/\n\n\n\n\nBuild and Install System Dependencies in Dockerfiles\n\n\nRefer to the \nopenmpf_build\n \nDockerfile\n, and execute the \nRUN\n steps to build/install the required dependencies in your local development environment. Use your best judgement. You will need to run some commands with \nsudo\n root privileges. Some of these may be redundant with the steps you've followed so far. Installation of the NVIDIA CUDA Toolkit is optional. Refer to the \nNVIDIA CUDA Toolkit\n section below.\n\n\nThe \nopenmpf_build\n Dockerfile may not include the dependencies you need to develop specific components. Refer to the Dockerfile for each of those components to determine which dependencies they require.\n\n\nNVIDIA CUDA Toolkit\n\n\nInstallation of the NVIDIA CUDA Toolkit is optional, and only necessary if you need to run a component on a GPU. Many components that support GPU processing also support execution on the CPU, and if this toolkit is not found in the build environment, the build system will automatically build those components for CPU processing only. For a discussion of NVIDIA GPU support in OpenMPF components, see the \nGPU Support Guide\n.\n\n\n\n\nNOTE:\n To run OpenMPF components that use the NVIDIA GPUs, you must ensure that the deployment machine has the same version of this Toolkit installed, including the NVIDIA GPU drivers. The instructions here are for a development environment only, and thus do not include steps to install the drivers. If you also need to set up the deployment machine, please see the full instructions at \nhttps://docs.nvidia.com/cuda/cuda-installation-guide-linux/index.html\n.\n\n\n\n\nBuild and Install Additional System Dependencies\n\n\nApache ActiveMQ 5.13.0\n\n\nFor reference only: \nhttp://activemq.apache.org\n\n\n\n\ncd /apps/bin/apache\n\n\nwget -O /apps/bin/apache/apache-activemq-5.13.0-bin.tar.gz \"https://archive.apache.org/dist/activemq/5.13.0/apache-activemq-5.13.0-bin.tar.gz\"\n\n\nsudo tar xvzf apache-activemq-5.13.0-bin.tar.gz -C /opt/\n\n\nsudo chown -R mpf:mpf /opt/apache-activemq-5.13.0\n\n\nsudo chmod -R 755 /opt/apache-activemq-5.13.0\n\n\nsudo ln -s /opt/apache-activemq-5.13.0 /opt/activemq\n\n\n\n\nApache Ant 1.9.6\n\n\nFor reference only: \nhttp://ant.apache.org\n\n\n\n\ncd /apps/bin/apache\n\n\nwget -O /apps/bin/apache/apache-ant-1.9.6-bin.tar.gz \"https://archive.apache.org/dist/ant/binaries/apache-ant-1.9.6-bin.tar.gz\"\n\n\ntar xzvf apache-ant-1.9.6-bin.tar.gz\n\n\nsudo cp -R /apps/bin/apache/apache-ant-1.9.6 /apps/install/\n\n\nsudo chown -R mpf:mpf /apps/install/apache-ant-1.9.6\n\n\nsudo sed -i '/^PATH/s/$/:\\/apps\\/install\\/apache-ant-1.9.6\\/bin/' /etc/profile.d/mpf.sh\n\n\n. /etc/profile.d/mpf.sh\n\n\n\n\nPostgreSQL\n\n\n\n\nAdd PostgreSQL repository:\n\nsudo yum install -y https://download.postgresql.org/pub/repos/yum/reporpms/EL-7-x86_64/pgdg-redhat-repo-latest.noarch.rpm\n\n\nInstall postgres with yum:\n\nsudo yum install -y postgresql12-server\n\n\nInitialize postgres:\n\nsudo /usr/pgsql-12/bin/postgresql-12-setup initdb\n\n\nModify the default authentication method for localhost.\n\n\nOpen \n/var/lib/pgsql/12/data/pg_hba.conf\n in a text editor.\n\n\nFind the line containing:\n\n    \nhost    all             all             127.0.0.1/32            ident\n\n  \nIt should be line 82. Change \"ident\" to \"md5\".\n\n\nFind the line containing:\n\n    \nhost    all             all             ::1/128                 ident\n\n  \nIt should be line 84. Change \"ident\" to \"md5\".\n\n\n\n\n\n\nStart postgres:\n\nsudo service postgresql-12 start\n\n\nCreate the mpf user. When prompted for a password use \"password\":\n\nsudo -i -u postgres createuser -P mpf\n\n\nCreate the mpf database with the mpf user as the owner:\n\nsudo -i -u postgres createdb -O mpf mpf\n\n\n\n\nPython 3.8\n\n\n\n\n\n\nInstall build dependencies:\n   \n \nsudo yum install -y yum-utils\n\n   \nsudo yum-builddep -y python3\n\n\n\n\n\n\nDownload the source code: \n   \ncurl https://www.python.org/ftp/python/3.8.2/Python-3.8.2.tar.xz | tar --extract --xz\n\n\n\n\n\n\nBuild Python:\n   \n \ncd Python-3.8.2\n\n   \n \n./configure --enable-optimizations --with-lto --enable-shared\n\n   \n \nmake -j8\n\n   \n \nsudo make install\n\n   \n \nsudo ln -s /usr/local/lib/libpython3.8.so.1.0 /usr/lib64/libpython3.8.so.1.0\n\n   \n \nsudo ln -sf /usr/local/bin/python3 /bin/python3\n\n   \n \nsudo ln -sf /usr/local/bin/python3.8 /bin/python3.8\n\n   \n \nsudo ln -sf /usr/local/bin/pip3 /bin/pip3\n\n   \n \nsudo ln -sf /usr/local/bin/pip3.8 /bin/pip3.8\n\n\n\n\n\n\nMake sure the output of running \npython3 --version\n is \nPython 3.8.2\n.\n\n\n\n\n\n\nMake sure the output of running \nsudo python3 --version\n is \nPython 3.8.2\n.\n\n\n\n\n\n\nMake sure the output of running \npip3 --version\n ends with \n(python 3.8)\n.\n\n\n\n\n\n\nMake sure the output of running \nsudo pip3 --version\n ends with \n(python 3.8)\n.\n\n\n\n\n\n\nUpgrade pip:\n   \n \nsudo pip3 install --upgrade pip\n\n\n\n\n\n\nInstall wheel:\n   \n \nsudo pip3 install wheel\n\n\n\n\n\n\nConfigure System Dependencies\n\n\nConfigure ActiveMQ\n\n\nSome additional manual configuration of ActiveMQ is required. For each step, open the specified file in a text editor, make the change, and save the file. If ActiveMQ is running, please stop it before making these changes.\n\n\nUse Additional Memory\n\n\nIn \n/opt/activemq/bin/env\n (line 27), comment out the line:\n\n\nACTIVEMQ_OPTS_MEMORY=\n-Xms64M -Xmx1G\n\n\n\n\n\nso that it reads:\n\n\n#ACTIVEMQ_OPTS_MEMORY=\n-Xms64M -Xmx1G\n\n\n\n\n\nDisable Persistence\n\n\nIn \n/opt/activemq/conf/activemq.xml\n (line 40), change the line:\n\n\nbroker xmlns=\nhttp://activemq.apache.org/schema/core\n brokerName=\nlocalhost\n dataDirectory=\n${activemq.data}\n\n\n\n\n\nso that it reads:\n\n\nbroker xmlns=\nhttp://activemq.apache.org/schema/core\n brokerName=\nlocalhost\n dataDirectory=\n${activemq.data}\n persistent=\nfalse\n\n\n\n\n\nRespect Priorities\n\n\nIn \n/opt/activemq/conf/activemq.xml\n (line 44) under the line:\n\n\npolicyEntries\n\n\n\n\n\nadd the line:\n\n\npolicyEntry queue=\n prioritizedMessages=\ntrue\n useCache=\nfalse\n expireMessagesPeriod=\n0\n queuePrefetch=\n1\n /\n\n\n\n\n\nEnable JMX\n\n\nIn \n/opt/activemq/conf/activemq.xml\n (line 72, after making the above addition), change the line:\n\n\nmanagementContext createConnector=\nfalse\n/\n\n\n\n\n\nso that it reads:\n\n\nmanagementContext createConnector=\ntrue\n/\n\n\n\n\n\nChange Log Conversion Pattern\n\n\nIn \n/opt/activemq/conf/log4j.properties\n (line 52), change the line:\n\n\nlog4j.appender.logfile.layout.ConversionPattern=%d | %-5p | %m | %c | %t%n\n\n\n\n\nso that it reads:\n\n\nlog4j.appender.logfile.layout.ConversionPattern=%d %p [%t] %c - %m%n\n\n\n\n\nConfigure Redis\n\n\nRedis should be set to run in the background (i.e. as a daemon process).\n\n\nIn \n/etc/redis.conf\n (line 136), change the line:\n\n\ndaemonize no\n\n\n\n\nso that it reads:\n\n\ndaemonize yes\n\n\n\n\nConfigure Users\n\n\nTo change the default user password settings, modify \n/home/mpf/openmpf-projects/openmpf/trunk/workflow-manager/src/main/resources/properties/user.properties\n. Note that the default settings are public knowledge, which could be a security risk.\n\n\nNote that \nmpf remove-user\n and \nmpf add-user\n commands explained in the \nCommand Line Tools\n section do not modify the \nuser.properties\n file. If you remove a user using the \nmpf remove-user\n command, the changes will take effect at runtime, but an entry may still exist for that user in the \nuser.properties\n file. If so, then the user account will be recreated the next time the Workflow Manager is restarted.\n\n\nConfigure Tomcat with HTTP\n\n\nWhen developing OpenMPF on a local machine, it is often most convenient to configure Tomcat to use HTTP instead of HTTPS.\n\n\n\n\nOpen the file \n/opt/apache-tomcat/conf/server.xml\n in a text editor.\n\n\nBelow the commented out section on lines 87 through 90, remove the following lines if they exist:\n\nConnector SSLEnabled=\"true\" acceptCount=\"100\" clientAuth=\"false\"\n    disableUploadTimeout=\"true\" enableLookups=\"false\" maxThreads=\"25\"\n    port=\"8443\" keystoreFile=\"/home/mpf/.keystore\" keystorePass=\"mpf123\"\n    protocol=\"org.apache.coyote.http11.Http11NioProtocol\" scheme=\"https\"\n    secure=\"true\" sslProtocol=\"TLS\" /\n\n\nSave and close the file.\n\n\nCreate the file \n/opt/apache-tomcat/bin/setenv.sh\n and open it in a text editor.\n\n\nAdd the following line:\n\nexport CATALINA_OPTS=\"-server -Xms256m -XX:PermSize=512m -XX:MaxPermSize=512m -Djava.library.path=$MPF_HOME/lib -Dtransport.guarantee='NONE' -Dweb.rest.protocol='http'\"\n\n\nSave and close the file.\n\n\n\n\n(Optional) Configure Tomcat with HTTPS\n\n\nAlternatively, OpenMPF can also be run using HTTPS instead of HTTP.\n\n\nGenerate a self-signed certificate and keystore\n\n\nA valid keystore is required to run OpenMPF with HTTPS support. These instructions will generate a keystore that should be used for local builds only. When deploying OpenMPF, a keystore containing a valid certificate trust chain should be used.\n\n\n\n\nOpen a new terminal window.\n\n\nsudo systemctl stop tomcat7\n\n\ncd /home/mpf\n\n\n$JAVA_HOME/bin/keytool -genkey -alias tomcat -keyalg RSA\n\n\nAt the prompt, enter a keystore password of: \nmpf123\n\n\nRe-enter the keystore password of: \nmpf123\n\n\nAt the \nWhat is your first and last name?\n prompt, press the Enter key for a blank value.\n\n\nAt the \nWhat is the name of your organizational unit?\n , press the Enter key for a blank value.\n\n\nAt the \nWhat is the name of your organization?\n prompt, press the Enter key for a blank value.\n\n\nAt the \nWhat is the name of your City or Locality?\n prompt, press the Enter key for a blank value.\n\n\nAt the \nWhat is the name of your State or Province?\n prompt, press the Enter key for a blank value.\n\n\nAt the \nWhat is the two-letter country code for this unit?\n prompt, press the Enter key for a blank value.\n\n\nAt the \nIs CN=Unknown, OU=Unknown, O=Unknown, L=Unknown, ST=Unknown, C=Unknown correct?\n prompt, type \nyes\n and press the Enter key to accept the values.\n\n\nAt the \nEnter key password for \ntomcat\n prompt, press the Enter key for a blank value.\n\n\nVerify the file \n/home/mpf/.keystore\n was created at the current time.\n\n\n\n\nTomcat Configuration\n\n\n\n\nOpen the file \n/opt/apache-tomcat/conf/server.xml\n in a text editor.\n\n\nBelow the commented out section on lines 87 through 90, add the following lines:\n\nConnector SSLEnabled=\"true\" acceptCount=\"100\" clientAuth=\"false\"\n    disableUploadTimeout=\"true\" enableLookups=\"false\" maxThreads=\"25\"\n    port=\"8443\" keystoreFile=\"/home/mpf/.keystore\" keystorePass=\"mpf123\"\n    protocol=\"org.apache.coyote.http11.Http11NioProtocol\" scheme=\"https\"\n    secure=\"true\" sslProtocol=\"TLS\" /\n\n\nSave and close the file.\n\n\nCreate the file \n/opt/apache-tomcat/bin/setenv.sh\n and open it in a text editor.\n\n\nAdd the following line:\n\nexport CATALINA_OPTS=\"-server -Xms256m -XX:PermSize=512m -XX:MaxPermSize=512m -Djava.library.path=$MPF_HOME/lib -Dtransport.guarantee='CONFIDENTIAL' -Dweb.rest.protocol='https'\"\n\n\nSave and close the file.\n\n\n\n\nUsing an IDE\n\n\nIf running Tomcat from an IDE, such as IntelliJ, then \n-Dtransport.guarantee=\"CONFIDENTIAL\" -Dweb.rest.protocol=\"https\"\n should be added at the end of the Tomcat VM arguments for your Tomcat run configuration. It is not necessary to add these arguments when running tomcat from the command line or a systemd command because of the configured \nCATALINA_OPTS\n variable.\n\n\nBuild and Run the OpenMPF Workflow Manager Web Application\n\n\nRun the following commands to build OpenMPF and launch the web application. Use this value for \nconfigFile\n:\n\n/home/mpf/openmpf-projects/openmpf/trunk/jenkins/scripts/config_files/openmpf-open-source-package.json\n\n\n\n\ncd /home/mpf/openmpf-projects/openmpf\n\n\n\n\nCopy the development properties file into place:\n\n\ncp trunk/workflow-manager/src/main/resources/properties/mpf-private-example.properties trunk/workflow-manager/src/main/resources/properties/mpf-private.properties\n\n\n\n\n\n\nOpen \n/etc/ansible/hosts\n in a text editor. \nsudo\n is required to edit this file.\n\n\n\n\n\n\nIf they do not already exist, add these two lines above \n# Ex 1: Ungrouped hosts, specify before any group headers.\n (line 11):\n\n\n[mpf-child]\nlocalhost.localdomain\n\n\n\n\n\n\nSave and close the file.\n\n\n\n\nmvn clean install -DskipTests -Dmaven.test.skip=true -DskipITs -Dmaven.tomcat.skip=true  -Dcomponents.build.package.json=\nconfigFile\n -Dstartup.auto.registration.skip=false -Dcomponents.build.dir=/home/mpf/openmpf-projects/openmpf/mpf-component-build\n\n\ncd /home/mpf/openmpf-projects/openmpf/trunk/workflow-manager\n\n\nrm -rf /opt/apache-tomcat/webapps/workflow-manager*\n\n\ncp target/workflow-manager.war /opt/apache-tomcat/webapps/workflow-manager.war\n\n\ncd ../..\n\n\nsudo cp trunk/install/libexec/node-manager /etc/init.d/\n\n\nsudo systemctl daemon-reload\n\n\nmpf start\n\n\n\n\nThe web application should start running in the background as a daemon. Look for this log message in the Tomcat log (\n/opt/apache-tomcat/logs/catalina.out\n) with a time value indicating the Workflow Manager has finished starting:\n\n\nINFO: Server startup in 39030 ms\n\n\n\n\nAfter startup, the Workflow Manager will be available at \nhttp://localhost:8080/workflow-manager\n (or \nhttps://localhost:8443/workflow-manager\n if configured to use HTTPS instead of HTTP). Browse to this URL using FireFox or Chrome.\n\n\nIf you want to test regular user capabilities, log in as the \"mpf\" user with the \"mpf123\" password. Please see the \nOpenMPF User Guide\n for more information. Alternatively, if you want to test admin capabilities then log in as \"admin\" user with the \"mpfadm\" password. Please see the \nOpenMPF Admin Guide\n for more information. When finished using OpenMPF, run \nmpf stop\n.\n\n\nThe preferred method to start and stop services for OpenMPF is with the \nmpf start\n and \nmpf stop\n commands. For additional information on these commands, please see the \nCommand Line Tools\n section of the \nOpenMPF Admin Guide\n. These will start and stop the ActiveMQ, PostgreSQL, Redis, Node Manager, and Tomcat system processes.\n\n\nFor debugging purposes, it may be helpful to manually start the Tomcat service in a separate terminal window to display the log output. To do that, use \nmpf start --xtc\n to start ActiveMQ, PostgreSQL, Redis, and the Node Manager without starting Tomcat. Then, in another terminal windows run:\n\n\n/opt/apache-tomcat/bin/catalina.sh run\n\n\n\n\nPress \nctrl-c\n in the Tomcat window to stop Tomcat.\n\n\n(Optional) Test OpenMPF\n\n\nRun the following commands to build OpenMPF and run the integration tests. Use this value for \nconfigFile\n:\n\n/home/mpf/openmpf-projects/openmpf/trunk/jenkins/scripts/config_files/openmpf-open-source-package.json\n\n\n\n\ncd /home/mpf/openmpf-projects/openmpf\n\n\n\n\nCopy the development properties file into place:\n\n\ncp trunk/workflow-manager/src/main/resources/properties/mpf-private-example.properties trunk/workflow-manager/src/main/resources/properties/mpf-private.properties\n\n\n\n\n\n\nOpen the file \n/etc/ansible/hosts\n in a text editor. \nsudo\n is required to edit this file.\n\n\n\n\n\n\nIf they do not already exist, add these two lines above \n# Ex 1: Ungrouped hosts, specify before any group headers.\n (line 11):\n\n\n[mpf-child]\nlocalhost.localdomain\n\n\n\n\n\n\nSave and close the file.\n\n\n\n\nmvn clean install -DskipTests -Dmaven.test.skip=true -DskipITs -Dmaven.tomcat.skip=true -Dcomponents.build.package.json=\nconfigFile\n -Dcomponents.build.dir=/home/mpf/openmpf-projects/openmpf/mpf-component-build -Dstartup.auto.registration.skip=false\n\n\nsudo cp /home/mpf/openmpf-projects/openmpf/trunk/install/libexec/node-manager /etc/init.d/\n\n\nsudo systemctl daemon-reload\n\n\nmpf start --xtc\n\n\nmvn verify -Pjenkins -Dtransport.guarantee=\"NONE\" -Dweb.rest.protocol=\"http\" -Dcomponents.build.package.json=\nconfigFile\n -Dstartup.auto.registration.skip=false -Dcomponents.build.dir=/home/mpf/openmpf-projects/openmpf/mpf-component-build\n\n\nmpf stop --xtc\n\n\n\n\nPlease see the appendix section \nKnown Issues\n regarding any \njava.lang.InterruptedException: null\n warning log messages observed when running the tests.\n\n\nRun this command to clean up and remove all traces of the test run:\n\n\n\n\nmpf clean --delete-uploaded-media --delete-logs\n\n\nType \"Y\" and press Enter.\n\n\n\n\nIf you choose not to run \nmpf clean\n before following the \nBuild and Run the OpenMPF Workflow Manager Web Application\n steps, the Job Status table will be pre-populated with some entries; however, the input media, markup, and JSON output objects for those jobs will not be available.\n\n\n\n\nAppendices\n\n\nCommand Line Tools\n\n\nOpenMPF installs command line tools that can be accessed through a terminal on the development machine. All of the tools take the form of actions: \nmpf \naction\n [options ...]\n. Note that tab-completion is enabled for ease of use.\n\n\nExecute \nmpf --help\n for general documentation and \nmpf \naction\n --help\n for documentation about a specific action.\n\n\n\n\nStart / Stop Actions\n: Actions for starting and stopping the OpenMPF system dependencies, including PostgreSQL, ActiveMQ, Redis, Tomcat, and the node managers on the various nodes in the OpenMPF cluster.\n\n\nmpf status\n: displays a message indicating whether each of the system dependencies is running or not\n\n\nmpf start\n: starts all of the system dependencies\n\n\nmpf stop\n: stops all of the system dependencies\n\n\nmpf restart\n : stops and then starts all of the system dependencies\n\n\n\n\n\n\nUser Actions\n: Actions for managing Workflow Manager user accounts. If changes are made to an existing user then that user will need to log off or the Workflow Manager will need to be restarted for the changes to take effect.\n\n\nmpf list-users\n : lists all of the existing user accounts and their role (non-admin or admin)\n\n\nmpf add-user \nusername\n \nrole\n: adds a new user account; will be prompted to enter the account password\n\n\nmpf remove-user \nusername\n : removes an existing user account\n\n\nmpf change-role \nusername\n \nrole\n : change the role (non-admin to admin or vice versa) for an existing user\n\n\nmpf change-password \nusername\n: change the password for an existing user; will be prompted to enter the new account password\n\n\n\n\n\n\nClean Actions\n: Actions to remove old data and revert the system to a new install state. User accounts, registered components, as well as custom actions, tasks, and pipelines, are preserved.\n\n\nmpf clean\n: cleans out old job information and results, pending job requests, marked up media files, and ActiveMQ data, but preserves log files and uploaded media\n\n\nmpf clean --delete-logs --delete-uploaded-media\n: the same as \nmpf clean\n but also deletes log files and uploaded media\n\n\n\n\n\n\nNode Action\n: Actions for managing node membership in the OpenMPF cluster.\n\n\nmpf list-nodes\n: If the Workflow Manager is running, get the current JGroups view; otherwise, list the core nodes\n\n\n\n\n\n\n\n\nKnown Issues\n\n\nThe following are known issues that are related to setting up and running OpenMPF. For a more complete list of known issues, please see the OpenMPF \nRelease Notes\n and \nworkboard\n.\n\n\nTest Exceptions\n\n\nWhen running the tests, you may observe warning log messages similar to this:\n\n\n//2016-07-25 16:16:27,848 WARN [Time-limited test] org.mitre.mpf.mst.TestSystem - Exception occurred while waiting. Assuming that the job has completed (but failed)\njava.lang.InterruptedException: null\n    at java.lang.Object.wait(Native Method) ~[na:1.8.0_60]\n    at java.lang.Object.wait(Object.java:502) ~[na:1.8.0_60]\n    at org.mitre.mpf.mst.TestSystem.waitFor(TestSystem.java:209) [test-classes/:na]\n    at org.mitre.mpf.mst.TestSystem.runPipelineOnMedia(TestSystem.java:201) [test-classes/:na]\n    at org.mitre.mpf.mst.TestSystemOnDiff.runSpeechSphinxDetectAudio(TestSystemOnDiff.java:250) [test-classes/:na]\n    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[na:1.8.0_60]\n    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[na:1.8.0_60]\n    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[na:1.8.0_60]\n    at java.lang.reflect.Method.invoke(Method.java:497) ~[na:1.8.0_60]\n    at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50) [junit-4.12.jar:4.12]\n    at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12) [junit-4.12.jar:4.12]\n    at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47) [junit-4.12.jar:4.12]\n    at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17) [junit-4.12.jar:4.12]\n    at org.springframework.test.context.junit4.statements.RunBeforeTestMethodCallbacks.evaluate(RunBeforeTestMethodCallbacks.java:75) [spring-test-4.2.5.RELEASE.jar:4.2.5.RELEASE]\n    at org.springframework.test.context.junit4.statements.RunAfterTestMethodCallbacks.evaluate(RunAfterTestMethodCallbacks.java:86) [spring-test-4.2.5.RELEASE.jar:4.2.5.RELEASE]\n    at org.springframework.test.context.junit4.statements.SpringRepeat.evaluate(SpringRepeat.java:84) [spring-test-4.2.5.RELEASE.jar:4.2.5.RELEASE]\n    at org.junit.internal.runners.statements.FailOnTimeout$CallableStatement.call(FailOnTimeout.java:298) [junit-4.12.jar:4.12]\n    at org.junit.internal.runners.statements.FailOnTimeout$CallableStatement.call(FailOnTimeout.java:292) [junit-4.12.jar:4.12]\n    at java.util.concurrent.FutureTask.run(FutureTask.java:266) [na:1.8.0_60]\n    at java.lang.Thread.run(Thread.java:745) [na:1.8.0_60]//\n\n\n\n\nThis does not necessarily indicate any type of software bug. The most likely cause for this message is that a test has timed out. Increasing the available system resources or increasing the test timeout values may help.\n\n\nProxy Configuration\n\n\nYum Package Manager Proxy Configuration\n\n\nBefore using the yum package manager,  it may be necessary to configure it to work with your environment's proxy settings. If credentials are not required, it is not necessary to add them to the yum configuration.\n\n\n\n\nsudo bash -c 'echo \"proxy=\naddress\n:\nport\n\" \n /etc/yum.conf'\n\n\nsudo bash -c 'echo \"proxy_username=\nusername\n\" \n /etc/yum.conf'\n\n\nsudo bash -c 'echo \"proxy_password=\npassword\n\" \n /etc/yum.conf'\n\n\n\n\nProxy Environment Variables\n\n\nIf your build environment is behind a proxy server, some applications and tools will need to be configured to use it. To configure an HTTP and HTTPS proxy, run the following commands. If credentials are not required, leave those fields blank.\n\n\n\n\nsudo bash -c 'echo \"export http_proxy=\nusername\n:\npassword\n@\nurl\n:\nport\n\" \n /etc/profile.d/mpf.sh\n\n\n. /etc/profile.d/mpf.sh\n\n\nsudo bash -c 'echo \"export https_proxy='${http_proxy}'\" \n /etc/profile.d/mpf.sh'\n\n\nsudo bash -c 'echo \"export HTTP_PROXY='${http_proxy}'\" \n /etc/profile.d/mpf.sh'\n\n\nsudo bash -c 'echo \"export HTTPS_PROXY='${http_proxy}'\" \n /etc/profile.d/mpf.sh'\n\n\n. /etc/profile.d/mpf.sh\n\n\n\n\nGit Proxy Configuration\n\n\nIf your build environment is behind a proxy server, git will need to be configured to use it. The following command will set the git global proxy. If the environment variable \n$http_proxy\n is not set, use the full proxy server address, port, and credentials (if needed).\n\n\ngit config --global http.proxy $http_proxy\n\n\n\n\nFirefox Proxy Configuration\n\n\nBefore running the integration tests and the web application, it may be necessary to configure Firefox with your environment's proxy settings.\n\n\n\n\nIn a new terminal window, type \nfirefox\n and press enter. This will launch a new Firefox window.\n\n\nIn the new Firefox window, enter \nabout:preferences#advanced\n in the URL text box and press enter.\n\n\nIn the left sidebar click \"Advanced\", then click the \nNetwork\n tab, and in the \nConnection\n section press the \"Settings...\" button.\n\n\nEnter the proxy settings for your environment.\n\n\nIn the \"No Proxy for:\" text box, verify that \nlocalhost\n is included.\n\n\nPress the \"OK\" button.\n\n\nClose all open Firefox instances.\n\n\n\n\nMaven Proxy Configuration\n\n\nBefore using Maven, it may be necessary to configure it to work with your environment's proxy settings. Open a new terminal window and run these commands. Afterwards, continue with adding the additional maven dependencies.\n\n\n\n\ncd /home/mpf\n\n\nmkdir -p .m2\n\n\ncp /opt/apache-maven/conf/settings.xml .m2/\n\n\nOpen the file \n.m2/settings.xml\n in a text editor.\n\n\nNavigate to the \nproxies\n section (line 85).\n\n\nThere is a commented-out example proxy server specification. Copy and paste the example specification below the commented-out section, but before the end of the closing \n/proxies\n tag.\n\n\nFill in your environment's proxy server information. For additional help and information, please see the Apache Maven guide to configuring a proxy server at \nhttps://maven.apache.org/guides/mini/guide-proxies.html\n.\n\n\n\n\nSSL Inspection\n\n\nIf your build environment is behind a proxy server that performs SSL inspection, some applications and tools will need to be configured to accommodate it. The following steps will add trusted certificates to your development machine.\n\n\nAdditional information on Java keytool can be found at \nhttps://docs.oracle.com/javase/8/docs/technotes/tools/unix/keytool.html\n.\n\n\n\n\nDownload any certificates needed for using SSL in your build environment to \n/home/mpf/Downloads\n.\n\n\nFor each certificate, run this command, filling in the values for certificate alias, certificate name, and keystore passphrase:\n    \n \nsudo $JAVA_HOME/bin/keytool -import -alias \ncertificate alias\n -file /home/mpf/Downloads/\ncertificate name\n.crt -keystore \"$JAVA_HOME/jre/lib/security/cacerts\" -storepass \nkeystore passphrase\n -noprompt\n\n\nFor each certificate, run this command, filling in the value for certificate name:\n    \n \nsudo cp /home/mpf/Downloads/\ncertificate name\n.crt /tmp\n\n\nFor each certificate, run this command, filling in the values for certificate name:\n    \n \nsudo -u root -H sh -c \"openssl x509 -in /tmp/\ncertificate name\n.crt    -text \n /etc/pki/ca-trust/source/anchors/\ncertificate name\n.pem\"\n\n\nRun these commands once:\n\n\nsudo -u root -H sh -c \"update-ca-trust enable\"\n\n\nsudo -u root -H sh -c \"update-ca-trust extract\"\n\n\n\n\n\n\nRun these commands once, filling in the value for the root certificate name:\n\n\nsudo cp /etc/pki/tls/certs/ca-bundle.crt /etc/pki/tls/certs/ca-bundle.crt.original\n\n\nsudo -u root -H sh -c \"cat /etc/pki/ca-trust/source/anchors/\nroot certificate name\n.pem \n /etc/pki/tls/certs/ca-bundle.crt\"\n\n\n\n\n\n\n\n\nAlternatively, if adding certificates is not an option, or difficulties are encountered, you may optionally skip SSL certificate verification for these tools. This is not recommended:\n\n\nwget\n\n\n\n\ncd /home/mpf\n\n\ntouch /home/mpf/.wgetrc\n\n\nIn a text editor, open the file \n/home/mpf/.wgetrc\n\n\n\n\nAdd this line:\n\n\ncheck_certificate=off\n\n\n\n\n\n\nSave and close the file.\n\n\n\n\n. /home/mpf/.wgetrc\n\n\n\n\ngit\n\n\n\n\ncd /home/mpf\n\n\ngit config http.sslVerify false\n\n\ngit config --global http.sslVerify false\n\n\n\n\nmaven\n\n\n\n\nIn a text editor, open the file \n/etc/profile.d/mpf.sh\n\n\nAt the bottom of the file, add this line:\n\nexport MAVEN_OPTS=\"-Dmaven.wagon.http.ssl.insecure=true -Dmaven.wagon.http.ssl.allowall=true -Dmaven.wagon.http.ssl.ignore.validity.dates=true\"\n\n\nSave and close the file.\n\n\n. /etc/profile.d/mpf.sh\n\n\n\n\nDevelopment Tools\n\n\nWhen developing for OpenMPF, you may find the following tools helpful:\n\n\n\n\nJenkins\n: \nhttps://jenkins.io\n\n\nIntelliJ\n: \nhttps://www.jetbrains.com/idea/\n\n\nCLion\n: \nhttps://www.jetbrains.com/clion\n\n\nPyCharm\n: \nhttps://www.jetbrains.com/pycharm", 
            "title": "Development Environment Guide"
        }, 
        {
            "location": "/Development-Environment-Guide/index.html#overview", 
            "text": "The following instructions are for setting up an environment for building OpenMPF outside of Docker.  If your environment is behind a proxy server, please refer to the  Proxy Configuration  and  SSL Inspection  appendix sections before continuing. Keep them in mind throughout this guide and perform the configuration steps as necessary.", 
            "title": "Overview"
        }, 
        {
            "location": "/Development-Environment-Guide/index.html#install-centos-7", 
            "text": "The recommended minimum system specifications are:   Memory : 8192MB  CPU : 4  Disk : 40GB on a SSD   Install  CentOS 7 . Most developers use a virtual machine for development.", 
            "title": "Install CentOS 7"
        }, 
        {
            "location": "/Development-Environment-Guide/index.html#install-system-dependencies-using-package-managers", 
            "text": "", 
            "title": "Install System Dependencies Using Package Managers"
        }, 
        {
            "location": "/Development-Environment-Guide/index.html#configure-additional-repositories", 
            "text": "Install the Remi Repo for Redis:  wget -P /home/mpf/Downloads \"http://rpms.remirepo.net/RPM-GPG-KEY-remi\"  wget -P /home/mpf/Downloads \"http://rpms.famillecollet.com/enterprise/remi-release-7.rpm\"  sudo rpm --import /home/mpf/Downloads/RPM-GPG-KEY-remi  sudo rpm -Uvh /home/mpf/Downloads/remi-release-7.rpm  sudo yum-config-manager --enable remi    Create an  /apps  directory and package subdirectories:  sudo mkdir -p /apps/install/lib  sudo mkdir -p /apps/bin/apache  sudo mkdir /apps/ansible  sudo mkdir -p /apps/source/cmake_sources  sudo mkdir /apps/source/apache_sources  sudo mkdir /apps/source/google_sources  sudo mkdir /apps/source/opencv_sources  sudo mkdir /apps/source/ffmpeg_sources  sudo mkdir /apps/source/dlib-sources  sudo mkdir /apps/source/openalpr_sources  sudo mkdir /apps/source/ansible_sources  sudo chown -R mpf:mpf /apps  sudo chmod -R 755 /apps    Create the OpenMPF  ldconfig  file:\n     sudo touch /etc/ld.so.conf.d/mpf-x86_64.conf  Add  /apps/install/lib  to the OpenMPF  ldconfig  file:\n     sudo sh -c 'echo \"/apps/install/lib\"   /etc/ld.so.conf.d/mpf-x86_64.conf'  Update the shared library cache:\n     sudo ldconfig", 
            "title": "Configure Additional Repositories"
        }, 
        {
            "location": "/Development-Environment-Guide/index.html#install-system-dependencies-via-yum", 
            "text": "Use  yum  to install packages:  sudo yum install -y asciidoc autoconf automake boost boost-devel cmake3 curl freetype-devel gcc-c++ git graphviz gstreamer-plugins-base-devel gtk2-devel gtkglext-devel gtkglext-libs jasper jasper-devel libavc1394-devel libcurl-devel libdc1394-devel libffi-devel libICE-devel libjpeg-turbo-devel libpng-devel libSM-devel libtiff-devel libtool libv4l-devel libXinerama-devel libXmu-devel libXt-devel log4cplus log4cplus-devel log4cxx log4cxx-devel make mercurial mesa-libGL-devel mesa-libGLU-devel nasm ncurses-devel numpy openssl-devel pangox-compat pangox-compat-devel perl-CPAN-Meta-YAML perl-DBI perl-Digest-MD5 perl-File-Find-Rule perl-File-Find-Rule-Perl perl-JSON perl-JSON-PP perl-List-Compare perl-Number-Compare perl-Params-Util perl-Parse-CPAN-Meta php pkgconfig qt qt-devel qt-x11 redis rpm-build sshpass tbb tbb-devel tree unzip uuid-devel wget yasm yum-utils zlib-devel", 
            "title": "Install System Dependencies via Yum"
        }, 
        {
            "location": "/Development-Environment-Guide/index.html#get-the-openmpf-source-code", 
            "text": "Clone the OpenMPF projects repository:   cd /home/mpf  git clone https://github.com/openmpf/openmpf-projects.git --recursive     Install the OpenMPF command line tools:\n       sudo pip3 install /home/mpf/openmpf-projects/openmpf/trunk/bin/mpf-scripts    Copy the mpf user profile script from the extracted source code:\n       sudo cp /home/mpf/openmpf-projects/openmpf/trunk/mpf-install/src/main/scripts/mpf-profile.sh /etc/profile.d/mpf.sh    Add  /apps/install/bin  to the system  PATH  variable:   sudo sh -c 'echo \"PATH=\\$PATH:/apps/install/bin\"   /etc/profile.d/mpf.sh'  . /etc/profile.d/mpf.sh     For more information on the command line tools, please refer to the  Command Line Tools  section below.", 
            "title": "Get the OpenMPF Source Code"
        }, 
        {
            "location": "/Development-Environment-Guide/index.html#add-maven-dependencies", 
            "text": "Some Maven dependencies needed for OpenMPF are not publicly available.   Download    mpf-maven-deps.tar.gz  to  /home/mpf/openmpf-projects/openmpf-build-tools/mpf-maven-deps.tar.gz .  Set up the local Maven repository:  cd /home/mpf  mkdir -p .m2/repository    Extract the archive to the local Maven repository: tar xvzf /home/mpf/openmpf-projects/openmpf-build-tools/mpf-maven-deps.tar.gz -C /home/mpf/.m2/repository/", 
            "title": "Add Maven Dependencies"
        }, 
        {
            "location": "/Development-Environment-Guide/index.html#build-and-install-system-dependencies-in-dockerfiles", 
            "text": "Refer to the  openmpf_build   Dockerfile , and execute the  RUN  steps to build/install the required dependencies in your local development environment. Use your best judgement. You will need to run some commands with  sudo  root privileges. Some of these may be redundant with the steps you've followed so far. Installation of the NVIDIA CUDA Toolkit is optional. Refer to the  NVIDIA CUDA Toolkit  section below.  The  openmpf_build  Dockerfile may not include the dependencies you need to develop specific components. Refer to the Dockerfile for each of those components to determine which dependencies they require.", 
            "title": "Build and Install System Dependencies in Dockerfiles"
        }, 
        {
            "location": "/Development-Environment-Guide/index.html#nvidia-cuda-toolkit", 
            "text": "Installation of the NVIDIA CUDA Toolkit is optional, and only necessary if you need to run a component on a GPU. Many components that support GPU processing also support execution on the CPU, and if this toolkit is not found in the build environment, the build system will automatically build those components for CPU processing only. For a discussion of NVIDIA GPU support in OpenMPF components, see the  GPU Support Guide .   NOTE:  To run OpenMPF components that use the NVIDIA GPUs, you must ensure that the deployment machine has the same version of this Toolkit installed, including the NVIDIA GPU drivers. The instructions here are for a development environment only, and thus do not include steps to install the drivers. If you also need to set up the deployment machine, please see the full instructions at  https://docs.nvidia.com/cuda/cuda-installation-guide-linux/index.html .", 
            "title": "NVIDIA CUDA Toolkit"
        }, 
        {
            "location": "/Development-Environment-Guide/index.html#build-and-install-additional-system-dependencies", 
            "text": "", 
            "title": "Build and Install Additional System Dependencies"
        }, 
        {
            "location": "/Development-Environment-Guide/index.html#apache-activemq-5130", 
            "text": "For reference only:  http://activemq.apache.org   cd /apps/bin/apache  wget -O /apps/bin/apache/apache-activemq-5.13.0-bin.tar.gz \"https://archive.apache.org/dist/activemq/5.13.0/apache-activemq-5.13.0-bin.tar.gz\"  sudo tar xvzf apache-activemq-5.13.0-bin.tar.gz -C /opt/  sudo chown -R mpf:mpf /opt/apache-activemq-5.13.0  sudo chmod -R 755 /opt/apache-activemq-5.13.0  sudo ln -s /opt/apache-activemq-5.13.0 /opt/activemq", 
            "title": "Apache ActiveMQ 5.13.0"
        }, 
        {
            "location": "/Development-Environment-Guide/index.html#apache-ant-196", 
            "text": "For reference only:  http://ant.apache.org   cd /apps/bin/apache  wget -O /apps/bin/apache/apache-ant-1.9.6-bin.tar.gz \"https://archive.apache.org/dist/ant/binaries/apache-ant-1.9.6-bin.tar.gz\"  tar xzvf apache-ant-1.9.6-bin.tar.gz  sudo cp -R /apps/bin/apache/apache-ant-1.9.6 /apps/install/  sudo chown -R mpf:mpf /apps/install/apache-ant-1.9.6  sudo sed -i '/^PATH/s/$/:\\/apps\\/install\\/apache-ant-1.9.6\\/bin/' /etc/profile.d/mpf.sh  . /etc/profile.d/mpf.sh", 
            "title": "Apache Ant 1.9.6"
        }, 
        {
            "location": "/Development-Environment-Guide/index.html#postgresql", 
            "text": "Add PostgreSQL repository: sudo yum install -y https://download.postgresql.org/pub/repos/yum/reporpms/EL-7-x86_64/pgdg-redhat-repo-latest.noarch.rpm  Install postgres with yum: sudo yum install -y postgresql12-server  Initialize postgres: sudo /usr/pgsql-12/bin/postgresql-12-setup initdb  Modify the default authentication method for localhost.  Open  /var/lib/pgsql/12/data/pg_hba.conf  in a text editor.  Find the line containing: \n     host    all             all             127.0.0.1/32            ident \n   It should be line 82. Change \"ident\" to \"md5\".  Find the line containing: \n     host    all             all             ::1/128                 ident \n   It should be line 84. Change \"ident\" to \"md5\".    Start postgres: sudo service postgresql-12 start  Create the mpf user. When prompted for a password use \"password\": sudo -i -u postgres createuser -P mpf  Create the mpf database with the mpf user as the owner: sudo -i -u postgres createdb -O mpf mpf", 
            "title": "PostgreSQL"
        }, 
        {
            "location": "/Development-Environment-Guide/index.html#python-38", 
            "text": "Install build dependencies:\n      sudo yum install -y yum-utils \n    sudo yum-builddep -y python3    Download the source code: \n    curl https://www.python.org/ftp/python/3.8.2/Python-3.8.2.tar.xz | tar --extract --xz    Build Python:\n      cd Python-3.8.2 \n      ./configure --enable-optimizations --with-lto --enable-shared \n      make -j8 \n      sudo make install \n      sudo ln -s /usr/local/lib/libpython3.8.so.1.0 /usr/lib64/libpython3.8.so.1.0 \n      sudo ln -sf /usr/local/bin/python3 /bin/python3 \n      sudo ln -sf /usr/local/bin/python3.8 /bin/python3.8 \n      sudo ln -sf /usr/local/bin/pip3 /bin/pip3 \n      sudo ln -sf /usr/local/bin/pip3.8 /bin/pip3.8    Make sure the output of running  python3 --version  is  Python 3.8.2 .    Make sure the output of running  sudo python3 --version  is  Python 3.8.2 .    Make sure the output of running  pip3 --version  ends with  (python 3.8) .    Make sure the output of running  sudo pip3 --version  ends with  (python 3.8) .    Upgrade pip:\n      sudo pip3 install --upgrade pip    Install wheel:\n      sudo pip3 install wheel", 
            "title": "Python 3.8"
        }, 
        {
            "location": "/Development-Environment-Guide/index.html#configure-system-dependencies", 
            "text": "", 
            "title": "Configure System Dependencies"
        }, 
        {
            "location": "/Development-Environment-Guide/index.html#configure-activemq", 
            "text": "Some additional manual configuration of ActiveMQ is required. For each step, open the specified file in a text editor, make the change, and save the file. If ActiveMQ is running, please stop it before making these changes.  Use Additional Memory  In  /opt/activemq/bin/env  (line 27), comment out the line:  ACTIVEMQ_OPTS_MEMORY= -Xms64M -Xmx1G   so that it reads:  #ACTIVEMQ_OPTS_MEMORY= -Xms64M -Xmx1G   Disable Persistence  In  /opt/activemq/conf/activemq.xml  (line 40), change the line:  broker xmlns= http://activemq.apache.org/schema/core  brokerName= localhost  dataDirectory= ${activemq.data}   so that it reads:  broker xmlns= http://activemq.apache.org/schema/core  brokerName= localhost  dataDirectory= ${activemq.data}  persistent= false   Respect Priorities  In  /opt/activemq/conf/activemq.xml  (line 44) under the line:  policyEntries   add the line:  policyEntry queue=  prioritizedMessages= true  useCache= false  expireMessagesPeriod= 0  queuePrefetch= 1  /   Enable JMX  In  /opt/activemq/conf/activemq.xml  (line 72, after making the above addition), change the line:  managementContext createConnector= false /   so that it reads:  managementContext createConnector= true /   Change Log Conversion Pattern  In  /opt/activemq/conf/log4j.properties  (line 52), change the line:  log4j.appender.logfile.layout.ConversionPattern=%d | %-5p | %m | %c | %t%n  so that it reads:  log4j.appender.logfile.layout.ConversionPattern=%d %p [%t] %c - %m%n", 
            "title": "Configure ActiveMQ"
        }, 
        {
            "location": "/Development-Environment-Guide/index.html#configure-redis", 
            "text": "Redis should be set to run in the background (i.e. as a daemon process).  In  /etc/redis.conf  (line 136), change the line:  daemonize no  so that it reads:  daemonize yes", 
            "title": "Configure Redis"
        }, 
        {
            "location": "/Development-Environment-Guide/index.html#configure-users", 
            "text": "To change the default user password settings, modify  /home/mpf/openmpf-projects/openmpf/trunk/workflow-manager/src/main/resources/properties/user.properties . Note that the default settings are public knowledge, which could be a security risk.  Note that  mpf remove-user  and  mpf add-user  commands explained in the  Command Line Tools  section do not modify the  user.properties  file. If you remove a user using the  mpf remove-user  command, the changes will take effect at runtime, but an entry may still exist for that user in the  user.properties  file. If so, then the user account will be recreated the next time the Workflow Manager is restarted.", 
            "title": "Configure Users"
        }, 
        {
            "location": "/Development-Environment-Guide/index.html#configure-tomcat-with-http", 
            "text": "When developing OpenMPF on a local machine, it is often most convenient to configure Tomcat to use HTTP instead of HTTPS.   Open the file  /opt/apache-tomcat/conf/server.xml  in a text editor.  Below the commented out section on lines 87 through 90, remove the following lines if they exist: Connector SSLEnabled=\"true\" acceptCount=\"100\" clientAuth=\"false\"\n    disableUploadTimeout=\"true\" enableLookups=\"false\" maxThreads=\"25\"\n    port=\"8443\" keystoreFile=\"/home/mpf/.keystore\" keystorePass=\"mpf123\"\n    protocol=\"org.apache.coyote.http11.Http11NioProtocol\" scheme=\"https\"\n    secure=\"true\" sslProtocol=\"TLS\" /  Save and close the file.  Create the file  /opt/apache-tomcat/bin/setenv.sh  and open it in a text editor.  Add the following line: export CATALINA_OPTS=\"-server -Xms256m -XX:PermSize=512m -XX:MaxPermSize=512m -Djava.library.path=$MPF_HOME/lib -Dtransport.guarantee='NONE' -Dweb.rest.protocol='http'\"  Save and close the file.", 
            "title": "Configure Tomcat with HTTP"
        }, 
        {
            "location": "/Development-Environment-Guide/index.html#optional-configure-tomcat-with-https", 
            "text": "Alternatively, OpenMPF can also be run using HTTPS instead of HTTP.  Generate a self-signed certificate and keystore  A valid keystore is required to run OpenMPF with HTTPS support. These instructions will generate a keystore that should be used for local builds only. When deploying OpenMPF, a keystore containing a valid certificate trust chain should be used.   Open a new terminal window.  sudo systemctl stop tomcat7  cd /home/mpf  $JAVA_HOME/bin/keytool -genkey -alias tomcat -keyalg RSA  At the prompt, enter a keystore password of:  mpf123  Re-enter the keystore password of:  mpf123  At the  What is your first and last name?  prompt, press the Enter key for a blank value.  At the  What is the name of your organizational unit?  , press the Enter key for a blank value.  At the  What is the name of your organization?  prompt, press the Enter key for a blank value.  At the  What is the name of your City or Locality?  prompt, press the Enter key for a blank value.  At the  What is the name of your State or Province?  prompt, press the Enter key for a blank value.  At the  What is the two-letter country code for this unit?  prompt, press the Enter key for a blank value.  At the  Is CN=Unknown, OU=Unknown, O=Unknown, L=Unknown, ST=Unknown, C=Unknown correct?  prompt, type  yes  and press the Enter key to accept the values.  At the  Enter key password for  tomcat  prompt, press the Enter key for a blank value.  Verify the file  /home/mpf/.keystore  was created at the current time.   Tomcat Configuration   Open the file  /opt/apache-tomcat/conf/server.xml  in a text editor.  Below the commented out section on lines 87 through 90, add the following lines: Connector SSLEnabled=\"true\" acceptCount=\"100\" clientAuth=\"false\"\n    disableUploadTimeout=\"true\" enableLookups=\"false\" maxThreads=\"25\"\n    port=\"8443\" keystoreFile=\"/home/mpf/.keystore\" keystorePass=\"mpf123\"\n    protocol=\"org.apache.coyote.http11.Http11NioProtocol\" scheme=\"https\"\n    secure=\"true\" sslProtocol=\"TLS\" /  Save and close the file.  Create the file  /opt/apache-tomcat/bin/setenv.sh  and open it in a text editor.  Add the following line: export CATALINA_OPTS=\"-server -Xms256m -XX:PermSize=512m -XX:MaxPermSize=512m -Djava.library.path=$MPF_HOME/lib -Dtransport.guarantee='CONFIDENTIAL' -Dweb.rest.protocol='https'\"  Save and close the file.   Using an IDE  If running Tomcat from an IDE, such as IntelliJ, then  -Dtransport.guarantee=\"CONFIDENTIAL\" -Dweb.rest.protocol=\"https\"  should be added at the end of the Tomcat VM arguments for your Tomcat run configuration. It is not necessary to add these arguments when running tomcat from the command line or a systemd command because of the configured  CATALINA_OPTS  variable.", 
            "title": "(Optional) Configure Tomcat with HTTPS"
        }, 
        {
            "location": "/Development-Environment-Guide/index.html#build-and-run-the-openmpf-workflow-manager-web-application", 
            "text": "Run the following commands to build OpenMPF and launch the web application. Use this value for  configFile : /home/mpf/openmpf-projects/openmpf/trunk/jenkins/scripts/config_files/openmpf-open-source-package.json   cd /home/mpf/openmpf-projects/openmpf   Copy the development properties file into place:  cp trunk/workflow-manager/src/main/resources/properties/mpf-private-example.properties trunk/workflow-manager/src/main/resources/properties/mpf-private.properties    Open  /etc/ansible/hosts  in a text editor.  sudo  is required to edit this file.    If they do not already exist, add these two lines above  # Ex 1: Ungrouped hosts, specify before any group headers.  (line 11):  [mpf-child]\nlocalhost.localdomain    Save and close the file.   mvn clean install -DskipTests -Dmaven.test.skip=true -DskipITs -Dmaven.tomcat.skip=true  -Dcomponents.build.package.json= configFile  -Dstartup.auto.registration.skip=false -Dcomponents.build.dir=/home/mpf/openmpf-projects/openmpf/mpf-component-build  cd /home/mpf/openmpf-projects/openmpf/trunk/workflow-manager  rm -rf /opt/apache-tomcat/webapps/workflow-manager*  cp target/workflow-manager.war /opt/apache-tomcat/webapps/workflow-manager.war  cd ../..  sudo cp trunk/install/libexec/node-manager /etc/init.d/  sudo systemctl daemon-reload  mpf start   The web application should start running in the background as a daemon. Look for this log message in the Tomcat log ( /opt/apache-tomcat/logs/catalina.out ) with a time value indicating the Workflow Manager has finished starting:  INFO: Server startup in 39030 ms  After startup, the Workflow Manager will be available at  http://localhost:8080/workflow-manager  (or  https://localhost:8443/workflow-manager  if configured to use HTTPS instead of HTTP). Browse to this URL using FireFox or Chrome.  If you want to test regular user capabilities, log in as the \"mpf\" user with the \"mpf123\" password. Please see the  OpenMPF User Guide  for more information. Alternatively, if you want to test admin capabilities then log in as \"admin\" user with the \"mpfadm\" password. Please see the  OpenMPF Admin Guide  for more information. When finished using OpenMPF, run  mpf stop .  The preferred method to start and stop services for OpenMPF is with the  mpf start  and  mpf stop  commands. For additional information on these commands, please see the  Command Line Tools  section of the  OpenMPF Admin Guide . These will start and stop the ActiveMQ, PostgreSQL, Redis, Node Manager, and Tomcat system processes.  For debugging purposes, it may be helpful to manually start the Tomcat service in a separate terminal window to display the log output. To do that, use  mpf start --xtc  to start ActiveMQ, PostgreSQL, Redis, and the Node Manager without starting Tomcat. Then, in another terminal windows run:  /opt/apache-tomcat/bin/catalina.sh run  Press  ctrl-c  in the Tomcat window to stop Tomcat.", 
            "title": "Build and Run the OpenMPF Workflow Manager Web Application"
        }, 
        {
            "location": "/Development-Environment-Guide/index.html#optional-test-openmpf", 
            "text": "Run the following commands to build OpenMPF and run the integration tests. Use this value for  configFile : /home/mpf/openmpf-projects/openmpf/trunk/jenkins/scripts/config_files/openmpf-open-source-package.json   cd /home/mpf/openmpf-projects/openmpf   Copy the development properties file into place:  cp trunk/workflow-manager/src/main/resources/properties/mpf-private-example.properties trunk/workflow-manager/src/main/resources/properties/mpf-private.properties    Open the file  /etc/ansible/hosts  in a text editor.  sudo  is required to edit this file.    If they do not already exist, add these two lines above  # Ex 1: Ungrouped hosts, specify before any group headers.  (line 11):  [mpf-child]\nlocalhost.localdomain    Save and close the file.   mvn clean install -DskipTests -Dmaven.test.skip=true -DskipITs -Dmaven.tomcat.skip=true -Dcomponents.build.package.json= configFile  -Dcomponents.build.dir=/home/mpf/openmpf-projects/openmpf/mpf-component-build -Dstartup.auto.registration.skip=false  sudo cp /home/mpf/openmpf-projects/openmpf/trunk/install/libexec/node-manager /etc/init.d/  sudo systemctl daemon-reload  mpf start --xtc  mvn verify -Pjenkins -Dtransport.guarantee=\"NONE\" -Dweb.rest.protocol=\"http\" -Dcomponents.build.package.json= configFile  -Dstartup.auto.registration.skip=false -Dcomponents.build.dir=/home/mpf/openmpf-projects/openmpf/mpf-component-build  mpf stop --xtc   Please see the appendix section  Known Issues  regarding any  java.lang.InterruptedException: null  warning log messages observed when running the tests.  Run this command to clean up and remove all traces of the test run:   mpf clean --delete-uploaded-media --delete-logs  Type \"Y\" and press Enter.   If you choose not to run  mpf clean  before following the  Build and Run the OpenMPF Workflow Manager Web Application  steps, the Job Status table will be pre-populated with some entries; however, the input media, markup, and JSON output objects for those jobs will not be available.", 
            "title": "(Optional) Test OpenMPF"
        }, 
        {
            "location": "/Development-Environment-Guide/index.html#appendices", 
            "text": "", 
            "title": "Appendices"
        }, 
        {
            "location": "/Development-Environment-Guide/index.html#command-line-tools", 
            "text": "OpenMPF installs command line tools that can be accessed through a terminal on the development machine. All of the tools take the form of actions:  mpf  action  [options ...] . Note that tab-completion is enabled for ease of use.  Execute  mpf --help  for general documentation and  mpf  action  --help  for documentation about a specific action.   Start / Stop Actions : Actions for starting and stopping the OpenMPF system dependencies, including PostgreSQL, ActiveMQ, Redis, Tomcat, and the node managers on the various nodes in the OpenMPF cluster.  mpf status : displays a message indicating whether each of the system dependencies is running or not  mpf start : starts all of the system dependencies  mpf stop : stops all of the system dependencies  mpf restart  : stops and then starts all of the system dependencies    User Actions : Actions for managing Workflow Manager user accounts. If changes are made to an existing user then that user will need to log off or the Workflow Manager will need to be restarted for the changes to take effect.  mpf list-users  : lists all of the existing user accounts and their role (non-admin or admin)  mpf add-user  username   role : adds a new user account; will be prompted to enter the account password  mpf remove-user  username  : removes an existing user account  mpf change-role  username   role  : change the role (non-admin to admin or vice versa) for an existing user  mpf change-password  username : change the password for an existing user; will be prompted to enter the new account password    Clean Actions : Actions to remove old data and revert the system to a new install state. User accounts, registered components, as well as custom actions, tasks, and pipelines, are preserved.  mpf clean : cleans out old job information and results, pending job requests, marked up media files, and ActiveMQ data, but preserves log files and uploaded media  mpf clean --delete-logs --delete-uploaded-media : the same as  mpf clean  but also deletes log files and uploaded media    Node Action : Actions for managing node membership in the OpenMPF cluster.  mpf list-nodes : If the Workflow Manager is running, get the current JGroups view; otherwise, list the core nodes", 
            "title": "Command Line Tools"
        }, 
        {
            "location": "/Development-Environment-Guide/index.html#known-issues", 
            "text": "The following are known issues that are related to setting up and running OpenMPF. For a more complete list of known issues, please see the OpenMPF  Release Notes  and  workboard .  Test Exceptions  When running the tests, you may observe warning log messages similar to this:  //2016-07-25 16:16:27,848 WARN [Time-limited test] org.mitre.mpf.mst.TestSystem - Exception occurred while waiting. Assuming that the job has completed (but failed)\njava.lang.InterruptedException: null\n    at java.lang.Object.wait(Native Method) ~[na:1.8.0_60]\n    at java.lang.Object.wait(Object.java:502) ~[na:1.8.0_60]\n    at org.mitre.mpf.mst.TestSystem.waitFor(TestSystem.java:209) [test-classes/:na]\n    at org.mitre.mpf.mst.TestSystem.runPipelineOnMedia(TestSystem.java:201) [test-classes/:na]\n    at org.mitre.mpf.mst.TestSystemOnDiff.runSpeechSphinxDetectAudio(TestSystemOnDiff.java:250) [test-classes/:na]\n    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[na:1.8.0_60]\n    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[na:1.8.0_60]\n    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[na:1.8.0_60]\n    at java.lang.reflect.Method.invoke(Method.java:497) ~[na:1.8.0_60]\n    at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50) [junit-4.12.jar:4.12]\n    at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12) [junit-4.12.jar:4.12]\n    at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47) [junit-4.12.jar:4.12]\n    at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17) [junit-4.12.jar:4.12]\n    at org.springframework.test.context.junit4.statements.RunBeforeTestMethodCallbacks.evaluate(RunBeforeTestMethodCallbacks.java:75) [spring-test-4.2.5.RELEASE.jar:4.2.5.RELEASE]\n    at org.springframework.test.context.junit4.statements.RunAfterTestMethodCallbacks.evaluate(RunAfterTestMethodCallbacks.java:86) [spring-test-4.2.5.RELEASE.jar:4.2.5.RELEASE]\n    at org.springframework.test.context.junit4.statements.SpringRepeat.evaluate(SpringRepeat.java:84) [spring-test-4.2.5.RELEASE.jar:4.2.5.RELEASE]\n    at org.junit.internal.runners.statements.FailOnTimeout$CallableStatement.call(FailOnTimeout.java:298) [junit-4.12.jar:4.12]\n    at org.junit.internal.runners.statements.FailOnTimeout$CallableStatement.call(FailOnTimeout.java:292) [junit-4.12.jar:4.12]\n    at java.util.concurrent.FutureTask.run(FutureTask.java:266) [na:1.8.0_60]\n    at java.lang.Thread.run(Thread.java:745) [na:1.8.0_60]//  This does not necessarily indicate any type of software bug. The most likely cause for this message is that a test has timed out. Increasing the available system resources or increasing the test timeout values may help.", 
            "title": "Known Issues"
        }, 
        {
            "location": "/Development-Environment-Guide/index.html#proxy-configuration", 
            "text": "Yum Package Manager Proxy Configuration  Before using the yum package manager,  it may be necessary to configure it to work with your environment's proxy settings. If credentials are not required, it is not necessary to add them to the yum configuration.   sudo bash -c 'echo \"proxy= address : port \"   /etc/yum.conf'  sudo bash -c 'echo \"proxy_username= username \"   /etc/yum.conf'  sudo bash -c 'echo \"proxy_password= password \"   /etc/yum.conf'   Proxy Environment Variables  If your build environment is behind a proxy server, some applications and tools will need to be configured to use it. To configure an HTTP and HTTPS proxy, run the following commands. If credentials are not required, leave those fields blank.   sudo bash -c 'echo \"export http_proxy= username : password @ url : port \"   /etc/profile.d/mpf.sh  . /etc/profile.d/mpf.sh  sudo bash -c 'echo \"export https_proxy='${http_proxy}'\"   /etc/profile.d/mpf.sh'  sudo bash -c 'echo \"export HTTP_PROXY='${http_proxy}'\"   /etc/profile.d/mpf.sh'  sudo bash -c 'echo \"export HTTPS_PROXY='${http_proxy}'\"   /etc/profile.d/mpf.sh'  . /etc/profile.d/mpf.sh   Git Proxy Configuration  If your build environment is behind a proxy server, git will need to be configured to use it. The following command will set the git global proxy. If the environment variable  $http_proxy  is not set, use the full proxy server address, port, and credentials (if needed).  git config --global http.proxy $http_proxy  Firefox Proxy Configuration  Before running the integration tests and the web application, it may be necessary to configure Firefox with your environment's proxy settings.   In a new terminal window, type  firefox  and press enter. This will launch a new Firefox window.  In the new Firefox window, enter  about:preferences#advanced  in the URL text box and press enter.  In the left sidebar click \"Advanced\", then click the  Network  tab, and in the  Connection  section press the \"Settings...\" button.  Enter the proxy settings for your environment.  In the \"No Proxy for:\" text box, verify that  localhost  is included.  Press the \"OK\" button.  Close all open Firefox instances.   Maven Proxy Configuration  Before using Maven, it may be necessary to configure it to work with your environment's proxy settings. Open a new terminal window and run these commands. Afterwards, continue with adding the additional maven dependencies.   cd /home/mpf  mkdir -p .m2  cp /opt/apache-maven/conf/settings.xml .m2/  Open the file  .m2/settings.xml  in a text editor.  Navigate to the  proxies  section (line 85).  There is a commented-out example proxy server specification. Copy and paste the example specification below the commented-out section, but before the end of the closing  /proxies  tag.  Fill in your environment's proxy server information. For additional help and information, please see the Apache Maven guide to configuring a proxy server at  https://maven.apache.org/guides/mini/guide-proxies.html .", 
            "title": "Proxy Configuration"
        }, 
        {
            "location": "/Development-Environment-Guide/index.html#ssl-inspection", 
            "text": "If your build environment is behind a proxy server that performs SSL inspection, some applications and tools will need to be configured to accommodate it. The following steps will add trusted certificates to your development machine.  Additional information on Java keytool can be found at  https://docs.oracle.com/javase/8/docs/technotes/tools/unix/keytool.html .   Download any certificates needed for using SSL in your build environment to  /home/mpf/Downloads .  For each certificate, run this command, filling in the values for certificate alias, certificate name, and keystore passphrase:\n       sudo $JAVA_HOME/bin/keytool -import -alias  certificate alias  -file /home/mpf/Downloads/ certificate name .crt -keystore \"$JAVA_HOME/jre/lib/security/cacerts\" -storepass  keystore passphrase  -noprompt  For each certificate, run this command, filling in the value for certificate name:\n       sudo cp /home/mpf/Downloads/ certificate name .crt /tmp  For each certificate, run this command, filling in the values for certificate name:\n       sudo -u root -H sh -c \"openssl x509 -in /tmp/ certificate name .crt    -text   /etc/pki/ca-trust/source/anchors/ certificate name .pem\"  Run these commands once:  sudo -u root -H sh -c \"update-ca-trust enable\"  sudo -u root -H sh -c \"update-ca-trust extract\"    Run these commands once, filling in the value for the root certificate name:  sudo cp /etc/pki/tls/certs/ca-bundle.crt /etc/pki/tls/certs/ca-bundle.crt.original  sudo -u root -H sh -c \"cat /etc/pki/ca-trust/source/anchors/ root certificate name .pem   /etc/pki/tls/certs/ca-bundle.crt\"     Alternatively, if adding certificates is not an option, or difficulties are encountered, you may optionally skip SSL certificate verification for these tools. This is not recommended:  wget   cd /home/mpf  touch /home/mpf/.wgetrc  In a text editor, open the file  /home/mpf/.wgetrc   Add this line:  check_certificate=off    Save and close the file.   . /home/mpf/.wgetrc   git   cd /home/mpf  git config http.sslVerify false  git config --global http.sslVerify false   maven   In a text editor, open the file  /etc/profile.d/mpf.sh  At the bottom of the file, add this line: export MAVEN_OPTS=\"-Dmaven.wagon.http.ssl.insecure=true -Dmaven.wagon.http.ssl.allowall=true -Dmaven.wagon.http.ssl.ignore.validity.dates=true\"  Save and close the file.  . /etc/profile.d/mpf.sh", 
            "title": "SSL Inspection"
        }, 
        {
            "location": "/Development-Environment-Guide/index.html#development-tools", 
            "text": "When developing for OpenMPF, you may find the following tools helpful:   Jenkins :  https://jenkins.io  IntelliJ :  https://www.jetbrains.com/idea/  CLion :  https://www.jetbrains.com/clion  PyCharm :  https://www.jetbrains.com/pycharm", 
            "title": "Development Tools"
        }, 
        {
            "location": "/Acknowledgements/index.html", 
            "text": "The OpenMPF acknowledgements are listed on the About page available within the OpenMPF Workflow Manager web application. Note that it does not include all of the dependencies for all of the OpenMPF components.\n\n\nClick \nhere\n for a list of acknowledgements from the master branch of the open source repository.", 
            "title": "Acknowledgements"
        }, 
        {
            "location": "/Component-API-Overview/index.html", 
            "text": "NOTICE:\n This software (or technical data) was produced for the U.S. Government under contract, and is subject to the Rights in Data-General Clause 52.227-14, Alt. IV (DEC 2007). Copyright 2019 The MITRE Corporation. All Rights Reserved.\n\n\n\n\nGoals\n\n\nThe OpenMPF Component Application Programming Interface (API) provides a mechanism for integrating components into OpenMPF. The goals of the document are to:\n\n\n\n\nProvide an overview of OpenMPF concepts\n\n\nDefine a \ncomponent\n in the context of OpenMPF\n\n\nExplain the role of the Component API\n\n\n\n\nTerminology\n\n\nIn order to talk about OpenMPF, readers should be familiar with the following key OpenMPF-specific terms:\n\n\n\n\nJob\n - An OpenMPF work unit. A job contains a list of media files and the pipeline that will be used to process that media.\n\n\nPipeline\n - A logical flow of processes that will be performed on a piece of media. For instance, a pipeline may perform motion tracking on a video and feed the results into a face detection algorithm.\n\n\nComponent\n - An OpenMPF plugin that receives jobs (containing media), processes that media, and returns results.\n\n\nDetection Component\n - A component that performs either detection (with or without tracking), or classification on a piece of media.\n\n\nNode\n - An OpenMPF host that launches components. There may be more than one node in an OpenMPF cluster, thus forming a distributed system. There is always a master node that runs the OpenMPF web application.\n\n\nService\n - An instance of an OpenMPF component process. Each OpenMPF node may run one or more services at a time. Multiple services may run in parallel to process a job. For example, each service may process a different piece of media, or a segment of the same video.\n\n\nBatch Processing\n - Process complete image, audio, video, and/or other files that reside on disk.\n\n\nStream Processing\n - Process live video streams.\n\n\n\n\n Background \n\n\n\nOpenMPF consists of the Workflow Manager (WFM), a Node Manager, components, and a message passing mechanism that enables communication between the WFM and the components.\n\n\nWorkflow Manager\n\n\nThe WFM receives job requests from user interface and external systems through the \nREST API\n. The WFM handles each request by creating a job, which consists of a collection of input media and a pipeline. These jobs are then broken down into job requests that are handled by component services, which in turn process media and return results.\n\n\nThe WFM orchestrates the flow of work within a job through the various stages of a processing pipeline. For each stage, the WFM communicates with the appropriate component services by exchanging JMS messages via a message broker. For example, if a pipeline consists of a motion detection stage, then the WFM will communicate with motion detection component services.\n\n\nThe WFM provides work to a component service by placing a job request on the request queue, and it retrieves the component\u2019s response by monitoring the appropriate response queue. The WFM may generate one or more job requests for a large video file, depending on how it segments the file into chunks. The segmentation properties can be specified system-wide using configuration files, or specified on a per-job basis.\n\n\n\n\nNOTE:\n All component messaging is abstracted within the OpenMPF Component API and component developers are not required or able to directly interact with the message queues.\n\n\n\n\nNode Manager\n\n\nThe Node Manager is a process that runs on each OpenMPF node. The Node Manager handles spawning the desired number of instances of a component based on the end-user's desired configuration. Each instance is referred to as a service.\n\n\nA service behaves differently based on the kind of processing that needs to be performed. After the Node Manager spawns a service:\n\n\n\n\nBatch processing\n - The service waits for job requests from the WFM and produces a response for each request.\n\n\nStream processing\n - The service waits for the next frame from the stream and produces activity alerts and segment summary reports.\n\n\n\n\nComponents\n\n\nComponents are identified by nine key characteristics:\n\n\n\n\nThe \ntype of action\n the component performs\n\n\nThe \ntype of processing\n the component performs\n\n\nThe \ntypes of data\n it supports\n\n\nThe \ntype of objects\n it detects\n\n\nThe \nname\n of the algorithm or vendor\n\n\nThe user-configurable \nproperties\n that the component exposes\n\n\nThe \nrequired states\n associated with a job prior to the execution of the component\n\n\nThe \nprovided states\n associated with a job following the execution of the component\n\n\nThe \nprogramming language\n used to implement the component\n\n\n\n\nA component\u2019s action type corresponds to the operation which the algorithm performs. Generally, this is \nDETECTION\n.\n\n\nA component can perform batch processing, stream processing, or both. Refer to the \nC++ Batch Component API\n, \nC++ Streaming Component API\n, and \nJava Batch Component API\n. There is also a \nPython Batch Component API\n. We currently do not support Python components that perform stream processing.\n\n\nThe data that a component accepts as inputs, and correspondingly produces as outputs, constrains its placement in a pipeline. This is some combination of \nIMAGE\n, \nAUDIO\n, and \nVIDEO\n for components that support batch processing, or just \nVIDEO\n for components that only support stream processing. Batch components can also support the \nUNKNOWN\n data type, meaning that they can accept jobs for any kind of media file.\n\n\nAs depicted in the figure below, detection components accept an input media file (or segment of the file in the case of video files) and produce a collection of object detections discovered in the data.\n\n\nThe type of objects produced depends on the input type. For example, video files produce video tracks, audio files produce audio tracks, and images produce image locations.\n\n\n\n\nThe OpenMPF Component API presented provides developers an interface for developing new components for OpenMPF without requiring the developers to understand the internals of the framework.\n\n\nThe figure below depicts a high-level block diagram of the OpenMPF architecture with components.\n\n\n\n\nThe Component Registry serves as a central location for information about the components registered with the OpenMPF instance. A future goal is to develop a web page that can be used to browse the registry and display the metadata associated with each available component.\n\n\nOpenMPF includes a Component Executable for the \nDETECTION\n action type, as denoted by the blue cubes. Note that the Component Executable is shown three times to represent three instances of that process, one for each component type. This executable is responsible for loading a component library based on information provided at launch time from the Node Manager. \n\n\nOne Component Executable instance is associated with each component service. For example, a motion detection service, face detection service, and text detection service will require three instances of the Component Executable process, one for each service. For another example, three motion detection services will also require three instances of the Component Executable process, one for each service. The Component Executable is abstract; it does not care what kind of detection is performed. It simply interacts with the component library through the Component API.\n\n\nThe Component Executable receives job requests from the message broker, translates those requests for the component, and converts the component\u2019s outputs into response messages for the OpenMPF.\n\n\nA separate Component Executable is maintained for C++ and Java components. The component library is compiled as a C++ shared object library, or Java JAR, and encapsulates the component's detection logic.", 
            "title": "Component API Overview"
        }, 
        {
            "location": "/Component-API-Overview/index.html#goals", 
            "text": "The OpenMPF Component Application Programming Interface (API) provides a mechanism for integrating components into OpenMPF. The goals of the document are to:   Provide an overview of OpenMPF concepts  Define a  component  in the context of OpenMPF  Explain the role of the Component API", 
            "title": "Goals"
        }, 
        {
            "location": "/Component-API-Overview/index.html#terminology", 
            "text": "In order to talk about OpenMPF, readers should be familiar with the following key OpenMPF-specific terms:   Job  - An OpenMPF work unit. A job contains a list of media files and the pipeline that will be used to process that media.  Pipeline  - A logical flow of processes that will be performed on a piece of media. For instance, a pipeline may perform motion tracking on a video and feed the results into a face detection algorithm.  Component  - An OpenMPF plugin that receives jobs (containing media), processes that media, and returns results.  Detection Component  - A component that performs either detection (with or without tracking), or classification on a piece of media.  Node  - An OpenMPF host that launches components. There may be more than one node in an OpenMPF cluster, thus forming a distributed system. There is always a master node that runs the OpenMPF web application.  Service  - An instance of an OpenMPF component process. Each OpenMPF node may run one or more services at a time. Multiple services may run in parallel to process a job. For example, each service may process a different piece of media, or a segment of the same video.  Batch Processing  - Process complete image, audio, video, and/or other files that reside on disk.  Stream Processing  - Process live video streams.", 
            "title": "Terminology"
        }, 
        {
            "location": "/Component-API-Overview/index.html#workflow-manager", 
            "text": "The WFM receives job requests from user interface and external systems through the  REST API . The WFM handles each request by creating a job, which consists of a collection of input media and a pipeline. These jobs are then broken down into job requests that are handled by component services, which in turn process media and return results.  The WFM orchestrates the flow of work within a job through the various stages of a processing pipeline. For each stage, the WFM communicates with the appropriate component services by exchanging JMS messages via a message broker. For example, if a pipeline consists of a motion detection stage, then the WFM will communicate with motion detection component services.  The WFM provides work to a component service by placing a job request on the request queue, and it retrieves the component\u2019s response by monitoring the appropriate response queue. The WFM may generate one or more job requests for a large video file, depending on how it segments the file into chunks. The segmentation properties can be specified system-wide using configuration files, or specified on a per-job basis.   NOTE:  All component messaging is abstracted within the OpenMPF Component API and component developers are not required or able to directly interact with the message queues.", 
            "title": "Workflow Manager"
        }, 
        {
            "location": "/Component-API-Overview/index.html#node-manager", 
            "text": "The Node Manager is a process that runs on each OpenMPF node. The Node Manager handles spawning the desired number of instances of a component based on the end-user's desired configuration. Each instance is referred to as a service.  A service behaves differently based on the kind of processing that needs to be performed. After the Node Manager spawns a service:   Batch processing  - The service waits for job requests from the WFM and produces a response for each request.  Stream processing  - The service waits for the next frame from the stream and produces activity alerts and segment summary reports.", 
            "title": "Node Manager"
        }, 
        {
            "location": "/Component-API-Overview/index.html#components", 
            "text": "Components are identified by nine key characteristics:   The  type of action  the component performs  The  type of processing  the component performs  The  types of data  it supports  The  type of objects  it detects  The  name  of the algorithm or vendor  The user-configurable  properties  that the component exposes  The  required states  associated with a job prior to the execution of the component  The  provided states  associated with a job following the execution of the component  The  programming language  used to implement the component   A component\u2019s action type corresponds to the operation which the algorithm performs. Generally, this is  DETECTION .  A component can perform batch processing, stream processing, or both. Refer to the  C++ Batch Component API ,  C++ Streaming Component API , and  Java Batch Component API . There is also a  Python Batch Component API . We currently do not support Python components that perform stream processing.  The data that a component accepts as inputs, and correspondingly produces as outputs, constrains its placement in a pipeline. This is some combination of  IMAGE ,  AUDIO , and  VIDEO  for components that support batch processing, or just  VIDEO  for components that only support stream processing. Batch components can also support the  UNKNOWN  data type, meaning that they can accept jobs for any kind of media file.  As depicted in the figure below, detection components accept an input media file (or segment of the file in the case of video files) and produce a collection of object detections discovered in the data.  The type of objects produced depends on the input type. For example, video files produce video tracks, audio files produce audio tracks, and images produce image locations.   The OpenMPF Component API presented provides developers an interface for developing new components for OpenMPF without requiring the developers to understand the internals of the framework.  The figure below depicts a high-level block diagram of the OpenMPF architecture with components.   The Component Registry serves as a central location for information about the components registered with the OpenMPF instance. A future goal is to develop a web page that can be used to browse the registry and display the metadata associated with each available component.  OpenMPF includes a Component Executable for the  DETECTION  action type, as denoted by the blue cubes. Note that the Component Executable is shown three times to represent three instances of that process, one for each component type. This executable is responsible for loading a component library based on information provided at launch time from the Node Manager.   One Component Executable instance is associated with each component service. For example, a motion detection service, face detection service, and text detection service will require three instances of the Component Executable process, one for each service. For another example, three motion detection services will also require three instances of the Component Executable process, one for each service. The Component Executable is abstract; it does not care what kind of detection is performed. It simply interacts with the component library through the Component API.  The Component Executable receives job requests from the message broker, translates those requests for the component, and converts the component\u2019s outputs into response messages for the OpenMPF.  A separate Component Executable is maintained for C++ and Java components. The component library is compiled as a C++ shared object library, or Java JAR, and encapsulates the component's detection logic.", 
            "title": "Components"
        }, 
        {
            "location": "/CPP-Batch-Component-API/index.html", 
            "text": "NOTICE:\n This software (or technical data) was produced for the U.S. Government under contract, and is subject to the Rights in Data-General Clause 52.227-14, Alt. IV (DEC 2007). Copyright 2019 The MITRE Corporation. All Rights Reserved.\n\n\n\n\nAPI Overview\n\n\nIn OpenMPF, a \ncomponent\n is a plugin that receives jobs (containing media), processes that  media, and returns results.\n\n\nThe OpenMPF Batch Component API currently supports the development of \ndetection components\n, which are used detect objects in image, video, audio, or other (generic) files that reside on disk.\n\n\nUsing this API, detection components can be built to provide:\n\n\n\n\nDetection (Localizing an object)\n\n\nTracking (Localizing an object across multiple frames)\n\n\nClassification (Detecting the type of object and optionally localizing that object)\n\n\nTranscription (Detecting speech and transcribing it into text)\n\n\n\n\nHow Components Integrate into OpenMPF\n\n\nComponents are integrated into OpenMPF through the use of OpenMPF's \nComponent Executable\n. Developers create component libraries that encapsulate the component detection logic. Each instance of the Component Executable loads one of these libraries and uses it to service job requests sent by the OpenMPF Workflow Manager (WFM).\n\n\nThe Component Executable:\n\n\n\n\nReceives and parses job requests from the WFM\n\n\nInvokes functions on the component library to obtain detection results\n\n\nPopulates and sends the respective responses to the WFM\n\n\n\n\nThe basic psuedocode for the Component Executable is as follows:\n\n\ncomponent-\nSetRunDirectory(...)\ncomponent-\nInit()\nwhile (true) {\n    job = ReceiveJob()\n    if (component-\nSupports(job.data_type))\n        component-\nGetDetections(...) // Component logic does the work here\n    SendJobResponse()\n}\ncomponent-\nClose()\n\n\n\n\nEach instance of a Component Executable runs as a separate process.\n\n\nThe Component Executable receives and parses requests from the WFM, invokes functions on the Component Logic to get detection objects, and subsequently populates responses with the component output and sends them to the WFM.\n\n\nA component developer implements a detection component by extending \nMPFDetectionComponent\n.\n\n\nAs an alternative to extending \nMPFDetectionComponent\n directly, a developer may extend one of several convenience adapter classes provided by OpenMPF. See \nConvenience Adapters\n for more information.\n\n\nGetting Started\n\n\nThe quickest way to get started with the C++ Batch Component API is to first read the \nOpenMPF Component API Overview\n and then \nreview the source\n for example OpenMPF C++ detection components.\n\n\nDetection components are implemented by:\n\n\n\n\nExtending \nMPFDetectionComponent\n.\n\n\nBuilding the component into a shared object library. (See \nHelloWorldComponent CMakeLists.txt\n).\n\n\nPackaging the component into an OpenMPF-compliant .tar.gz file. (See \nComponent Packaging\n).\n\n\nRegistering the component with OpenMPF. (See \nPackaging and Registering a Component\n).\n\n\n\n\nAPI Specification\n\n\nThe figure below presents a high-level component diagram of the C++ Batch Component API:\n\n\n\n\nThe API consists of \nComponent Interfaces\n, which provide interfaces and abstract classes for developing components; \nJob Definitions\n, which define the work to be performed by a component; \nJob Results\n, which define the results generated by the component; \nComponent Adapters\n, which provide default implementations of several of the \nMPFDetectionComponent\n interface functions; and \nComponent Utilities\n, which perform actions such as image rotation, and cropping.\n\n\nComponent Interface\n\n\n\n\nMPFComponent\n - Abstract base class for components.\n\n\n\n\nDetection Component Interface\n\n\n\n\nMPFDetectionComponent\n extends \nMPFComponent\n - Abstract class that should be extended by all OpenMPF C++ detection components that perform batch processing.\n\n\n\n\nJob Definitions\n\n\nThe following data structures contain details about a specific job (work unit):\n\n\n\n\nMPFImageJob\n extends \nMPFJob\n\n\nMPFVideoJob\n extends \nMPFJob\n\n\nMPFAudioJob\n extends \nMPFJob\n\n\nMPFGenericJob\n extends \nMPFJob\n\n\n\n\nJob Results\n\n\nThe following data structures define detection results:\n\n\n\n\nMPFImageLocation\n\n\nMPFVideoTrack\n\n\nMPFAudioTrack\n\n\nMPFGenericTrack\n\n\n\n\nComponents must also include two \nComponent Factory Functions\n.\n\n\nComponent Interface\n\n\nThe \nMPFComponent\n class is the abstract base class utilized by all OpenMPF C++ components that perform batch processing.\n\n\nSee the latest source here.\n\n\n\n\nIMPORTANT:\n This interface should not be directly implemented, because no mechanism exists for launching components based off of it. Currently, the only supported type of component is detection, and all batch detection components should instead extend \nMPFDetectionComponent\n.\n\n\n\n\nSetRunDirectory(string)\n\n\nSets the value of the private \nrun_directory\n data member which contains the full path of the parent folder above where the component is installed.\n\n\n\n\nFunction Definition:\n\n\n\n\nvoid SetRunDirectory(const string \nrun_dir)\n\n\n\n\n\n\nParameters:\n\n\n\n\n\n\n\n\n\n\nParameter\n\n\nData Type\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nrun_dir\n\n\nconst string \n\n\nFull path of the parent folder above where the component is installed.\n\n\n\n\n\n\n\n\n\n\nReturns: none\n\n\n\n\n\n\nIMPORTANT:\n \nSetRunDirectory\n is called by the Component Executable to set the correct path. This function should not be called within your implementation.\n\n\n\n\nGetRunDirectory()\n\n\nReturns the value of the private \nrun_directory\n data member which contains the full path of the parent folder above where the component is installed. This parent folder is also known as the plugin folder.\n\n\n\n\nFunction Definition:\n\n\n\n\nstring GetRunDirectory()\n\n\n\n\n\n\n\n\nParameters: none\n\n\n\n\n\n\nReturns: (\nstring\n) Full path of the parent folder above where the component is installed.\n\n\n\n\n\n\nExample:\n\n\n\n\n\n\nstring run_dir = GetRunDirectory();\nstring plugin_path = run_dir + \n/SampleComponent\n;\nstring config_path = plugin_path + \n/config\n;\nstring logconfig_file = config_path + \n/Log4cxxConfig.xml\n;\n\n\n\n\nInit()\n\n\nThe component should perform all initialization operations in the \nInit\n member function.\nThis will be executed once by the Component Executable, on component startup, before the first job, after \nSetRunDirectory\n.\n\n\n\n\nFunction Definition:\n\n\n\n\nbool Init()\n\n\n\n\n\n\n\n\nParameters: none\n\n\n\n\n\n\nReturns: (\nbool\n) Return true if initialization is successful, otherwise return false.\n\n\n\n\n\n\nExample:\n\n\n\n\n\n\nbool SampleComponent::Init() {\n  // Get component paths\n  string run_dir = GetRunDirectory();\n  string plugin_path = run_dir + \n/SampleComponent\n;\n  string config_path = plugin_path + \n/config\n;\n\n  // Setup logger, load data models, etc.\n\n  return true;\n}\n\n\n\n\nClose()\n\n\nThe component should perform all shutdown operations in the \nClose\n member function.\nThis will be executed once by the Component Executable, on component shutdown, usually after the last job.\n\n\nThis function is called before the component instance is deleted (see \nComponent Factory Functions\n).\n\n\n\n\nFunction Definition:\n\n\n\n\nbool Close()\n\n\n\n\n\n\n\n\nParameters: none\n\n\n\n\n\n\nReturns: (\nbool\n) Return true if successful, otherwise return false.\n\n\n\n\n\n\nExample:\n\n\n\n\n\n\nbool SampleComponent::Close() {\n    // Free memory, etc.\n    return true;\n}\n\n\n\n\nGetComponentType()\n\n\nThe GetComponentType() member function allows the C++ Batch Component API to determine the component \"type.\" Currently \nMPF_DETECTION_COMPONENT\n is the only supported component type. APIs for other component types may be developed in the future.\n\n\n\n\nFunction Definition:\n\n\n\n\nMPFComponentType GetComponentType()\n\n\n\n\n\n\n\n\nParameters: none\n\n\n\n\n\n\nReturns: (\nMPFComponentType\n) Currently, \nMPF_DETECTION_COMPONENT\n is the only supported return value.\n\n\n\n\n\n\nExample:\n\n\n\n\n\n\nMPFComponentType SampleComponent::GetComponentType() {\n    return MPF_DETECTION_COMPONENT;\n};\n\n\n\n\nComponent Factory Functions\n\n\nEvery detection component must include the following macros in its implementation:\n\n\nMPF_COMPONENT_CREATOR(TYPENAME);\n\n\n\n\nMPF_COMPONENT_DELETER();\n\n\n\n\nThe creator macro takes the \nTYPENAME\n of the detection component (for example, \u201cHelloWorld\u201d). This macro creates the factory function that the OpenMPF Component Executable will call in order to instantiate the detection component. The creation function is called once, to obtain an instance of the component, after the component library has been loaded into memory.\n\n\nThe deleter macro creates the factory function that the Component Executable will use to delete that instance of the detection component.\n\n\nThese macros must be used outside of a class declaration, preferably at the bottom or top of a component source (.cpp) file.\n\n\nExample:\n\n\n// Note: Do not put the TypeName/Class Name in quotes\nMPF_COMPONENT_CREATOR(HelloWorld);\nMPF_COMPONENT_DELETER();\n\n\n\n\nDetection Component Interface\n\n\nThe \nMPFDetectionComponent\n class is the abstract class utilized by all OpenMPF C++ detection components that perform batch processing. This class provides functions for developers to integrate detection logic into OpenMPF.\n\n\nSee the latest source here.\n\n\n\n\nIMPORTANT:\n Each batch detection component must implement all of the \nGetDetections()\n functions or extend from a superclass which provides implementations for them (see \nconvenience adapters\n).\n\n\nIf your component does not support a particular data type, it should simply:\n\nreturn MPF_UNSUPPORTED_DATA_TYPE;\n\n\n\n\nConvenience Adapters\n\n\nAs an alternative to extending \nMPFDetectionComponent\n directly, developers may extend one of several convenience adapter classes provided by OpenMPF.\n\n\nThese adapters provide default implementations of several functions in \nMPFDetectionComponent\n and ensure that the component's logic properly extends from the Component API. This enables developers to concentrate on implementation of the detection algorithm.\n\n\nThe following adapters are provided:\n\n\n\n\nImage Detection (\nsource\n)\n\n\nVideo Detection (\nsource\n)\n\n\nImage and Video Detection (\nsource\n)\n\n\nAudio Detection (\nsource\n)\n\n\nAudio and Video Detection (\nsource\n)\n\n\nGeneric Detection (\nsource\n)\n\n\n\n\n\n\nExample: Creating Adaptors to Perform Naive Tracking:\n\nA simple detector that operates on videos may simply go through the video frame-by-frame, extract each frame\u2019s data, and perform detections on that data as though it were processing a new unrelated image each time. As each frame is processed, one or more \nMPFImageLocations\n are generated.\n\n\nGenerally, it is preferred that a detection component that supports \nVIDEO\n data is able to perform tracking across video frames to appropriately correlate \nMPFImageLocation\n detections across frames.\n\n\nAn adapter could be developed to perform simple tracking. This would correlate \nMPFImageLocation\n detections across frames by na\u00efvely looking for bounding box regions in each contiguous frame that overlap by a given threshold such as 50%.\n\n\n\n\nSupports(MPFDetectionDataType)\n\n\nReturns true or false depending on the data type is supported or not.\n\n\n\n\nFunction Definition:\n\n\n\n\nbool Supports(MPFDetectionDataType data_type)\n\n\n\n\n\n\nParameters:\n\n\n\n\n\n\n\n\n\n\nParameter\n\n\nData Type\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\ndata_type\n\n\nMPFDetectionDataType\n\n\nReturn true if the component supports IMAGE, VIDEO, AUDIO, and/or UNKNOWN (generic) processing.\n\n\n\n\n\n\n\n\n\n\n\n\nReturns: (\nbool\n) True if the component supports the data type, otherwise false.\n\n\n\n\n\n\nExample:\n\n\n\n\n\n\n// Sample component that supports only image and video files\nbool SampleComponent::Supports(MPFDetectionDataType data_type) {\n    return data_type == MPFDetectionDataType::IMAGE || data_type == MPFDetectionDataType::VIDEO;\n}\n\n\n\n\nGetDetectionType()\n\n\nReturns the type of object detected by the component.\n\n\n\n\nFunction Definition:\n\n\n\n\nstring GetDetectionType()\n\n\n\n\n\n\n\n\nParameters: none\n\n\n\n\n\n\nReturns: (\nstring\n) The type of object detected by the component. Should be in all CAPS. Examples include: \nFACE\n, \nMOTION\n, \nPERSON\n, \nSPEECH\n, \nCLASS\n (for object classification), or \nTEXT\n.\n\n\n\n\n\n\nExample:\n\n\n\n\n\n\nstring SampleComponent::GetDetectionType() {\n    return \nFACE\n;\n}\n\n\n\n\nGetDetections(MPFImageJob \u2026)\n\n\nUsed to detect objects in an image file. The MPFImageJob structure contains \nthe data_uri specifying the location of the image file.\n\n\nCurrently, the data_uri is always a local file path. For example, \"/opt/mpf/share/remote-media/test-file.jpg\". \nThis is because all media is copied to the OpenMPF server before the job is executed.\n\n\n\n\nFunction Definition:\n\n\n\n\nstd::vector\nMPFImageLocation\n GetDetections(const MPFImageJob \njob);\n\n\n\n\n\n\nParameters:\n\n\n\n\n\n\n\n\n\n\nParameter\n\n\nData Type\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\njob\n\n\nconst MPFImageJob\n\n\nStructure containing details about the work to be performed. See \nMPFImageJob\n\n\n\n\n\n\n\n\n\n\nReturns: (\nstd::vector\nMPFImageLocation\n) The \nMPFImageLocation\n data for each detected object.\n\n\n\n\nGetDetections(MPFVideoJob \u2026)\n\n\nUsed to detect objects in a video file. Prior to being sent to the component, videos are split into logical \"segments\" \nof video data and each segment (containing a range of frames) is assigned to a different job. Components are not \nguaranteed to receive requests in any order. For example, the first request processed by a component might receive \na request for frames 300-399 of a Video A, while the next request may cover frames 900-999 of a Video B.\n\n\n\n\nFunction Definition:\n\n\n\n\nstd::vector\nMPFVideoTrack\n  GetDetections(const MPFVideoJob \njob);\n\n\n\n\n\n\nParameters:\n\n\n\n\n\n\n\n\n\n\nParameter\n\n\nData Type\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\njob\n\n\nconst MPFVideoJob\n\n\nStructure containing details about the work to be performed. See \nMPFVideoJob\n\n\n\n\n\n\n\n\n\n\nReturns: (\nstd::vector\nMPFVideoTrack\n) The \nMPFVideoTrack\n data for each detected object.  \n\n\n\n\nGetDetections(MPFAudioJob \u2026)\n\n\nUsed to detect objects in an audio file. Currently, audio files are not logically segmented, so a job will contain \nthe entirety of the audio file.\n\n\n\n\nFunction Definition:\n\n\n\n\nstd::vector\nMPFAudioTrack\n GetDetections(const MPFAudioJob \njob);\n\n\n\n\n\n\nParameters:\n\n\n\n\n\n\n\n\n\n\nParameter\n\n\nData Type\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\njob\n\n\nconst MPFAudioJob \n\n\nStructure containing details about the work to be performed. See \nMPFAudioJob\n\n\n\n\n\n\n\n\n\n\nReturns: (\nstd::vector\nMPFAudioTrack\n) The \nMPFAudioTrack\n data for each detected object.\n\n\n\n\nGetDetections(MPFGenericJob \u2026)\n\n\nUsed to detect objects in files that aren't video, image, or audio files. Such files are of the UNKNOWN type and \nhandled generically. These files are not logically segmented, so a job will contain the entirety of the file.\n\n\n\n\nFunction Definition:\n\n\n\n\nstd::vector\nMPFGenericTrack\n GetDetections(const MPFGenericJob \njob);\n\n\n\n\n\n\nParameters:\n\n\n\n\n\n\n\n\n\n\nParameter\n\n\nData Type\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\njob\n\n\nconst MPFGenericJob \n\n\nStructure containing details about the work to be performed. See \nMPFGenericJob\n\n\n\n\n\n\n\n\n\n\nReturns: (\nstd::vector\nMPFGenericTrack\n) The \nMPFGenericTrack\n data for each detected object.\n\n\n\n\nDetection Job Data Structures\n\n\nThe following data structures contain details about a specific job (work unit):\n\n\n\n\nMPFImageJob\n extends \nMPFJob\n\n\nMPFVideoJob\n extends \nMPFJob\n\n\nMPFAudioJob\n extends \nMPFJob\n\n\nMPFGenericJob\n extends \nMPFJob\n\n\n\n\nThe following data structures define detection results:\n\n\n\n\nMPFImageLocation\n\n\nMPFVideoTrack\n\n\nMPFAudioTrack\n\n\nMPFGenericTrack\n\n\n\n\nMPFJob\n\n\nStructure containing information about a job to be performed on a piece of media.\n\n\n\n\nConstructor(s):\n\n\n\n\nMPFJob(\n  const string \njob_name,\n  const string \ndata_uri,\n  const Properties \njob_properties,\n  const Properties \nmedia_properties)\n\n\n\n\n\n\nMembers:\n\n\n\n\n\n\n\n\n\n\nMember\n\n\nData Type\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\njob_name \n\n\nconst string  \n\n\nA specific name given to the job by the OpenMPF framework. This value may be used, for example, for logging and debugging purposes.\n\n\n\n\n\n\ndata_uri \n\n\nconst string  \n\n\nThe URI of the input media file to be processed. Currently, this is a file path. For example, \"/opt/mpf/share/remote-media/test-file.avi\".\n\n\n\n\n\n\njob_properties \n\n\nconst Properties \n\n\nContains a map of \nstring, string\n which represents the property name and the property value. The key corresponds to the property name specified in the component descriptor file described in \nPackaging and Registering a Component\n. Values are determined when creating a pipeline or when submitting a job. \n Note: The job_properties map may not contain the full set of job properties. For properties not contained in the map, the component must use a default value.\n\n\n\n\n\n\nmedia_properties \n\n\nconst Properties \n\n\nContains a map of \nstring, string\n of metadata about the media associated with the job. The entries in the map vary depending on the type of media. Refer to the type-specific job structures below.\n\n\n\n\n\n\n\n\nMPFImageJob\n\n\nExtends \nMPFJob\n\n\nStructure containing data used for detection of objects in an image file.\n\n\n\n\nConstructor(s):\n\n\n\n\nMPFImageJob(\n  const string \njob_name,\n  const string \ndata_uri,\n  const Properties \njob_properties,\n  const Properties \nmedia_properties)\n\n\n\n\nMPFImageJob(\n  const string \njob_name,\n  const string \ndata_uri,\n  const MPFImageLocation \nlocation,\n  const Properties \njob_properties,\n  const Properties \nmedia_properties)\n\n\n\n\n\n\nMembers:\n\n\n\n\n\n  \n\n    \n\n      \nMember\n\n      \nData Type\n\n      \nDescription\n\n    \n\n  \n\n  \n\n    \n\n      \njob_name\n\n      \nconst string \n&\n\n      \nSee \nMPFJob.job_name\n for description.\n\n    \n\n    \n\n      \ndata_uri\n\n      \nconst string \n&\n\n      \nSee \nMPFJob.data_uri\n for description.\n\n    \n\n    \n\n      \nlocation\n\n      \nconst MPFImageLocation \n&\n\n      \nAn \nMPFImageLocation\n from the previous pipeline stage. Provided when feed forward is enabled. See \nFeed Forward Guide\n.\n\n    \n\n    \n\n      \njob_properties\n\n      \nconst Properties \n&\n\n      \nSee \nMPFJob.job_properties\n for description.\n\n    \n\n    \n\n      \nmedia_properties\n\n      \nconst Properties \n&\n\n      \n\n        See \nMPFJob.media_properties\n for description.\n        \n\n        Includes the following key-value pairs:\n        \n\n          \nMIME_TYPE\n : the MIME type of the media\n\n          \nFRAME_WIDTH\n : the width of the image in pixels\n\n          \nFRAME_HEIGHT\n : the height of the image in pixels\n\n        \n\n        May include the following key-value pairs:\n        \n\n          \nROTATION\n : A floating point value in the interval \n[0.0, 360.0)\n indicating the orientation of the media in degrees in the counter-clockwise direction. In order to view the media in the upright orientation, it must be rotated the given number of degrees in the clockwise direction.\n\n          \nHORIZONTAL_FLIP\n : true if the image is mirrored across the Y-axis, otherwise false\n\n          \nEXIF_ORIENTATION\n : the standard EXIF orientation tag; a value between 1 and 8\n\n        \n\n      \n\n    \n\n  \n\n\n\n\n\nMPFVideoJob\n\n\nExtends \nMPFJob\n\n\nStructure containing data used for detection of objects in a video file.\n\n\n\n\nConstructor(s):\n\n\n\n\nMPFVideoJob(\n  const string \njob_name,\n  const string \ndata_uri,\n  int start_frame,\n  int stop_frame,\n  const Properties \njob_properties,\n  const Properties \nmedia_properties)\n\n\n\n\nMPFVideoJob(\n  const string \njob_name,\n  const string \ndata_uri,\n  int start_frame,\n  int stop_frame,\n  const MPFVideoTrack \ntrack,\n  const Properties \njob_properties,\n  const Properties \nmedia_properties)\n\n\n\n\n\n\nMembers:\n\n\n\n\n\n  \n\n    \n\n      \nMember\n\n      \nData Type\n\n      \nDescription\n\n    \n\n  \n\n  \n\n    \n\n      \njob_name\n\n      \nconst string \n&\n\n      \nSee \nMPFJob.job_name\n for description.\n\n    \n\n    \n\n      \ndata_uri\n\n      \nconst string \n&\n\n      \nSee \nMPFJob.data_uri\n for description.\n\n    \n\n    \n\n      \nstart_frame\n\n      \nconst int\n\n      \nThe first frame number (0-based index) of the video that should be processed to look for detections.\n\n    \n\n    \n\n      \nstop_frame\n\n      \nconst int\n\n      \nThe last frame number (0-based index) of the video that should be processed to look for detections.\n\n    \n    \n    \n\n      \ntrack\n\n      \nconst MPFVideoTrack \n&\n\n      \nAn \nMPFVideoTrack\n from the previous pipeline stage. Provided when feed forward is enabled. See \nFeed Forward Guide\n.\n\n    \n\n    \n\n      \njob_properties\n\n      \nconst Properties \n&\n\n      \nSee \nMPFJob.job_properties\n for description.\n\n    \n\n    \n\n      \nmedia_properties\n\n      \nconst Properties \n&\n\n      \n\n        See \nMPFJob.media_properties\n for description.\n        \n\n        Includes the following key-value pairs:\n        \n\n          \nDURATION\n : length of video in milliseconds\n\n          \nFPS\n : frames per second (averaged for variable frame rate video)\n\n          \nFRAME_COUNT\n : the number of frames in the video\n\n          \nMIME_TYPE\n : the MIME type of the media\n\n          \nFRAME_WIDTH\n : the width of a frame in pixels\n\n          \nFRAME_HEIGHT\n : the height of a frame in pixels\n\n        \n\n        May include the following key-value pair:\n        \n\n          \nROTATION\n : A floating point value in the interval \n[0.0, 360.0)\n indicating the orientation of the media in degrees in the counter-clockwise direction. In order to view the media in the upright orientation, it must be rotated the given number of degrees in the clockwise direction.\n\n        \n\n      \n\n    \n\n  \n\n\n\n\n\n\n\nIMPORTANT:\n \nFRAME_INTERVAL\n is a common job property that many components support. For frame intervals greater than 1, the component must look for detections starting with the first frame, and then skip frames as specified by the frame interval, until or before it reaches the stop frame. For example, given a start frame of 0, a stop frame of 99, and a frame interval of 2, then the detection component must look for objects in frames numbered 0, 2, 4, 6, ..., 98.\n\n\n\n\nMPFAudioJob\n\n\nExtends \nMPFJob\n\n\nStructure containing data used for detection of objects in an audio file. Currently, audio files are not logically segmented, so a job will contain the entirety of the audio file.\n\n\n\n\nConstructor(s):\n\n\n\n\nMPFAudioJob(\n  const string \njob_name,\n  const string \ndata_uri,\n  int start_time,\n  int stop_time,\n  const Properties \njob_properties,\n  const Properties \nmedia_properties)\n\n\n\n\nMPFAudioJob(\n  const string \njob_name,\n  const string \ndata_uri,\n  int start_time,\n  int stop_time,\n  const MPFAudioTrack \ntrack,    \n  const Properties \njob_properties,\n  const Properties \nmedia_properties)\n\n\n\n\n\n\nMembers:\n\n\n\n\n\n  \n\n    \n\n      \nMember\n\n      \nData Type\n\n      \nDescription\n\n    \n\n  \n\n  \n\n    \n\n      \njob_name\n\n      \nconst string \n&\n\n      \nSee \nMPFJob.job_name\n for description.\n\n    \n\n    \n\n      \ndata_uri\n\n      \nconst string \n&\n\n      \nSee \nMPFJob.data_uri\n for description.\n\n    \n\n    \n\n      \nstart_time\n\n      \nconst int\n\n      \nThe time (0-based index, in milliseconds) associated with the beginning of the segment of the audio file that should be processed to look for detections.\n\n    \n\n    \n\n      \nstop_time\n\n      \nconst int\n\n      \nThe time (0-based index, in milliseconds) associated with the end of the segment of the audio file that should be processed to look for detections.\n\n    \n\n    \n\n      \ntrack\n\n      \nconst MPFAudioTrack \n&\n\n      \nAn \nMPFAudioTrack\n from the previous pipeline stage. Provided when feed forward is enabled. See \nFeed Forward Guide\n.\n\n    \n\n    \n\n      \njob_properties\n\n      \nconst Properties \n&\n\n      \nSee \nMPFJob.job_properties\n for description.\n\n    \n\n    \n\n      \nmedia_properties\n\n      \nconst Properties \n&\n\n      \n\n        See \nMPFJob.media_properties\n for description.\n        \n\n        Includes the following key-value pairs:\n        \n\n          \nDURATION\n : length of audio file in milliseconds\n\n          \nMIME_TYPE\n : the MIME type of the media\n\n        \n\n      \n\n    \n\n  \n\n\n\n\n\nMPFGenericJob\n\n\nExtends \nMPFJob\n\n\nStructure containing data used for detection of objects in a file that isn't a video, image, or audio file. The file is of the UNKNOWN type and handled generically. The file is not logically segmented, so a job will contain the entirety of the file.\n\n\n\n\nConstructor(s):\n\n\n\n\nMPFGenericJob(\n  const string \njob_name,\n  const string \ndata_uri,\n  const Properties \njob_properties,\n  const Properties \nmedia_properties)\n\n\n\n\nMPFGenericJob(\n  const string \njob_name,\n  const string \ndata_uri,\n  const MPFGenericTrack \ntrack,\n  const Properties \njob_properties,\n  const Properties \nmedia_properties)\n}\n\n\n\n\n\n\nMembers:\n\n\n\n\n\n  \n\n    \n\n      \nMember\n\n      \nData Type\n\n      \nDescription\n\n    \n\n  \n\n  \n\n    \n\n      \njob_name\n\n      \nconst string \n&\n\n      \nSee \nMPFJob.job_name\n for description.\n\n    \n\n    \n\n      \ndata_uri\n\n      \nconst string \n&\n\n      \nSee \nMPFJob.data_uri\n for description.\n\n    \n\n    \n\n      \ntrack\n\n      \nconst MPFGenericTrack \n&\n\n      \nAn \nMPFGenericTrack\n from the previous pipeline stage. Provided when feed forward is enabled. See \nFeed Forward Guide\n.\n\n    \n\n    \n\n      \njob_properties\n\n      \nconst Properties \n&\n\n      \nSee \nMPFJob.job_properties\n for description.\n\n    \n\n    \n\n      \nmedia_properties\n\n      \nconst Properties \n&\n\n      \n\n        See \nMPFJob.media_properties\n for description.\n        \n\n        Includes the following key-value pair:\n        \n\n          \nMIME_TYPE\n : the MIME type of the media\n\n        \n\n      \n\n    \n\n  \n\n\n\n\n\nDetection Job Result Classes\n\n\nMPFImageLocation\n\n\nStructure used to store the location of detected objects in a image file.\n\n\n\n\nConstructor(s):\n\n\n\n\nMPFImageLocation()\nMPFImageLocation(\n  int x_left_upper,\n  int y_left_upper,\n  int width,\n  int height,\n  float confidence = -1,\n  const Properties \ndetection_properties = {})\n\n\n\n\n\n\nMembers:\n\n\n\n\n\n\n\n\n\n\nMember\n\n\nData Type\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nx_left_upper\n\n\nint\n\n\nUpper left X coordinate of the detected object.\n\n\n\n\n\n\ny_left_upper\n\n\nint\n\n\nUpper left Y coordinate of the detected object.\n\n\n\n\n\n\nwidth\n\n\nint\n\n\nThe width of the detected object.\n\n\n\n\n\n\nheight\n\n\nint\n\n\nThe height of the detected object.\n\n\n\n\n\n\nconfidence\n\n\nfloat\n\n\nRepresents the \"quality\" of the detection. The range depends on the detection algorithm. 0.0 is lowest quality. Higher values are higher quality. Using a standard range of [0.0 - 1.0] is advised. If the component is unable to supply a confidence value, it should return -1.0.\n\n\n\n\n\n\ndetection_properties\n\n\nProperties \n\n\nOptional additional information about the detected object. There is no restriction on the keys or the number of entries that can be added to the detection_properties map. For best practice, keys should be in all CAPS. See the \nsection\n for \nROTATION\n and \nHORIZONTAL_FLIP\n below,\n\n\n\n\n\n\n\n\n\n\nExample:\n\n\n\n\nA component that performs generic object classification can add an entry to \ndetection_properties\n where the key is \nCLASSIFICATION\n and the value is the type of object detected.\n\n\n\nMPFImageLocation { \n    x_left_upper = 0, y_left_upper = 0, width = 100, height = 50, confidence = 1.0,\n    { {\"CLASSIFICATION\", \"backpack\"} } \n}\n\n\n\n\n\n\nRotation and Horizontal Flip\n\n\nWhen the \ndetection_properties\n map contains a \nROTATION\n key, it should be a floating point value in the interval\n\n[0.0, 360.0)\n indicating the orientation of the detection in degrees in the counter-clockwise direction.\nIn order to view the detection in the upright orientation, it must be rotated the given number of degrees in the\nclockwise direction.\n\n\nThe \ndetection_properties\n map can also contain a \nHORIZONTAL_FLIP\n property that will either be \n\"true\"\n or \n\"false\"\n.\nThe \ndetection_properties\n map may have both \nHORIZONTAL_FLIP\n and \nROTATION\n keys.\n\n\nThe Workflow Manager performs the following algorithm to draw the bounding box when generating markup:\n\n\n\n\n\n    Draw the rectangle ignoring rotation and flip. \n\n\n\n\n\n   Rotate the rectangle counter-clockwise the given number of degrees around its top left corner. \n\n\n\n\n\n  If the rectangle is flipped, flip horizontally around the top left corner.\n\n\n\n\n\n\n\n\nIn the image above you can see the three steps required to properly draw a bounding box.\nStep 1 is drawn in red. Step 2 is drawn in blue. Step 3 and the final result is drawn in green.\nThe detection for the image above is:\n\n\n\nMPFImageLocation { \n    x_left_upper = 210, y_left_upper = 189, width = 177, height = 41, confidence = 1.0,\n    { {\"ROTATION\", \"15\"}, { \"HORIZONTAL_FLIP\", \"true\" } } \n}\n\n\n\n\nNote that the \nx_left_upper\n, \ny_left_upper\n, \nwidth\n, and \nheight\n values describe the red rectangle. The addition\nof the \nROTATION\n property results in the blue rectangle, and the addition of the \nHORIZONTAL_FLIP\n property results\nin the green rectangle. \n\n\nOne way to think about the process is \"draw the unrotated and unflipped rectangle, stick a pin in the upper left corner,\nand then rotate and flip around the pin\".\n\n\nRotation-Only Example\n\n\n\n\nThe Workflow Manager generated the above image by performing markup on the original image with the following\ndetection:\n\n\n\nMPFImageLocation { \n    x_left_upper = 156, y_left_upper = 339, width = 194, height = 243, confidence = 1.0,\n    { {\"ROTATION\", \"90.0\"} } \n}\n\n\n\n\nThe markup process followed steps 1 and 2 in the previous section, skipping step 3 because there is no\n\nHORIZONTAL_FLIP\n. \n\n\nIn order to properly extract the detection region from the original image, such as when generating an artifact, you\nwould need to rotate the region in the above image 90 degrees clockwise around the cyan dot currently shown in the\nbottom-left corner so that the face is in the proper upright position. \n\n\nWhen the rotation is properly corrected in this way, the cyan dot will appear in the top-left corner of the bounding\nbox. That is why its position is described using the \nx_left_upper\n, and \ny_left_upper\n variables. They refer to the\ntop-left corner of the correctly oriented region. \n\n\nMPFVideoTrack\n\n\nStructure used to store the location of detected objects in a video file.\n\n\n\n\nConstructor(s):\n\n\n\n\nMPFVideoTrack()\nMPFVideoTrack(\n  int start_frame,\n  int stop_frame,\n  float confidence = -1,\n  map\nint, MPFImageLocation\n frame_locations,\n  const Properties \ndetection_properties = {})\n\n\n\n\n\n\nMembers:\n\n\n\n\n\n\n\n\n\n\nMember\n\n\nData Type\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nstart_frame\n\n\nint\n\n\nThe first frame number (0-based index) that contained the detected object.\n\n\n\n\n\n\nstop_frame\n\n\nint\n\n\nThe last frame number (0-based index) that contained the detected object.\n\n\n\n\n\n\nframe_locations\n\n\nmap\nint, MPFImageLocation\n\n\nA map of individual detections. The key for each map entry is the frame number where the detection was generated, and the value is a \nMPFImageLocation\n calculated as if that frame was a still image. Note that a key-value pair is \nnot\n required for every frame between the track start frame and track stop frame.\n\n\n\n\n\n\nconfidence\n\n\nfloat\n\n\nRepresents the \"quality\" of the detection. The range depends on the detection algorithm. 0.0 is lowest quality. Higher values are higher quality. Using a standard range of [0.0 - 1.0] is advised. If the component is unable to supply a confidence value, it should return -1.0.\n\n\n\n\n\n\ndetection_properties\n\n\nProperties \n\n\nOptional additional information about the detected object. There is no restriction on the keys or the number of entries that can be added to the detection_properties map. For best practice, keys should be in all CAPS.\n\n\n\n\n\n\n\n\n\n\nExample:\n\n\n\n\n\n\nNOTE:\n Currently, \nMPFVideoTrack.detection_properties\n do not show up in the JSON output object or are used by the WFM in any way.\n\n\n\n\nA component that detects text can add an entry to \ndetection_properties\n where the key is \nTRANSCRIPT\n and the value is a string representing the text found in the video segment.\n\n\nMPFVideoTrack track;\ntrack.start_frame = 0;\ntrack.stop_frame = 5;\ntrack.confidence = 1.0;\ntrack.frame_locations = frame_locations;\ntrack.detection_properties[\nTRANSCRIPT\n] = \nRE5ULTS FR0M A TEXT DETECTER\n;\n\n\n\n\nMPFAudioTrack\n\n\nStructure used to store the location of detected objects in an audio file.\n\n\n\n\nConstructor(s):\n\n\n\n\nMPFAudioTrack()\nMPFAudioTrack(\n  int start_time,\n  int stop_time,\n  float confidence = -1,\n  const Properties \ndetection_properties = {})\n\n\n\n\n\n\nMembers:\n\n\n\n\n\n\n\n\n\n\nMember\n\n\nData Type\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nstart_time\n\n\nint\n\n\nThe time (0-based index, in ms) when the audio detection event started.\n\n\n\n\n\n\nstop_time\n\n\nint\n\n\nThe time (0-based index, in ms) when the audio detection event stopped.\n\n\n\n\n\n\nconfidence\n\n\nfloat\n\n\nRepresents the \"quality\" of the detection. The range depends on the detection algorithm. 0.0 is lowest quality. Higher values are higher quality. Using a standard range of [0.0 - 1.0] is advised. If the component is unable to supply a confidence value, it should return -1.0.\n\n\n\n\n\n\ndetection_properties\n\n\nProperties \n\n\nOptional additional information about the detection. There is no restriction on the keys or the number of entries that can be added to the detection_properties map. For best practice, keys should be in all CAPS.\n\n\n\n\n\n\n\n\n\n\nNOTE:\n Currently, \nMPFAudioTrack.detection_properties\n do not show up in the JSON output object or are used by the WFM in any way.\n\n\n\n\nMPFGenericTrack\n\n\nStructure used to store the location of detected objects in a file that is not a video, image, or audio file. The file is of the UNKNOWN type and handled generically.\n\n\n\n\nConstructor(s):\n\n\n\n\nMPFGenericTrack()\nMPFGenericTrack(\n  float confidence = -1,\n  const Properties \ndetection_properties = {})\n\n\n\n\n\n\nMembers:\n\n\n\n\n\n\n\n\n\n\nMember\n\n\nData Type\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nconfidence\n\n\nfloat\n\n\nRepresents the \"quality\" of the detection. The range depends on the detection algorithm. 0.0 is lowest quality. Higher values are higher quality. Using a standard range of [0.0 - 1.0] is advised. If the component is unable to supply a confidence value, it should return -1.0.\n\n\n\n\n\n\ndetection_properties\n\n\nProperties \n\n\nOptional additional information about the detection. There is no restriction on the keys or the number of entries that can be added to the detection_properties map. For best practice, keys should be in all CAPS.\n\n\n\n\n\n\n\n\nException Types\n\n\nMPFDetectionException\n\n\nException that should be thrown by the \nGetDetections()\n methods when an error occurs. \nThe content of the \nerror_code\n and \nwhat()\n members will appear in the JSON output object.\n\n\n\n\nConstructors:\n\n\n\n\nMPFDetectionException(MPFDetectionError error_code, const std::string \nwhat = \n)\nMPFDetectionException(const std::string \nwhat)\n\n\n\n\n\n\n\n\n\n\nMember\n\n\nData Type\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nerror_code\n\n\nMPFDetectionError\n\n\nSpecifies the error type. See \nMPFDetectionError\n.\n\n\n\n\n\n\nwhat()\n\n\nconst char*\n\n\nTextual description of the specific error. (Inherited from \nstd::exception\n)\n\n\n\n\n\n\n\n\nEnumeration Types\n\n\nMPFDetectionError\n\n\nEnum used to indicate the type of error that occurred in a \nGetDetections()\n method. It is used as a parameter to \nthe \nMPFDetectionException\n constructor. A component is not required to support all error types.\n\n\n\n\n\n\n\n\nENUM\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nMPF_OTHER_DETECTION_ERROR_TYPE\n\n\nThe component function has failed for a reason that is not captured by any of the other error codes.\n\n\n\n\n\n\nMPF_DETECTION_NOT_INITIALIZED\n\n\nThe initialization of the component, or the initialization of any of its dependencies, has failed for any reason.\n\n\n\n\n\n\nMPF_UNRECOGNIZED_DATA_TYPE\n\n\nThe media data type received by a component is not one of the values contained in the MPFDetectionDataType enum.  Note that this failure is normally caught by the Component Executable, before a job is passed to the component logic.\n\n\n\n\n\n\nMPF_UNSUPPORTED_DATA_TYPE\n\n\nThe job passed to a component requests processing of a job of an unsupported type. For instance, a component that is only capable of processing audio files should return this error code if a video or image job request is received.\n\n\n\n\n\n\nMPF_INVALID_DATAFILE_URI\n\n\nThe string containing the URI location of the input data file is invalid or empty.\n\n\n\n\n\n\nMPF_COULD_NOT_OPEN_DATAFILE\n\n\nThe data file to be processed could not be opened for any reason, such as a permissions failure, or an unreachable URI.\n\n\n\n\n\n\nMPF_COULD_NOT_READ_DATAFILE\n\n\nThere is a failure reading data from a successfully opened input data file.\n\n\n\n\n\n\nMPF_FILE_WRITE_ERROR\n\n\nThe component received a failure for any reason when attempting to write to a file.\n\n\n\n\n\n\nMPF_IMAGE_READ_ERROR\n\n\nThe component failed to read the image provided by the URI. For example, it might indicate the failure of a call to \nMPFImageReader::GetImage()\n, or \ncv::imread()\n.\n\n\n\n\n\n\nMPF_BAD_FRAME_SIZE\n\n\nThe frame data retrieved has an incorrect or invalid frame size. For example, if a call to \ncv::imread()\n returns a frame of data with either the number of rows or columns less than or equal to 0.\n\n\n\n\n\n\nMPF_BOUNDING_BOX_SIZE_ERROR\n\n\nThe calculation of a detection location bounding box has failed. For example, a component may be using an external library to detect objects, but the bounding box returned by that library lies partially outside the frame boundaries.\n\n\n\n\n\n\nMPF_INVALID_FRAME_INTERVAL\n\n\nAn invalid or unsupported frame interval was received.\n\n\n\n\n\n\nMPF_INVALID_START_FRAME\n\n\nThe component received an invalid start frame number. For example, if the start frame is less than zero, or greater than the stop frame, this error code should be used.\n\n\n\n\n\n\nMPF_INVALID_STOP_FRAME\n\n\nThe component receives an invalid stop frame number. For example, if the stop frame is less than the start frame, or greater than the number of the last frame in a video segment, this error code should be used.\n\n\n\n\n\n\nMPF_DETECTION_FAILED\n\n\nGeneral failure of a detection algorithm.  This does not indicate a lack of detections found in the media, but rather a break down in the algorithm that makes it impossible to continue to try to detect objects.\n\n\n\n\n\n\nMPF_DETECTION_TRACKING_FAILED\n\n\nGeneral failure of a tracking algorithm.  This does not indicate a lack of tracks generated for the media, but rather a break down in the algorithm that makes it impossible to continue to try to track objects.\n\n\n\n\n\n\nMPF_INVALID_PROPERTY\n\n\nThe component received a property that is unrecognized or has an invalid/out-of-bounds value.\n\n\n\n\n\n\nMPF_MISSING_PROPERTY\n\n\nThe component received a job that is missing a required property.\n\n\n\n\n\n\nMPF_JOB_PROPERTY_IS_NOT_INT\n\n\nA job property is supposed to be an integer type, but it is of some other type, such as a boolean or a floating point value.\n\n\n\n\n\n\nMPF_JOB_PROPERTY_IS_NOT_FLOAT\n\n\nA job property is supposed to be a floating point type, but it is of some other type, such as a boolean value.\n\n\n\n\n\n\nMPF_MEMORY_ALLOCATION_FAILED\n\n\nThe component failed to allocate memory for any reason.\n\n\n\n\n\n\nMPF_GPU_ERROR\n\n\nThe job was configured to execute on a GPU, but there was an issue with the GPU or no GPU was detected.\n\n\n\n\n\n\n\n\nUtility Classes\n\n\nFor convenience, the OpenMPF provides the \nMPFImageReader\n (\nsource\n) and \nMPFVideoCapture\n (\nsource\n) utility classes to perform horizontal flipping, rotation, and cropping to a region of interest. Note, that when using these classes, the component will also need to utilize the class to perform a reverse transform to convert the transformed pixel coordinates back to the original (e.g. pre-flipped, pre-rotated, and pre-cropped) coordinate space.\n\n\nC++ Component Build Environment\n\n\nA C++ component library must be built for the same C++ compiler and Linux version that is used by the OpenMPF Component Executable. This is to ensure compatibility between the executable and the library functions at the Application Binary Interface (ABI) level. At this writing, the OpenMPF runs on CentOS 7.4.1708 (kernel version 3.10.0-693), and the OpenMPF C++ Component Executable is built with g++ (GCC) 4.8.5 20150623 (Red Hat 4.8.5-16).\n\n\nComponents should be supplied as a tar file, which includes not only the component library, but any other libraries or files needed for execution. This includes all other non-standard libraries used by the component (aside from the standard Linux and C++ libraries), and any configuration or data files.\n\n\nComponent Development Best Practices\n\n\nSingle-threaded Operation\n\n\nImplementations are encouraged to operate in single-threaded mode. OpenMPF will parallelize components through multiple instantiations of the component, each running as a separate service.\n\n\nStateless Behavior\n\n\nOpenMPF components should be stateless in operation and give identical output for a provided input (i.e. when processing the same \nMPFJob\n).\n\n\nGPU Support\n\n\nFor components that want to take advantage of NVIDA GPU processors, please read the \nGPU Support Guide\n. Also ensure that your build environment has the NVIDIA CUDA Toolkit installed, as described in the \nBuild Environment Setup Guide\n.\n\n\nComponent Packaging\n\n\nIt is recommended that C++ components are organized according to the following directory structure:\n\n\ncomponentName\n\u251c\u2500\u2500 config - Logging and other component-specific configuration\n\u251c\u2500\u2500 descriptor\n\u2502   \u2514\u2500\u2500 descriptor.json\n\u2514\u2500\u2500 lib\n    \u2514\u2500\u2500libComponentName.so - Compiled component library\n\n\n\n\nOnce built, components should be packaged into a .tar.gz containing the contents of the directory shown above.\n\n\nLogging\n\n\nIt is recommended to use \nApache log4cxx\n for OpenMPF Component logging.\n\n\nNote that multiple instances of the same component can log to the same file. Also, logging content can span multiple lines.\n\n\nLog files should be output to:\n\n${MPF_LOG_PATH}/${THIS_MPF_NODE}/log/\ncomponentName\n.log\n\n\nEach log statement must take the form:\n\nDATE TIME LEVEL CONTENT\n\n\nThe following log LEVELs are supported:\n\nFATAL, ERROR, WARN,  INFO,  DEBUG, TRACE\n.\n\n\nFor example:\n\n2016-02-09 13:42:42,341 INFO - Starting sample-component: [  OK  ]\n\n\nThe following configuration can be used to match the format of other OpenMPF logs:\n\n\nlog4j:configuration xmlns:log4j=\nhttp://jakarta.apache.org/log4j/\n\n\n  \n!-- Output the log message to log file--\n\n  \nappender name=\nSAMPLECOMPONENT-FILE\n class=\norg.apache.log4j.DailyRollingFileAppender\n\n    \nparam name=\nfile\n value=\n${MPF_LOG_PATH}/${THIS_MPF_NODE}/log/\ncomponentName\n.log\n /\n\n    \nparam name=\nDatePattern\n value=\n'.'yyyy-MM-dd\n /\n\n    \nlayout class=\norg.apache.log4j.PatternLayout\n\n      \nparam name=\nConversionPattern\n value=\n%d %p [%t] %c{36}:%L - %m%n\n /\n\n    \n/layout\n\n  \n/appender\n\n\n  \nlogger name= \nSampleComponent\n additivity=\nfalse\n\n    \nlevel value=\nINFO\n/\n\n    \nappender-ref ref=\nSAMPLECOMPONENT-FILE\n/\n\n  \n/logger\n\n\n\n/log4j:configuration", 
            "title": "C++ Batch Component API"
        }, 
        {
            "location": "/CPP-Batch-Component-API/index.html#api-overview", 
            "text": "In OpenMPF, a  component  is a plugin that receives jobs (containing media), processes that  media, and returns results.  The OpenMPF Batch Component API currently supports the development of  detection components , which are used detect objects in image, video, audio, or other (generic) files that reside on disk.  Using this API, detection components can be built to provide:   Detection (Localizing an object)  Tracking (Localizing an object across multiple frames)  Classification (Detecting the type of object and optionally localizing that object)  Transcription (Detecting speech and transcribing it into text)", 
            "title": "API Overview"
        }, 
        {
            "location": "/CPP-Batch-Component-API/index.html#how-components-integrate-into-openmpf", 
            "text": "Components are integrated into OpenMPF through the use of OpenMPF's  Component Executable . Developers create component libraries that encapsulate the component detection logic. Each instance of the Component Executable loads one of these libraries and uses it to service job requests sent by the OpenMPF Workflow Manager (WFM).  The Component Executable:   Receives and parses job requests from the WFM  Invokes functions on the component library to obtain detection results  Populates and sends the respective responses to the WFM   The basic psuedocode for the Component Executable is as follows:  component- SetRunDirectory(...)\ncomponent- Init()\nwhile (true) {\n    job = ReceiveJob()\n    if (component- Supports(job.data_type))\n        component- GetDetections(...) // Component logic does the work here\n    SendJobResponse()\n}\ncomponent- Close()  Each instance of a Component Executable runs as a separate process.  The Component Executable receives and parses requests from the WFM, invokes functions on the Component Logic to get detection objects, and subsequently populates responses with the component output and sends them to the WFM.  A component developer implements a detection component by extending  MPFDetectionComponent .  As an alternative to extending  MPFDetectionComponent  directly, a developer may extend one of several convenience adapter classes provided by OpenMPF. See  Convenience Adapters  for more information.", 
            "title": "How Components Integrate into OpenMPF"
        }, 
        {
            "location": "/CPP-Batch-Component-API/index.html#getting-started", 
            "text": "The quickest way to get started with the C++ Batch Component API is to first read the  OpenMPF Component API Overview  and then  review the source  for example OpenMPF C++ detection components.  Detection components are implemented by:   Extending  MPFDetectionComponent .  Building the component into a shared object library. (See  HelloWorldComponent CMakeLists.txt ).  Packaging the component into an OpenMPF-compliant .tar.gz file. (See  Component Packaging ).  Registering the component with OpenMPF. (See  Packaging and Registering a Component ).", 
            "title": "Getting Started"
        }, 
        {
            "location": "/CPP-Batch-Component-API/index.html#api-specification", 
            "text": "The figure below presents a high-level component diagram of the C++ Batch Component API:   The API consists of  Component Interfaces , which provide interfaces and abstract classes for developing components;  Job Definitions , which define the work to be performed by a component;  Job Results , which define the results generated by the component;  Component Adapters , which provide default implementations of several of the  MPFDetectionComponent  interface functions; and  Component Utilities , which perform actions such as image rotation, and cropping.  Component Interface   MPFComponent  - Abstract base class for components.   Detection Component Interface   MPFDetectionComponent  extends  MPFComponent  - Abstract class that should be extended by all OpenMPF C++ detection components that perform batch processing.   Job Definitions  The following data structures contain details about a specific job (work unit):   MPFImageJob  extends  MPFJob  MPFVideoJob  extends  MPFJob  MPFAudioJob  extends  MPFJob  MPFGenericJob  extends  MPFJob   Job Results  The following data structures define detection results:   MPFImageLocation  MPFVideoTrack  MPFAudioTrack  MPFGenericTrack   Components must also include two  Component Factory Functions .", 
            "title": "API Specification"
        }, 
        {
            "location": "/CPP-Batch-Component-API/index.html#component-interface", 
            "text": "The  MPFComponent  class is the abstract base class utilized by all OpenMPF C++ components that perform batch processing.  See the latest source here.   IMPORTANT:  This interface should not be directly implemented, because no mechanism exists for launching components based off of it. Currently, the only supported type of component is detection, and all batch detection components should instead extend  MPFDetectionComponent .", 
            "title": "Component Interface"
        }, 
        {
            "location": "/CPP-Batch-Component-API/index.html#setrundirectorystring", 
            "text": "Sets the value of the private  run_directory  data member which contains the full path of the parent folder above where the component is installed.   Function Definition:   void SetRunDirectory(const string  run_dir)   Parameters:      Parameter  Data Type  Description      run_dir  const string   Full path of the parent folder above where the component is installed.      Returns: none    IMPORTANT:   SetRunDirectory  is called by the Component Executable to set the correct path. This function should not be called within your implementation.", 
            "title": "SetRunDirectory(string)"
        }, 
        {
            "location": "/CPP-Batch-Component-API/index.html#getrundirectory", 
            "text": "Returns the value of the private  run_directory  data member which contains the full path of the parent folder above where the component is installed. This parent folder is also known as the plugin folder.   Function Definition:   string GetRunDirectory()    Parameters: none    Returns: ( string ) Full path of the parent folder above where the component is installed.    Example:    string run_dir = GetRunDirectory();\nstring plugin_path = run_dir +  /SampleComponent ;\nstring config_path = plugin_path +  /config ;\nstring logconfig_file = config_path +  /Log4cxxConfig.xml ;", 
            "title": "GetRunDirectory()"
        }, 
        {
            "location": "/CPP-Batch-Component-API/index.html#init", 
            "text": "The component should perform all initialization operations in the  Init  member function.\nThis will be executed once by the Component Executable, on component startup, before the first job, after  SetRunDirectory .   Function Definition:   bool Init()    Parameters: none    Returns: ( bool ) Return true if initialization is successful, otherwise return false.    Example:    bool SampleComponent::Init() {\n  // Get component paths\n  string run_dir = GetRunDirectory();\n  string plugin_path = run_dir +  /SampleComponent ;\n  string config_path = plugin_path +  /config ;\n\n  // Setup logger, load data models, etc.\n\n  return true;\n}", 
            "title": "Init()"
        }, 
        {
            "location": "/CPP-Batch-Component-API/index.html#close", 
            "text": "The component should perform all shutdown operations in the  Close  member function.\nThis will be executed once by the Component Executable, on component shutdown, usually after the last job.  This function is called before the component instance is deleted (see  Component Factory Functions ).   Function Definition:   bool Close()    Parameters: none    Returns: ( bool ) Return true if successful, otherwise return false.    Example:    bool SampleComponent::Close() {\n    // Free memory, etc.\n    return true;\n}", 
            "title": "Close()"
        }, 
        {
            "location": "/CPP-Batch-Component-API/index.html#getcomponenttype", 
            "text": "The GetComponentType() member function allows the C++ Batch Component API to determine the component \"type.\" Currently  MPF_DETECTION_COMPONENT  is the only supported component type. APIs for other component types may be developed in the future.   Function Definition:   MPFComponentType GetComponentType()    Parameters: none    Returns: ( MPFComponentType ) Currently,  MPF_DETECTION_COMPONENT  is the only supported return value.    Example:    MPFComponentType SampleComponent::GetComponentType() {\n    return MPF_DETECTION_COMPONENT;\n};", 
            "title": "GetComponentType()"
        }, 
        {
            "location": "/CPP-Batch-Component-API/index.html#component-factory-functions", 
            "text": "Every detection component must include the following macros in its implementation:  MPF_COMPONENT_CREATOR(TYPENAME);  MPF_COMPONENT_DELETER();  The creator macro takes the  TYPENAME  of the detection component (for example, \u201cHelloWorld\u201d). This macro creates the factory function that the OpenMPF Component Executable will call in order to instantiate the detection component. The creation function is called once, to obtain an instance of the component, after the component library has been loaded into memory.  The deleter macro creates the factory function that the Component Executable will use to delete that instance of the detection component.  These macros must be used outside of a class declaration, preferably at the bottom or top of a component source (.cpp) file.  Example:  // Note: Do not put the TypeName/Class Name in quotes\nMPF_COMPONENT_CREATOR(HelloWorld);\nMPF_COMPONENT_DELETER();", 
            "title": "Component Factory Functions"
        }, 
        {
            "location": "/CPP-Batch-Component-API/index.html#detection-component-interface", 
            "text": "The  MPFDetectionComponent  class is the abstract class utilized by all OpenMPF C++ detection components that perform batch processing. This class provides functions for developers to integrate detection logic into OpenMPF.  See the latest source here.   IMPORTANT:  Each batch detection component must implement all of the  GetDetections()  functions or extend from a superclass which provides implementations for them (see  convenience adapters ).  If your component does not support a particular data type, it should simply: return MPF_UNSUPPORTED_DATA_TYPE;", 
            "title": "Detection Component Interface"
        }, 
        {
            "location": "/CPP-Batch-Component-API/index.html#convenience-adapters", 
            "text": "As an alternative to extending  MPFDetectionComponent  directly, developers may extend one of several convenience adapter classes provided by OpenMPF.  These adapters provide default implementations of several functions in  MPFDetectionComponent  and ensure that the component's logic properly extends from the Component API. This enables developers to concentrate on implementation of the detection algorithm.  The following adapters are provided:   Image Detection ( source )  Video Detection ( source )  Image and Video Detection ( source )  Audio Detection ( source )  Audio and Video Detection ( source )  Generic Detection ( source )    Example: Creating Adaptors to Perform Naive Tracking: \nA simple detector that operates on videos may simply go through the video frame-by-frame, extract each frame\u2019s data, and perform detections on that data as though it were processing a new unrelated image each time. As each frame is processed, one or more  MPFImageLocations  are generated.  Generally, it is preferred that a detection component that supports  VIDEO  data is able to perform tracking across video frames to appropriately correlate  MPFImageLocation  detections across frames.  An adapter could be developed to perform simple tracking. This would correlate  MPFImageLocation  detections across frames by na\u00efvely looking for bounding box regions in each contiguous frame that overlap by a given threshold such as 50%.", 
            "title": "Convenience Adapters"
        }, 
        {
            "location": "/CPP-Batch-Component-API/index.html#supportsmpfdetectiondatatype", 
            "text": "Returns true or false depending on the data type is supported or not.   Function Definition:   bool Supports(MPFDetectionDataType data_type)   Parameters:      Parameter  Data Type  Description      data_type  MPFDetectionDataType  Return true if the component supports IMAGE, VIDEO, AUDIO, and/or UNKNOWN (generic) processing.       Returns: ( bool ) True if the component supports the data type, otherwise false.    Example:    // Sample component that supports only image and video files\nbool SampleComponent::Supports(MPFDetectionDataType data_type) {\n    return data_type == MPFDetectionDataType::IMAGE || data_type == MPFDetectionDataType::VIDEO;\n}", 
            "title": "Supports(MPFDetectionDataType)"
        }, 
        {
            "location": "/CPP-Batch-Component-API/index.html#getdetectiontype", 
            "text": "Returns the type of object detected by the component.   Function Definition:   string GetDetectionType()    Parameters: none    Returns: ( string ) The type of object detected by the component. Should be in all CAPS. Examples include:  FACE ,  MOTION ,  PERSON ,  SPEECH ,  CLASS  (for object classification), or  TEXT .    Example:    string SampleComponent::GetDetectionType() {\n    return  FACE ;\n}", 
            "title": "GetDetectionType()"
        }, 
        {
            "location": "/CPP-Batch-Component-API/index.html#getdetectionsmpfimagejob", 
            "text": "Used to detect objects in an image file. The MPFImageJob structure contains \nthe data_uri specifying the location of the image file.  Currently, the data_uri is always a local file path. For example, \"/opt/mpf/share/remote-media/test-file.jpg\". \nThis is because all media is copied to the OpenMPF server before the job is executed.   Function Definition:   std::vector MPFImageLocation  GetDetections(const MPFImageJob  job);   Parameters:      Parameter  Data Type  Description      job  const MPFImageJob  Structure containing details about the work to be performed. See  MPFImageJob      Returns: ( std::vector MPFImageLocation ) The  MPFImageLocation  data for each detected object.", 
            "title": "GetDetections(MPFImageJob \u2026)"
        }, 
        {
            "location": "/CPP-Batch-Component-API/index.html#getdetectionsmpfvideojob", 
            "text": "Used to detect objects in a video file. Prior to being sent to the component, videos are split into logical \"segments\" \nof video data and each segment (containing a range of frames) is assigned to a different job. Components are not \nguaranteed to receive requests in any order. For example, the first request processed by a component might receive \na request for frames 300-399 of a Video A, while the next request may cover frames 900-999 of a Video B.   Function Definition:   std::vector MPFVideoTrack   GetDetections(const MPFVideoJob  job);   Parameters:      Parameter  Data Type  Description      job  const MPFVideoJob  Structure containing details about the work to be performed. See  MPFVideoJob      Returns: ( std::vector MPFVideoTrack ) The  MPFVideoTrack  data for each detected object.", 
            "title": "GetDetections(MPFVideoJob \u2026)"
        }, 
        {
            "location": "/CPP-Batch-Component-API/index.html#getdetectionsmpfaudiojob", 
            "text": "Used to detect objects in an audio file. Currently, audio files are not logically segmented, so a job will contain \nthe entirety of the audio file.   Function Definition:   std::vector MPFAudioTrack  GetDetections(const MPFAudioJob  job);   Parameters:      Parameter  Data Type  Description      job  const MPFAudioJob   Structure containing details about the work to be performed. See  MPFAudioJob      Returns: ( std::vector MPFAudioTrack ) The  MPFAudioTrack  data for each detected object.", 
            "title": "GetDetections(MPFAudioJob \u2026)"
        }, 
        {
            "location": "/CPP-Batch-Component-API/index.html#getdetectionsmpfgenericjob", 
            "text": "Used to detect objects in files that aren't video, image, or audio files. Such files are of the UNKNOWN type and \nhandled generically. These files are not logically segmented, so a job will contain the entirety of the file.   Function Definition:   std::vector MPFGenericTrack  GetDetections(const MPFGenericJob  job);   Parameters:      Parameter  Data Type  Description      job  const MPFGenericJob   Structure containing details about the work to be performed. See  MPFGenericJob      Returns: ( std::vector MPFGenericTrack ) The  MPFGenericTrack  data for each detected object.", 
            "title": "GetDetections(MPFGenericJob \u2026)"
        }, 
        {
            "location": "/CPP-Batch-Component-API/index.html#detection-job-data-structures", 
            "text": "The following data structures contain details about a specific job (work unit):   MPFImageJob  extends  MPFJob  MPFVideoJob  extends  MPFJob  MPFAudioJob  extends  MPFJob  MPFGenericJob  extends  MPFJob   The following data structures define detection results:   MPFImageLocation  MPFVideoTrack  MPFAudioTrack  MPFGenericTrack", 
            "title": "Detection Job Data Structures"
        }, 
        {
            "location": "/CPP-Batch-Component-API/index.html#mpfjob", 
            "text": "Structure containing information about a job to be performed on a piece of media.   Constructor(s):   MPFJob(\n  const string  job_name,\n  const string  data_uri,\n  const Properties  job_properties,\n  const Properties  media_properties)   Members:      Member  Data Type  Description      job_name   const string    A specific name given to the job by the OpenMPF framework. This value may be used, for example, for logging and debugging purposes.    data_uri   const string    The URI of the input media file to be processed. Currently, this is a file path. For example, \"/opt/mpf/share/remote-media/test-file.avi\".    job_properties   const Properties   Contains a map of  string, string  which represents the property name and the property value. The key corresponds to the property name specified in the component descriptor file described in  Packaging and Registering a Component . Values are determined when creating a pipeline or when submitting a job.   Note: The job_properties map may not contain the full set of job properties. For properties not contained in the map, the component must use a default value.    media_properties   const Properties   Contains a map of  string, string  of metadata about the media associated with the job. The entries in the map vary depending on the type of media. Refer to the type-specific job structures below.", 
            "title": "MPFJob"
        }, 
        {
            "location": "/CPP-Batch-Component-API/index.html#mpfimagejob", 
            "text": "Extends  MPFJob  Structure containing data used for detection of objects in an image file.   Constructor(s):   MPFImageJob(\n  const string  job_name,\n  const string  data_uri,\n  const Properties  job_properties,\n  const Properties  media_properties)  MPFImageJob(\n  const string  job_name,\n  const string  data_uri,\n  const MPFImageLocation  location,\n  const Properties  job_properties,\n  const Properties  media_properties)   Members:   \n   \n     \n       Member \n       Data Type \n       Description \n     \n   \n   \n     \n       job_name \n       const string  & \n       See  MPFJob.job_name  for description. \n     \n     \n       data_uri \n       const string  & \n       See  MPFJob.data_uri  for description. \n     \n     \n       location \n       const MPFImageLocation  & \n       An  MPFImageLocation  from the previous pipeline stage. Provided when feed forward is enabled. See  Feed Forward Guide . \n     \n     \n       job_properties \n       const Properties  & \n       See  MPFJob.job_properties  for description. \n     \n     \n       media_properties \n       const Properties  & \n       \n        See  MPFJob.media_properties  for description.\n         \n        Includes the following key-value pairs:\n         \n           MIME_TYPE  : the MIME type of the media \n           FRAME_WIDTH  : the width of the image in pixels \n           FRAME_HEIGHT  : the height of the image in pixels \n         \n        May include the following key-value pairs:\n         \n           ROTATION  : A floating point value in the interval  [0.0, 360.0)  indicating the orientation of the media in degrees in the counter-clockwise direction. In order to view the media in the upright orientation, it must be rotated the given number of degrees in the clockwise direction. \n           HORIZONTAL_FLIP  : true if the image is mirrored across the Y-axis, otherwise false \n           EXIF_ORIENTATION  : the standard EXIF orientation tag; a value between 1 and 8", 
            "title": "MPFImageJob"
        }, 
        {
            "location": "/CPP-Batch-Component-API/index.html#mpfvideojob", 
            "text": "Extends  MPFJob  Structure containing data used for detection of objects in a video file.   Constructor(s):   MPFVideoJob(\n  const string  job_name,\n  const string  data_uri,\n  int start_frame,\n  int stop_frame,\n  const Properties  job_properties,\n  const Properties  media_properties)  MPFVideoJob(\n  const string  job_name,\n  const string  data_uri,\n  int start_frame,\n  int stop_frame,\n  const MPFVideoTrack  track,\n  const Properties  job_properties,\n  const Properties  media_properties)   Members:   \n   \n     \n       Member \n       Data Type \n       Description \n     \n   \n   \n     \n       job_name \n       const string  & \n       See  MPFJob.job_name  for description. \n     \n     \n       data_uri \n       const string  & \n       See  MPFJob.data_uri  for description. \n     \n     \n       start_frame \n       const int \n       The first frame number (0-based index) of the video that should be processed to look for detections. \n     \n     \n       stop_frame \n       const int \n       The last frame number (0-based index) of the video that should be processed to look for detections. \n         \n     \n       track \n       const MPFVideoTrack  & \n       An  MPFVideoTrack  from the previous pipeline stage. Provided when feed forward is enabled. See  Feed Forward Guide . \n     \n     \n       job_properties \n       const Properties  & \n       See  MPFJob.job_properties  for description. \n     \n     \n       media_properties \n       const Properties  & \n       \n        See  MPFJob.media_properties  for description.\n         \n        Includes the following key-value pairs:\n         \n           DURATION  : length of video in milliseconds \n           FPS  : frames per second (averaged for variable frame rate video) \n           FRAME_COUNT  : the number of frames in the video \n           MIME_TYPE  : the MIME type of the media \n           FRAME_WIDTH  : the width of a frame in pixels \n           FRAME_HEIGHT  : the height of a frame in pixels \n         \n        May include the following key-value pair:\n         \n           ROTATION  : A floating point value in the interval  [0.0, 360.0)  indicating the orientation of the media in degrees in the counter-clockwise direction. In order to view the media in the upright orientation, it must be rotated the given number of degrees in the clockwise direction. \n         \n       \n     \n      IMPORTANT:   FRAME_INTERVAL  is a common job property that many components support. For frame intervals greater than 1, the component must look for detections starting with the first frame, and then skip frames as specified by the frame interval, until or before it reaches the stop frame. For example, given a start frame of 0, a stop frame of 99, and a frame interval of 2, then the detection component must look for objects in frames numbered 0, 2, 4, 6, ..., 98.", 
            "title": "MPFVideoJob"
        }, 
        {
            "location": "/CPP-Batch-Component-API/index.html#mpfaudiojob", 
            "text": "Extends  MPFJob  Structure containing data used for detection of objects in an audio file. Currently, audio files are not logically segmented, so a job will contain the entirety of the audio file.   Constructor(s):   MPFAudioJob(\n  const string  job_name,\n  const string  data_uri,\n  int start_time,\n  int stop_time,\n  const Properties  job_properties,\n  const Properties  media_properties)  MPFAudioJob(\n  const string  job_name,\n  const string  data_uri,\n  int start_time,\n  int stop_time,\n  const MPFAudioTrack  track,    \n  const Properties  job_properties,\n  const Properties  media_properties)   Members:   \n   \n     \n       Member \n       Data Type \n       Description \n     \n   \n   \n     \n       job_name \n       const string  & \n       See  MPFJob.job_name  for description. \n     \n     \n       data_uri \n       const string  & \n       See  MPFJob.data_uri  for description. \n     \n     \n       start_time \n       const int \n       The time (0-based index, in milliseconds) associated with the beginning of the segment of the audio file that should be processed to look for detections. \n     \n     \n       stop_time \n       const int \n       The time (0-based index, in milliseconds) associated with the end of the segment of the audio file that should be processed to look for detections. \n     \n     \n       track \n       const MPFAudioTrack  & \n       An  MPFAudioTrack  from the previous pipeline stage. Provided when feed forward is enabled. See  Feed Forward Guide . \n     \n     \n       job_properties \n       const Properties  & \n       See  MPFJob.job_properties  for description. \n     \n     \n       media_properties \n       const Properties  & \n       \n        See  MPFJob.media_properties  for description.\n         \n        Includes the following key-value pairs:\n         \n           DURATION  : length of audio file in milliseconds \n           MIME_TYPE  : the MIME type of the media", 
            "title": "MPFAudioJob"
        }, 
        {
            "location": "/CPP-Batch-Component-API/index.html#mpfgenericjob", 
            "text": "Extends  MPFJob  Structure containing data used for detection of objects in a file that isn't a video, image, or audio file. The file is of the UNKNOWN type and handled generically. The file is not logically segmented, so a job will contain the entirety of the file.   Constructor(s):   MPFGenericJob(\n  const string  job_name,\n  const string  data_uri,\n  const Properties  job_properties,\n  const Properties  media_properties)  MPFGenericJob(\n  const string  job_name,\n  const string  data_uri,\n  const MPFGenericTrack  track,\n  const Properties  job_properties,\n  const Properties  media_properties)\n}   Members:   \n   \n     \n       Member \n       Data Type \n       Description \n     \n   \n   \n     \n       job_name \n       const string  & \n       See  MPFJob.job_name  for description. \n     \n     \n       data_uri \n       const string  & \n       See  MPFJob.data_uri  for description. \n     \n     \n       track \n       const MPFGenericTrack  & \n       An  MPFGenericTrack  from the previous pipeline stage. Provided when feed forward is enabled. See  Feed Forward Guide . \n     \n     \n       job_properties \n       const Properties  & \n       See  MPFJob.job_properties  for description. \n     \n     \n       media_properties \n       const Properties  & \n       \n        See  MPFJob.media_properties  for description.\n         \n        Includes the following key-value pair:\n         \n           MIME_TYPE  : the MIME type of the media", 
            "title": "MPFGenericJob"
        }, 
        {
            "location": "/CPP-Batch-Component-API/index.html#detection-job-result-classes", 
            "text": "", 
            "title": "Detection Job Result Classes"
        }, 
        {
            "location": "/CPP-Batch-Component-API/index.html#mpfimagelocation", 
            "text": "Structure used to store the location of detected objects in a image file.   Constructor(s):   MPFImageLocation()\nMPFImageLocation(\n  int x_left_upper,\n  int y_left_upper,\n  int width,\n  int height,\n  float confidence = -1,\n  const Properties  detection_properties = {})   Members:      Member  Data Type  Description      x_left_upper  int  Upper left X coordinate of the detected object.    y_left_upper  int  Upper left Y coordinate of the detected object.    width  int  The width of the detected object.    height  int  The height of the detected object.    confidence  float  Represents the \"quality\" of the detection. The range depends on the detection algorithm. 0.0 is lowest quality. Higher values are higher quality. Using a standard range of [0.0 - 1.0] is advised. If the component is unable to supply a confidence value, it should return -1.0.    detection_properties  Properties   Optional additional information about the detected object. There is no restriction on the keys or the number of entries that can be added to the detection_properties map. For best practice, keys should be in all CAPS. See the  section  for  ROTATION  and  HORIZONTAL_FLIP  below,      Example:   A component that performs generic object classification can add an entry to  detection_properties  where the key is  CLASSIFICATION  and the value is the type of object detected.  \nMPFImageLocation { \n    x_left_upper = 0, y_left_upper = 0, width = 100, height = 50, confidence = 1.0,\n    { {\"CLASSIFICATION\", \"backpack\"} } \n}", 
            "title": "MPFImageLocation"
        }, 
        {
            "location": "/CPP-Batch-Component-API/index.html#rotation-and-horizontal-flip", 
            "text": "When the  detection_properties  map contains a  ROTATION  key, it should be a floating point value in the interval [0.0, 360.0)  indicating the orientation of the detection in degrees in the counter-clockwise direction.\nIn order to view the detection in the upright orientation, it must be rotated the given number of degrees in the\nclockwise direction.  The  detection_properties  map can also contain a  HORIZONTAL_FLIP  property that will either be  \"true\"  or  \"false\" .\nThe  detection_properties  map may have both  HORIZONTAL_FLIP  and  ROTATION  keys.  The Workflow Manager performs the following algorithm to draw the bounding box when generating markup:   \n    Draw the rectangle ignoring rotation and flip.   \n   Rotate the rectangle counter-clockwise the given number of degrees around its top left corner.   \n  If the rectangle is flipped, flip horizontally around the top left corner.    In the image above you can see the three steps required to properly draw a bounding box.\nStep 1 is drawn in red. Step 2 is drawn in blue. Step 3 and the final result is drawn in green.\nThe detection for the image above is:  \nMPFImageLocation { \n    x_left_upper = 210, y_left_upper = 189, width = 177, height = 41, confidence = 1.0,\n    { {\"ROTATION\", \"15\"}, { \"HORIZONTAL_FLIP\", \"true\" } } \n}  Note that the  x_left_upper ,  y_left_upper ,  width , and  height  values describe the red rectangle. The addition\nof the  ROTATION  property results in the blue rectangle, and the addition of the  HORIZONTAL_FLIP  property results\nin the green rectangle.   One way to think about the process is \"draw the unrotated and unflipped rectangle, stick a pin in the upper left corner,\nand then rotate and flip around the pin\".", 
            "title": "Rotation and Horizontal Flip"
        }, 
        {
            "location": "/CPP-Batch-Component-API/index.html#rotation-only-example", 
            "text": "The Workflow Manager generated the above image by performing markup on the original image with the following\ndetection:  \nMPFImageLocation { \n    x_left_upper = 156, y_left_upper = 339, width = 194, height = 243, confidence = 1.0,\n    { {\"ROTATION\", \"90.0\"} } \n}  The markup process followed steps 1 and 2 in the previous section, skipping step 3 because there is no HORIZONTAL_FLIP .   In order to properly extract the detection region from the original image, such as when generating an artifact, you\nwould need to rotate the region in the above image 90 degrees clockwise around the cyan dot currently shown in the\nbottom-left corner so that the face is in the proper upright position.   When the rotation is properly corrected in this way, the cyan dot will appear in the top-left corner of the bounding\nbox. That is why its position is described using the  x_left_upper , and  y_left_upper  variables. They refer to the\ntop-left corner of the correctly oriented region.", 
            "title": "Rotation-Only Example"
        }, 
        {
            "location": "/CPP-Batch-Component-API/index.html#mpfvideotrack", 
            "text": "Structure used to store the location of detected objects in a video file.   Constructor(s):   MPFVideoTrack()\nMPFVideoTrack(\n  int start_frame,\n  int stop_frame,\n  float confidence = -1,\n  map int, MPFImageLocation  frame_locations,\n  const Properties  detection_properties = {})   Members:      Member  Data Type  Description      start_frame  int  The first frame number (0-based index) that contained the detected object.    stop_frame  int  The last frame number (0-based index) that contained the detected object.    frame_locations  map int, MPFImageLocation  A map of individual detections. The key for each map entry is the frame number where the detection was generated, and the value is a  MPFImageLocation  calculated as if that frame was a still image. Note that a key-value pair is  not  required for every frame between the track start frame and track stop frame.    confidence  float  Represents the \"quality\" of the detection. The range depends on the detection algorithm. 0.0 is lowest quality. Higher values are higher quality. Using a standard range of [0.0 - 1.0] is advised. If the component is unable to supply a confidence value, it should return -1.0.    detection_properties  Properties   Optional additional information about the detected object. There is no restriction on the keys or the number of entries that can be added to the detection_properties map. For best practice, keys should be in all CAPS.      Example:    NOTE:  Currently,  MPFVideoTrack.detection_properties  do not show up in the JSON output object or are used by the WFM in any way.   A component that detects text can add an entry to  detection_properties  where the key is  TRANSCRIPT  and the value is a string representing the text found in the video segment.  MPFVideoTrack track;\ntrack.start_frame = 0;\ntrack.stop_frame = 5;\ntrack.confidence = 1.0;\ntrack.frame_locations = frame_locations;\ntrack.detection_properties[ TRANSCRIPT ] =  RE5ULTS FR0M A TEXT DETECTER ;", 
            "title": "MPFVideoTrack"
        }, 
        {
            "location": "/CPP-Batch-Component-API/index.html#mpfaudiotrack", 
            "text": "Structure used to store the location of detected objects in an audio file.   Constructor(s):   MPFAudioTrack()\nMPFAudioTrack(\n  int start_time,\n  int stop_time,\n  float confidence = -1,\n  const Properties  detection_properties = {})   Members:      Member  Data Type  Description      start_time  int  The time (0-based index, in ms) when the audio detection event started.    stop_time  int  The time (0-based index, in ms) when the audio detection event stopped.    confidence  float  Represents the \"quality\" of the detection. The range depends on the detection algorithm. 0.0 is lowest quality. Higher values are higher quality. Using a standard range of [0.0 - 1.0] is advised. If the component is unable to supply a confidence value, it should return -1.0.    detection_properties  Properties   Optional additional information about the detection. There is no restriction on the keys or the number of entries that can be added to the detection_properties map. For best practice, keys should be in all CAPS.      NOTE:  Currently,  MPFAudioTrack.detection_properties  do not show up in the JSON output object or are used by the WFM in any way.", 
            "title": "MPFAudioTrack"
        }, 
        {
            "location": "/CPP-Batch-Component-API/index.html#mpfgenerictrack", 
            "text": "Structure used to store the location of detected objects in a file that is not a video, image, or audio file. The file is of the UNKNOWN type and handled generically.   Constructor(s):   MPFGenericTrack()\nMPFGenericTrack(\n  float confidence = -1,\n  const Properties  detection_properties = {})   Members:      Member  Data Type  Description      confidence  float  Represents the \"quality\" of the detection. The range depends on the detection algorithm. 0.0 is lowest quality. Higher values are higher quality. Using a standard range of [0.0 - 1.0] is advised. If the component is unable to supply a confidence value, it should return -1.0.    detection_properties  Properties   Optional additional information about the detection. There is no restriction on the keys or the number of entries that can be added to the detection_properties map. For best practice, keys should be in all CAPS.", 
            "title": "MPFGenericTrack"
        }, 
        {
            "location": "/CPP-Batch-Component-API/index.html#exception-types", 
            "text": "", 
            "title": "Exception Types"
        }, 
        {
            "location": "/CPP-Batch-Component-API/index.html#mpfdetectionexception", 
            "text": "Exception that should be thrown by the  GetDetections()  methods when an error occurs. \nThe content of the  error_code  and  what()  members will appear in the JSON output object.   Constructors:   MPFDetectionException(MPFDetectionError error_code, const std::string  what =  )\nMPFDetectionException(const std::string  what)     Member  Data Type  Description      error_code  MPFDetectionError  Specifies the error type. See  MPFDetectionError .    what()  const char*  Textual description of the specific error. (Inherited from  std::exception )", 
            "title": "MPFDetectionException"
        }, 
        {
            "location": "/CPP-Batch-Component-API/index.html#enumeration-types", 
            "text": "", 
            "title": "Enumeration Types"
        }, 
        {
            "location": "/CPP-Batch-Component-API/index.html#mpfdetectionerror", 
            "text": "Enum used to indicate the type of error that occurred in a  GetDetections()  method. It is used as a parameter to \nthe  MPFDetectionException  constructor. A component is not required to support all error types.     ENUM  Description      MPF_OTHER_DETECTION_ERROR_TYPE  The component function has failed for a reason that is not captured by any of the other error codes.    MPF_DETECTION_NOT_INITIALIZED  The initialization of the component, or the initialization of any of its dependencies, has failed for any reason.    MPF_UNRECOGNIZED_DATA_TYPE  The media data type received by a component is not one of the values contained in the MPFDetectionDataType enum.  Note that this failure is normally caught by the Component Executable, before a job is passed to the component logic.    MPF_UNSUPPORTED_DATA_TYPE  The job passed to a component requests processing of a job of an unsupported type. For instance, a component that is only capable of processing audio files should return this error code if a video or image job request is received.    MPF_INVALID_DATAFILE_URI  The string containing the URI location of the input data file is invalid or empty.    MPF_COULD_NOT_OPEN_DATAFILE  The data file to be processed could not be opened for any reason, such as a permissions failure, or an unreachable URI.    MPF_COULD_NOT_READ_DATAFILE  There is a failure reading data from a successfully opened input data file.    MPF_FILE_WRITE_ERROR  The component received a failure for any reason when attempting to write to a file.    MPF_IMAGE_READ_ERROR  The component failed to read the image provided by the URI. For example, it might indicate the failure of a call to  MPFImageReader::GetImage() , or  cv::imread() .    MPF_BAD_FRAME_SIZE  The frame data retrieved has an incorrect or invalid frame size. For example, if a call to  cv::imread()  returns a frame of data with either the number of rows or columns less than or equal to 0.    MPF_BOUNDING_BOX_SIZE_ERROR  The calculation of a detection location bounding box has failed. For example, a component may be using an external library to detect objects, but the bounding box returned by that library lies partially outside the frame boundaries.    MPF_INVALID_FRAME_INTERVAL  An invalid or unsupported frame interval was received.    MPF_INVALID_START_FRAME  The component received an invalid start frame number. For example, if the start frame is less than zero, or greater than the stop frame, this error code should be used.    MPF_INVALID_STOP_FRAME  The component receives an invalid stop frame number. For example, if the stop frame is less than the start frame, or greater than the number of the last frame in a video segment, this error code should be used.    MPF_DETECTION_FAILED  General failure of a detection algorithm.  This does not indicate a lack of detections found in the media, but rather a break down in the algorithm that makes it impossible to continue to try to detect objects.    MPF_DETECTION_TRACKING_FAILED  General failure of a tracking algorithm.  This does not indicate a lack of tracks generated for the media, but rather a break down in the algorithm that makes it impossible to continue to try to track objects.    MPF_INVALID_PROPERTY  The component received a property that is unrecognized or has an invalid/out-of-bounds value.    MPF_MISSING_PROPERTY  The component received a job that is missing a required property.    MPF_JOB_PROPERTY_IS_NOT_INT  A job property is supposed to be an integer type, but it is of some other type, such as a boolean or a floating point value.    MPF_JOB_PROPERTY_IS_NOT_FLOAT  A job property is supposed to be a floating point type, but it is of some other type, such as a boolean value.    MPF_MEMORY_ALLOCATION_FAILED  The component failed to allocate memory for any reason.    MPF_GPU_ERROR  The job was configured to execute on a GPU, but there was an issue with the GPU or no GPU was detected.", 
            "title": "MPFDetectionError"
        }, 
        {
            "location": "/CPP-Batch-Component-API/index.html#utility-classes", 
            "text": "For convenience, the OpenMPF provides the  MPFImageReader  ( source ) and  MPFVideoCapture  ( source ) utility classes to perform horizontal flipping, rotation, and cropping to a region of interest. Note, that when using these classes, the component will also need to utilize the class to perform a reverse transform to convert the transformed pixel coordinates back to the original (e.g. pre-flipped, pre-rotated, and pre-cropped) coordinate space.", 
            "title": "Utility Classes"
        }, 
        {
            "location": "/CPP-Batch-Component-API/index.html#c-component-build-environment", 
            "text": "A C++ component library must be built for the same C++ compiler and Linux version that is used by the OpenMPF Component Executable. This is to ensure compatibility between the executable and the library functions at the Application Binary Interface (ABI) level. At this writing, the OpenMPF runs on CentOS 7.4.1708 (kernel version 3.10.0-693), and the OpenMPF C++ Component Executable is built with g++ (GCC) 4.8.5 20150623 (Red Hat 4.8.5-16).  Components should be supplied as a tar file, which includes not only the component library, but any other libraries or files needed for execution. This includes all other non-standard libraries used by the component (aside from the standard Linux and C++ libraries), and any configuration or data files.", 
            "title": "C++ Component Build Environment"
        }, 
        {
            "location": "/CPP-Batch-Component-API/index.html#component-development-best-practices", 
            "text": "", 
            "title": "Component Development Best Practices"
        }, 
        {
            "location": "/CPP-Batch-Component-API/index.html#single-threaded-operation", 
            "text": "Implementations are encouraged to operate in single-threaded mode. OpenMPF will parallelize components through multiple instantiations of the component, each running as a separate service.", 
            "title": "Single-threaded Operation"
        }, 
        {
            "location": "/CPP-Batch-Component-API/index.html#stateless-behavior", 
            "text": "OpenMPF components should be stateless in operation and give identical output for a provided input (i.e. when processing the same  MPFJob ).", 
            "title": "Stateless Behavior"
        }, 
        {
            "location": "/CPP-Batch-Component-API/index.html#gpu-support", 
            "text": "For components that want to take advantage of NVIDA GPU processors, please read the  GPU Support Guide . Also ensure that your build environment has the NVIDIA CUDA Toolkit installed, as described in the  Build Environment Setup Guide .", 
            "title": "GPU Support"
        }, 
        {
            "location": "/CPP-Batch-Component-API/index.html#component-packaging", 
            "text": "It is recommended that C++ components are organized according to the following directory structure:  componentName\n\u251c\u2500\u2500 config - Logging and other component-specific configuration\n\u251c\u2500\u2500 descriptor\n\u2502   \u2514\u2500\u2500 descriptor.json\n\u2514\u2500\u2500 lib\n    \u2514\u2500\u2500libComponentName.so - Compiled component library  Once built, components should be packaged into a .tar.gz containing the contents of the directory shown above.", 
            "title": "Component Packaging"
        }, 
        {
            "location": "/CPP-Batch-Component-API/index.html#logging", 
            "text": "It is recommended to use  Apache log4cxx  for OpenMPF Component logging.  Note that multiple instances of the same component can log to the same file. Also, logging content can span multiple lines.  Log files should be output to: ${MPF_LOG_PATH}/${THIS_MPF_NODE}/log/ componentName .log  Each log statement must take the form: DATE TIME LEVEL CONTENT  The following log LEVELs are supported: FATAL, ERROR, WARN,  INFO,  DEBUG, TRACE .  For example: 2016-02-09 13:42:42,341 INFO - Starting sample-component: [  OK  ]  The following configuration can be used to match the format of other OpenMPF logs:  log4j:configuration xmlns:log4j= http://jakarta.apache.org/log4j/ \n\n   !-- Output the log message to log file-- \n   appender name= SAMPLECOMPONENT-FILE  class= org.apache.log4j.DailyRollingFileAppender \n     param name= file  value= ${MPF_LOG_PATH}/${THIS_MPF_NODE}/log/ componentName .log  / \n     param name= DatePattern  value= '.'yyyy-MM-dd  / \n     layout class= org.apache.log4j.PatternLayout \n       param name= ConversionPattern  value= %d %p [%t] %c{36}:%L - %m%n  / \n     /layout \n   /appender \n\n   logger name=  SampleComponent  additivity= false \n     level value= INFO / \n     appender-ref ref= SAMPLECOMPONENT-FILE / \n   /logger  /log4j:configuration", 
            "title": "Logging"
        }, 
        {
            "location": "/CPP-Streaming-Component-API/index.html", 
            "text": "NOTICE:\n This software (or technical data) was produced for the U.S. Government under contract, and is subject to the Rights in Data-General Clause 52.227-14, Alt. IV (DEC 2007). Copyright 2019 The MITRE Corporation. All Rights Reserved.\n\n\n\n\nAPI Overview\n\n\nIn OpenMPF, a \ncomponent\n is a plugin that receives jobs (containing media), processes that  media, and returns results.\n\n\nThe OpenMPF Streaming Component API currently supports the development of \ndetection components\n, which are used detect objects in live RTSP or HTTP video streams.\n\n\nUsing this API, detection components can be built to provide:\n\n\n\n\nDetection (Localizing an object)\n\n\nTracking (Localizing an object across multiple frames)\n\n\nClassification (Detecting the type of object and optionally localizing that object)\n\n\n\n\nEach frame of the video is processed as it is read from the stream. After processing enough frames to form a segment (for example, 100 frames), the component starts processing the next segment. Like with batch processing, each segment read from the stream is processed independently of the rest. No detection or track information is carried over between segments. Tracks are not merged across segments.\n\n\nHow Components Integrate into OpenMPF\n\n\nComponents are integrated into OpenMPF through the use of OpenMPF's \nComponent Executable\n. Developers create component libraries that encapsulate the component detection logic. Each instance of the Component Executable loads one of these libraries and uses it to service job requests sent by the OpenMPF Workflow Manager (WFM).\n\n\nThe Component Executable:\n\n\n\n\nReceives and parses job requests from the WFM\n\n\nInvokes functions on the component library to obtain detection results\n\n\nPopulates and sends the respective responses to the WFM\n\n\n\n\nThe basic psuedocode for the Component Executable is as follows:\n\n\nwhile (has_next_frame) {\n    if (is_new_segment) {\n        component-\nBeginSegment(video_segment_info)\n    }\n    activity_found = component-\nProcessFrame(frame, frame_number) // Component logic does the work here\n    if (activity_found \n !already_sent_new_activity_alert_for_this_segment) {\n        SendActivityAlert(frame_number)\n    }\n    if (is_end_of_segment) {\n        streaming_video_tracks = component-\nEndSegment()\n        SendSummaryReport(frame_number, component-\ngetDetectionType(), streaming_video_tracks)\n    }\n}\n\n\n\n\nEach instance of a Component Executable runs as a separate process. Generally, each process will execute a different detection algorithm that corresponds to a single stage in a detection pipeline. Each instance is started by the Node Manager as needed in order to execute a streaming video job. The Node Manager will monitor the process status and eventually stop it.\n\n\nThe Component Executable invokes functions on the Component Logic to get detection objects, and subsequently generates new track alerts and segment summary reports based on the output. These alerts and reports are sent to the WFM.\n\n\nA component developer implements a detection component by extending \nMPFStreamingDetectionComponent\n.\n\n\nGetting Started\n\n\nThe quickest way to get started with the C++ Streaming Component API is to first read the \nOpenMPF Component API Overview\n and then \nreview the source\n of an example OpenMPF C++ detection component that supports stream processing.\n\n\nDetection components are implemented by:\n\n\n\n\nExtending \nMPFStreamingDetectionComponent\n.\n\n\nBuilding the component into a shared object library. (See \nHelloWorldComponent CMakeLists.txt\n).\n\n\nPackaging the component into an OpenMPF-compliant .tar.gz file. (See \nComponent Packaging\n).\n\n\nRegistering the component with OpenMPF. (See \nPackaging and Registering a Component\n).\n\n\n\n\nAPI Specification\n\n\nThe figure below presents a high-level component diagram of the C++ Streaming Component API:\n\n\n\n\nThe API consists of a \nDetection Component Interface\n and related input and output structures.\n\n\nDetection Component Interface\n\n\n\n\nMPFStreamingDetectionComponent\n - Abstract class that should be extended by all OpenMPF C++ detection components that perform stream processing.\n\n\n\n\nInputs\n\n\nThe following data structures contain details about a specific job, and a video segment (work unit) associated with that job:\n\n\n\n\nMPFStreamingVideoJob\n\n\nVideoSegmentInfo\n\n\n\n\nOutputs\n\n\nThe following data structures define detection results:\n\n\n\n\nMPFImageLocation\n\n\nMPFVideoTrack\n\n\n\n\nComponent Factory Functions\n\n\nEvery detection component must include the following macro in its implementation:\n\n\nEXPORT_MPF_STREAMING_COMPONENT(TYPENAME);\n\n\n\n\nThis creator macro takes the \nTYPENAME\n of the detection component (for example, \u201cStreamingHelloWorld\u201d). This macro creates the factory function that the OpenMPF Component Executable will call in order to instantiate the detection component. The creation function is called once, to obtain an instance of the component, after the component library has been loaded into memory.\n\n\nThis macro also creates the factory function that the Component Executable will use to delete that instance of the detection component.\n\n\nThis macro must be used outside of a class declaration, preferably at the bottom or top of a component source (.cpp) file.\n\n\nExample:\n\n\n// Note: Do not put the TypeName/Class Name in quotes\nEXPORT_MPF_STREAMING_COMPONENT(StreamingHelloWorld);\n\n\n\n\nDetection Component Interface\n\n\nThe \nMPFStreamingDetectionComponent\n class is the abstract class utilized by all OpenMPF C++ detection components that perform stream processing. This class provides functions for developers to integrate detection logic into OpenMPF.\n\n\nSee the latest source here.\n\n\nConstructor\n\n\nSuperclass constructor that must be invoked by the constructor of the component subclass.\n\n\n\n\nFunction Definition:\n\n\n\n\nMPFStreamingDetectionComponent(const MPFStreamingVideoJob \njob)\n\n\n\n\n\n\nParameters:\n\n\n\n\n\n\n\n\n\n\nParameter\n\n\nData Type\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\njob\n\n\nconst MPFStreamingVideoJob \n\n\nStructure containing details about the work to be performed. See \nMPFStreamingVideoJob\n\n\n\n\n\n\n\n\n\n\n\n\nReturns: none\n\n\n\n\n\n\nExample:\n\n\n\n\n\n\nSampleComponent::SampleComponent(const MPFStreamingVideoJob \njob)\n        : MPFStreamingDetectionComponent(job)\n        , hw_logger_(GetLogger(job.run_directory))\n        , job_name_(job.job_name) {\n\n    LOG4CXX_INFO(hw_logger_, \n[\n \n job_name_ \n \n] Initialized SampleComponent component.\n)\n}\n\n\n\n\nGetDetectionType()\n\n\nReturns the type of object detected by the component.\n\n\n\n\nFunction Definition:\n\n\n\n\nstring GetDetectionType()\n\n\n\n\n\n\n\n\nParameters: none\n\n\n\n\n\n\nReturns: (\nstring\n) The type of object detected by the component. Should be in all CAPS. Examples include: \nFACE\n, \nMOTION\n, \nPERSON\n, \nCLASS\n (for object classification), or \nTEXT\n.\n\n\n\n\n\n\nExample:\n\n\n\n\n\n\nstring SampleComponent::GetDetectionType() {\n    return \nFACE\n;\n}\n\n\n\n\nBeginSegment(VideoSegmentInfo)\n\n\nIndicate the beginning of a new video segment. The next call to \nProcessFrame()\n will be the first frame of the new segment. \nProcessFrame()\n will never be called before this function.\n\n\n\n\nFunction Definition:\n\n\n\n\nvoid BeginSegment(const VideoSegmentInfo \nsegment_info)\n\n\n\n\n\n\nParameters:\n\n\n\n\n\n\n\n\n\n\nParameter\n\n\nData Type\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nsegment_info\n\n\nconst VideoSegmentInfo \n\n\nStructure containing details about next video segment to process. See \nVideoSegmentInfo\n\n\n\n\n\n\n\n\n\n\n\n\nReturns: none\n\n\n\n\n\n\nExample:\n\n\n\n\n\n\nvoid SampleComponent::BeginSegment(const VideoSegmentInfo \nsegment_info) {\n    // Prepare for next segment\n}\n\n\n\n\nProcessFrame(Mat ...)\n\n\nProcess a single video frame for the current segment.\n\n\nMust return true when the component begins generating the first track for the current segment. After it returns true, the Component Executable will ignore the return value until the component begins processing the next segment.\n\n\nIf the \njob_properties\n map contained in the \nMPFStreamingVideoJob\n struct passed to the component constructor contains a CONFIDENCE_THRESHOLD entry, then this function should only return true for a detection with a confidence value that meets or exceeds that threshold. After the Component Executable invokes \nEndSegment()\n to retrieve the segment tracks, it will discard detections that are below the threshold. If all the detections in a track are below the threshold, then the entire track will be discarded.\n\n\nNote that this function may not be invoked for every frame in the current segment. For example, if FRAME_INTERVAL = 2, then this function will only be invoked for every other frame since those are the only ones that need to be processed.\n\n\nAlso, it may not be invoked for the first nor last frame in the segment. For example, if FRAME_INTERVAL = 3 and the segment size is 10, then it will be invoked for frames {0, 3, 6, 9} for the first segment, and frames {12, 15, 18} for the second segment.\n\n\n\n\nFunction Definition:   \n\n\n\n\nbool ProcessFrame(const cv::Mat \nframe, int frame_number)\n\n\n\n\n\n\nParameters:\n\n\n\n\n\n\n\n\n\n\nParameter\n\n\nData Type\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nframe\n\n\nconst cv::Mat \n\n\nOpenCV class containing frame data. See \ncv::Mat\n\n\n\n\n\n\nframe_number\n\n\nint\n\n\nA unique frame number (0-based index). Guaranteed to be greater than the frame number passed to the last invocation of this function.\n\n\n\n\n\n\n\n\n\n\n\n\nReturns: (\nbool\n) True when the component begins generating the first track for the current segment; false otherwise.\n\n\n\n\n\n\nExample:\n\n\n\n\n\n\nbool SampleComponent::ProcessFrame(const cv::Mat \nframe, int frame_number) {\n    // Look for detections. Generate tracks and store them until the end of the segment.\n    if (started_first_track_in_current_segment) {\n        return true;        \n    } else {\n        return false;\n    }\n}\n\n\n\n\nEndSegment()\n\n\nIndicate the end of the current video segment. This will always be called after \nBeginSegment()\n. Generally, \nProcessFrame()\n will be called one or more times before this function, depending on the number of frames in the segment and the number of frames actually read from the stream.\n\n\nNote that the next time \nBeginSegment()\n is called, this component should start generating new tracks. Each time \nEndSegment()\n is called, it should return only the most recent track data for that segment. Tracks should not be carried over between segments. Do not append new detections to a preexisting track from the previous segment and return that cumulative track when this function is called.\n\n\n\n\nFunction Definition:\n\n\n\n\nvector\nMPFVideoTrack\n EndSegment()\n\n\n\n\n\n\n\n\nParameters: none\n\n\n\n\n\n\nReturns: (\nvector\nMPFVideoTrack\n) The \nMPFVideoTrack\n data for each detected object.\n\n\n\n\n\n\nExample:\n\n\n\n\n\n\nvector\nMPFVideoTrack\n SampleComponent::EndSegment() {\n    // Perform any necessary cleanup before processing the next segment.\n    // Return the collection of tracks generated for this segment only.\n}\n\n\n\n\nDetection Job Data Structures\n\n\nThe following data structures contain details about a specific job, and a video segment (work unit) associated with that job:\n\n\n\n\nMPFStreamingVideoJob\n\n\nVideoSegmentInfo\n\n\n\n\nThe following data structures define detection results:\n\n\n\n\nMPFImageLocation\n\n\nMPFVideoTrack\n\n\n\n\nMPFStreamingVideoJob\n\n\nStructure containing information about a job to be performed on a video stream.\n\n\n\n\nConstructor(s):\n\n\n\n\nMPFStreamingVideoJob(\n  const string \njob_name,\n  const string \nrun_directory,\n  const Properties \njob_properties,\n  const Properties \nmedia_properties)\n}\n\n\n\n\n\n\nMembers:\n\n\n\n\n\n\n\n\n\n\nMember\n\n\nData Type\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\njob_name\n\n\nconst string  \n\n\nA specific name given to the job by the OpenMPF framework. This value may be used, for example, for logging and debugging purposes.\n\n\n\n\n\n\nrun_directory \n\n\nconst string  \n\n\nContains the full path of the parent folder above where the component is installed. This parent folder is also known as the plugin folder.\n\n\n\n\n\n\njob_properties \n\n\nconst Properties \n\n\nContains a map of \nstring, string\n which represents the property name and the property value. The key corresponds to the property name specified in the component descriptor file described in \nPackaging and Registering a Component\n. Values are determined when creating a pipeline or when submitting a job. \n Note: The job_properties map may not contain the full set of job properties. For properties not contained in the map, the component must use a default value.\n\n\n\n\n\n\nmedia_properties \n\n\nconst Properties \n\n\nContains a map of \nstring, string\n of metadata about the media associated with the job. The entries in the map vary depending on the type of media. Refer to the type-specific job structures below.\n\n\n\n\n\n\n\n\nVideoSegmentInfo\n\n\nStructure containing information about a segment of a video stream to be processed. A segment is a subset of contiguous video frames.\n\n\n\n\nConstructor(s):\n\n\n\n\nVideoSegmentInfo(\n  int segment_number,\n  int start_frame,\n  int end_frame,\n  int frame_width,\n  int frame_height\n}\n\n\n\n\n\n\nMembers:\n\n\n\n\n\n\n\n\n\n\nMember\n\n\nData Type\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nsegment_number\n\n\nint\n\n\nA unique segment number (0-based index).\n\n\n\n\n\n\nstart_frame\n\n\nint\n\n\nThe frame number (0-based index) corresponding to the first frame in this segment.\n\n\n\n\n\n\nend_frame\n\n\nint\n\n\nThe frame number (0-based index) corresponding to the last frame in this segment.\n\n\n\n\n\n\nframe_width\n\n\nint\n\n\nThe height of each frame in this segment.\n\n\n\n\n\n\nframe_height\n\n\nint\n\n\nThe width of each frame in this segment.\n\n\n\n\n\n\n\n\nDetection Job Result Classes\n\n\nMPFImageLocation\n\n\nStructure used to store the location of detected objects in a single video frame (image).\n\n\n\n\nConstructor(s):\n\n\n\n\nMPFImageLocation()\nMPFImageLocation(\n  int x_left_upper,\n  int y_left_upper,\n  int width,\n  int height,\n  float confidence = -1,\n  const Properties \ndetection_properties = {})\n\n\n\n\n\n\nMembers:\n\n\n\n\n\n\n\n\n\n\nMember\n\n\nData Type\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nx_left_upper\n\n\nint\n\n\nUpper left X coordinate of the detected object.\n\n\n\n\n\n\ny_left_upper\n\n\nint\n\n\nUpper left Y coordinate of the detected object.\n\n\n\n\n\n\nwidth\n\n\nint\n\n\nThe width of the detected object.\n\n\n\n\n\n\nheight\n\n\nint\n\n\nThe height of the detected object.\n\n\n\n\n\n\nconfidence\n\n\nfloat\n\n\nRepresents the \"quality\" of the detection. The range depends on the detection algorithm. 0.0 is lowest quality. Higher values are higher quality. Using a standard range of [0.0 - 1.0] is advised. If the component is unable to supply a confidence value, it should return -1.0.\n\n\n\n\n\n\ndetection_properties\n\n\nProperties \n\n\nOptional additional information about the detected object. There is no restriction on the keys or the number of entries that can be added to the detection_properties map. For best practice, keys should be in all CAPS.\n\n\n\n\n\n\n\n\n\n\nExample:\n\n\n\n\nA component that performs generic object classification can add an entry to \ndetection_properties\n where the key is \nCLASSIFICATION\n and the value is the type of object detected.\n\n\nMPFImageLocation detection;\ndetection.x_left_upper = 0;\ndetection.y_left_upper = 0;\ndetection.width = 100;\ndetection.height = 100;\ndetection.confidence = 1.0;\ndetection.detection_properties[\nCLASSIFICATION\n] = \nbackpack\n;\n\n\n\n\nMPFVideoTrack\n\n\nStructure used to store the location of detected objects in a video file.\n\n\n\n\nConstructor(s):\n\n\n\n\nMPFVideoTrack()\nMPFVideoTrack(\n  int start_frame,\n  int stop_frame,\n  float confidence = -1,\n  map\nint, MPFImageLocation\n frame_locations,\n  const Properties \ndetection_properties = {})\n\n\n\n\n\n\nMembers:\n\n\n\n\n\n\n\n\n\n\nMember\n\n\nData Type\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nstart_frame\n\n\nint\n\n\nThe first frame number (0-based index) that contained the detected object.\n\n\n\n\n\n\nstop_frame\n\n\nint\n\n\nThe last frame number (0-based index) that contained the detected object.\n\n\n\n\n\n\nframe_locations\n\n\nmap\nint, MPFImageLocation\n\n\nA map of individual detections. The key for each map entry is the frame number where the detection was generated, and the value is a \nMPFImageLocation\n calculated as if that frame was a still image. Note that a key-value pair is \nnot\n required for every frame between the track start frame and track stop frame.\n\n\n\n\n\n\nconfidence\n\n\nfloat\n\n\nRepresents the \"quality\" of the detection. The range depends on the detection algorithm. 0.0 is lowest quality. Higher values are higher quality. Using a standard range of [0.0 - 1.0] is advised. If the component is unable to supply a confidence value, it should return -1.0.\n\n\n\n\n\n\ndetection_properties\n\n\nProperties \n\n\nOptional additional information about the detected object. There is no restriction on the keys or the number of entries that can be added to the detection_properties map. For best practice, keys should be in all CAPS.\n\n\n\n\n\n\n\n\n\n\nExample:\n\n\n\n\n\n\nNOTE:\n Currently, \nMPFVideoTrack.detection_properties\n do not show up in the JSON output object or are used by the WFM in any way.\n\n\n\n\nA component that detects text can add an entry to \ndetection_properties\n where the key is \nTRANSCRIPT\n and the value is a string representing the text found in the video segment.\n\n\nMPFVideoTrack track;\ntrack.start_frame = 0;\ntrack.stop_frame = 5;\ntrack.confidence = 1.0;\ntrack.frame_locations = frame_locations;\ntrack.detection_properties[\nTRANSCRIPT\n] = \nRE5ULTS FR0M A TEXT DETECTER\n;\n\n\n\n\nC++ Component Build Environment\n\n\nA C++ component library must be built for the same C++ compiler and Linux version that is used by the OpenMPF Component Executable. This is to ensure compatibility between the executable and the library functions at the Application Binary Interface (ABI) level. At this writing, the OpenMPF runs on CentOS 7.4.1708 (kernel version 3.10.0-693), and the OpenMPF C++ Component Executable is built with g++ (GCC) 4.8.5 20150623 (Red Hat 4.8.5-16).\n\n\nComponents should be supplied as a tar file, which includes not only the component library, but any other libraries or files needed for execution. This includes all other non-standard libraries used by the component (aside from the standard Linux and C++ libraries), and any configuration or data files.\n\n\nComponent Development Best Practices\n\n\nThrow Exceptions\n\n\nUnlike the \nC++ Batch Component API\n, none of the the C++ Streaming Component API functions return an \nMPFDetectionError\n. Instead, streaming components should throw an exception when a non-recoverable error occurs. The exception should be an instantiation or subclass of \nstd::exception\n and provide a descriptive error message that can be retrieved using \nwhat()\n. For example:\n\n\nbool SampleComponent::ProcessFrame(const cv::Mat \nframe, int frame_number) {\n    // Something bad happened\n    throw std::exception(\nError: Cannot do X with value Y.\n);\n}\n\n\n\n\nThe exception will be handled by the Component Executable. It will immediately invoke \nEndSegment()\n to retrieve the current tracks. Then the component process and streaming job will be terminated.\n\n\nSingle-threaded Operation\n\n\nImplementations are encouraged to operate in single-threaded mode. OpenMPF will parallelize components through multiple instantiations of the component, each running as a separate service.\n\n\nStateless Behavior\n\n\nOpenMPF components should be stateless in operation and give identical output for a provided input (i.e. when processing a segment with the same \nVideoSegmentInfo\n).\n\n\nGPU Support\n\n\nFor components that want to take advantage of NVIDA GPU processors, please read the \nGPU Support Guide\n. Also ensure that your build environment has the NVIDIA CUDA Toolkit installed, as described in the \nBuild Environment Setup Guide\n.\n\n\nComponent Packaging\n\n\nIt is recommended that C++ components are organized according to the following directory structure:\n\n\ncomponentName\n\u251c\u2500\u2500 config - Logging and other component-specific configuration\n\u251c\u2500\u2500 descriptor\n\u2502   \u2514\u2500\u2500 descriptor.json\n\u2514\u2500\u2500 lib\n    \u2514\u2500\u2500libComponentName.so - Compiled component library\n\n\n\n\nOnce built, components should be packaged into a .tar.gz containing the contents of the directory shown above.\n\n\nLogging\n\n\nIt is recommended to use \nApache log4cxx\n for OpenMPF Component logging.\n\n\nNote that multiple instances of the same component can log to the same file. Also, logging content can span multiple lines.\n\n\nLog files should be output to:\n\n${MPF_LOG_PATH}/${THIS_MPF_NODE}/log/\ncomponentName\n.log\n\n\nEach log statement must take the form:\n\nDATE TIME LEVEL CONTENT\n\n\nThe following log LEVELs are supported:\n\nFATAL, ERROR, WARN,  INFO,  DEBUG, TRACE\n.\n\n\nFor example:\n\n2016-02-09 13:42:42,341 INFO - Starting sample-component: [  OK  ]\n\n\nThe following configuration can be used to match the format of other OpenMPF logs:\n\n\nlog4j:configuration xmlns:log4j=\nhttp://jakarta.apache.org/log4j/\n\n\n  \n!-- Output the log message to log file--\n\n  \nappender name=\nSAMPLECOMPONENT-FILE\n class=\norg.apache.log4j.DailyRollingFileAppender\n\n    \nparam name=\nfile\n value=\n${MPF_LOG_PATH}/${THIS_MPF_NODE}/log/\ncomponentName\n.log\n /\n\n    \nparam name=\nDatePattern\n value=\n'.'yyyy-MM-dd\n /\n\n    \nlayout class=\norg.apache.log4j.PatternLayout\n\n      \nparam name=\nConversionPattern\n value=\n%d %p [%t] %c{36}:%L - %m%n\n /\n\n    \n/layout\n\n  \n/appender\n\n\n  \nlogger name= \nSampleComponent\n additivity=\nfalse\n\n    \nlevel value=\nINFO\n/\n\n    \nappender-ref ref=\nSAMPLECOMPONENT-FILE\n/\n\n  \n/logger\n\n\n\n/log4j:configuration", 
            "title": "C++ Streaming Component API"
        }, 
        {
            "location": "/CPP-Streaming-Component-API/index.html#api-overview", 
            "text": "In OpenMPF, a  component  is a plugin that receives jobs (containing media), processes that  media, and returns results.  The OpenMPF Streaming Component API currently supports the development of  detection components , which are used detect objects in live RTSP or HTTP video streams.  Using this API, detection components can be built to provide:   Detection (Localizing an object)  Tracking (Localizing an object across multiple frames)  Classification (Detecting the type of object and optionally localizing that object)   Each frame of the video is processed as it is read from the stream. After processing enough frames to form a segment (for example, 100 frames), the component starts processing the next segment. Like with batch processing, each segment read from the stream is processed independently of the rest. No detection or track information is carried over between segments. Tracks are not merged across segments.", 
            "title": "API Overview"
        }, 
        {
            "location": "/CPP-Streaming-Component-API/index.html#how-components-integrate-into-openmpf", 
            "text": "Components are integrated into OpenMPF through the use of OpenMPF's  Component Executable . Developers create component libraries that encapsulate the component detection logic. Each instance of the Component Executable loads one of these libraries and uses it to service job requests sent by the OpenMPF Workflow Manager (WFM).  The Component Executable:   Receives and parses job requests from the WFM  Invokes functions on the component library to obtain detection results  Populates and sends the respective responses to the WFM   The basic psuedocode for the Component Executable is as follows:  while (has_next_frame) {\n    if (is_new_segment) {\n        component- BeginSegment(video_segment_info)\n    }\n    activity_found = component- ProcessFrame(frame, frame_number) // Component logic does the work here\n    if (activity_found   !already_sent_new_activity_alert_for_this_segment) {\n        SendActivityAlert(frame_number)\n    }\n    if (is_end_of_segment) {\n        streaming_video_tracks = component- EndSegment()\n        SendSummaryReport(frame_number, component- getDetectionType(), streaming_video_tracks)\n    }\n}  Each instance of a Component Executable runs as a separate process. Generally, each process will execute a different detection algorithm that corresponds to a single stage in a detection pipeline. Each instance is started by the Node Manager as needed in order to execute a streaming video job. The Node Manager will monitor the process status and eventually stop it.  The Component Executable invokes functions on the Component Logic to get detection objects, and subsequently generates new track alerts and segment summary reports based on the output. These alerts and reports are sent to the WFM.  A component developer implements a detection component by extending  MPFStreamingDetectionComponent .", 
            "title": "How Components Integrate into OpenMPF"
        }, 
        {
            "location": "/CPP-Streaming-Component-API/index.html#getting-started", 
            "text": "The quickest way to get started with the C++ Streaming Component API is to first read the  OpenMPF Component API Overview  and then  review the source  of an example OpenMPF C++ detection component that supports stream processing.  Detection components are implemented by:   Extending  MPFStreamingDetectionComponent .  Building the component into a shared object library. (See  HelloWorldComponent CMakeLists.txt ).  Packaging the component into an OpenMPF-compliant .tar.gz file. (See  Component Packaging ).  Registering the component with OpenMPF. (See  Packaging and Registering a Component ).", 
            "title": "Getting Started"
        }, 
        {
            "location": "/CPP-Streaming-Component-API/index.html#api-specification", 
            "text": "The figure below presents a high-level component diagram of the C++ Streaming Component API:   The API consists of a  Detection Component Interface  and related input and output structures.  Detection Component Interface   MPFStreamingDetectionComponent  - Abstract class that should be extended by all OpenMPF C++ detection components that perform stream processing.   Inputs  The following data structures contain details about a specific job, and a video segment (work unit) associated with that job:   MPFStreamingVideoJob  VideoSegmentInfo   Outputs  The following data structures define detection results:   MPFImageLocation  MPFVideoTrack", 
            "title": "API Specification"
        }, 
        {
            "location": "/CPP-Streaming-Component-API/index.html#component-factory-functions", 
            "text": "Every detection component must include the following macro in its implementation:  EXPORT_MPF_STREAMING_COMPONENT(TYPENAME);  This creator macro takes the  TYPENAME  of the detection component (for example, \u201cStreamingHelloWorld\u201d). This macro creates the factory function that the OpenMPF Component Executable will call in order to instantiate the detection component. The creation function is called once, to obtain an instance of the component, after the component library has been loaded into memory.  This macro also creates the factory function that the Component Executable will use to delete that instance of the detection component.  This macro must be used outside of a class declaration, preferably at the bottom or top of a component source (.cpp) file.  Example:  // Note: Do not put the TypeName/Class Name in quotes\nEXPORT_MPF_STREAMING_COMPONENT(StreamingHelloWorld);", 
            "title": "Component Factory Functions"
        }, 
        {
            "location": "/CPP-Streaming-Component-API/index.html#detection-component-interface", 
            "text": "The  MPFStreamingDetectionComponent  class is the abstract class utilized by all OpenMPF C++ detection components that perform stream processing. This class provides functions for developers to integrate detection logic into OpenMPF.  See the latest source here.", 
            "title": "Detection Component Interface"
        }, 
        {
            "location": "/CPP-Streaming-Component-API/index.html#constructor", 
            "text": "Superclass constructor that must be invoked by the constructor of the component subclass.   Function Definition:   MPFStreamingDetectionComponent(const MPFStreamingVideoJob  job)   Parameters:      Parameter  Data Type  Description      job  const MPFStreamingVideoJob   Structure containing details about the work to be performed. See  MPFStreamingVideoJob       Returns: none    Example:    SampleComponent::SampleComponent(const MPFStreamingVideoJob  job)\n        : MPFStreamingDetectionComponent(job)\n        , hw_logger_(GetLogger(job.run_directory))\n        , job_name_(job.job_name) {\n\n    LOG4CXX_INFO(hw_logger_,  [    job_name_    ] Initialized SampleComponent component. )\n}", 
            "title": "Constructor"
        }, 
        {
            "location": "/CPP-Streaming-Component-API/index.html#getdetectiontype", 
            "text": "Returns the type of object detected by the component.   Function Definition:   string GetDetectionType()    Parameters: none    Returns: ( string ) The type of object detected by the component. Should be in all CAPS. Examples include:  FACE ,  MOTION ,  PERSON ,  CLASS  (for object classification), or  TEXT .    Example:    string SampleComponent::GetDetectionType() {\n    return  FACE ;\n}", 
            "title": "GetDetectionType()"
        }, 
        {
            "location": "/CPP-Streaming-Component-API/index.html#beginsegmentvideosegmentinfo", 
            "text": "Indicate the beginning of a new video segment. The next call to  ProcessFrame()  will be the first frame of the new segment.  ProcessFrame()  will never be called before this function.   Function Definition:   void BeginSegment(const VideoSegmentInfo  segment_info)   Parameters:      Parameter  Data Type  Description      segment_info  const VideoSegmentInfo   Structure containing details about next video segment to process. See  VideoSegmentInfo       Returns: none    Example:    void SampleComponent::BeginSegment(const VideoSegmentInfo  segment_info) {\n    // Prepare for next segment\n}", 
            "title": "BeginSegment(VideoSegmentInfo)"
        }, 
        {
            "location": "/CPP-Streaming-Component-API/index.html#processframemat", 
            "text": "Process a single video frame for the current segment.  Must return true when the component begins generating the first track for the current segment. After it returns true, the Component Executable will ignore the return value until the component begins processing the next segment.  If the  job_properties  map contained in the  MPFStreamingVideoJob  struct passed to the component constructor contains a CONFIDENCE_THRESHOLD entry, then this function should only return true for a detection with a confidence value that meets or exceeds that threshold. After the Component Executable invokes  EndSegment()  to retrieve the segment tracks, it will discard detections that are below the threshold. If all the detections in a track are below the threshold, then the entire track will be discarded.  Note that this function may not be invoked for every frame in the current segment. For example, if FRAME_INTERVAL = 2, then this function will only be invoked for every other frame since those are the only ones that need to be processed.  Also, it may not be invoked for the first nor last frame in the segment. For example, if FRAME_INTERVAL = 3 and the segment size is 10, then it will be invoked for frames {0, 3, 6, 9} for the first segment, and frames {12, 15, 18} for the second segment.   Function Definition:      bool ProcessFrame(const cv::Mat  frame, int frame_number)   Parameters:      Parameter  Data Type  Description      frame  const cv::Mat   OpenCV class containing frame data. See  cv::Mat    frame_number  int  A unique frame number (0-based index). Guaranteed to be greater than the frame number passed to the last invocation of this function.       Returns: ( bool ) True when the component begins generating the first track for the current segment; false otherwise.    Example:    bool SampleComponent::ProcessFrame(const cv::Mat  frame, int frame_number) {\n    // Look for detections. Generate tracks and store them until the end of the segment.\n    if (started_first_track_in_current_segment) {\n        return true;        \n    } else {\n        return false;\n    }\n}", 
            "title": "ProcessFrame(Mat ...)"
        }, 
        {
            "location": "/CPP-Streaming-Component-API/index.html#endsegment", 
            "text": "Indicate the end of the current video segment. This will always be called after  BeginSegment() . Generally,  ProcessFrame()  will be called one or more times before this function, depending on the number of frames in the segment and the number of frames actually read from the stream.  Note that the next time  BeginSegment()  is called, this component should start generating new tracks. Each time  EndSegment()  is called, it should return only the most recent track data for that segment. Tracks should not be carried over between segments. Do not append new detections to a preexisting track from the previous segment and return that cumulative track when this function is called.   Function Definition:   vector MPFVideoTrack  EndSegment()    Parameters: none    Returns: ( vector MPFVideoTrack ) The  MPFVideoTrack  data for each detected object.    Example:    vector MPFVideoTrack  SampleComponent::EndSegment() {\n    // Perform any necessary cleanup before processing the next segment.\n    // Return the collection of tracks generated for this segment only.\n}", 
            "title": "EndSegment()"
        }, 
        {
            "location": "/CPP-Streaming-Component-API/index.html#detection-job-data-structures", 
            "text": "The following data structures contain details about a specific job, and a video segment (work unit) associated with that job:   MPFStreamingVideoJob  VideoSegmentInfo   The following data structures define detection results:   MPFImageLocation  MPFVideoTrack", 
            "title": "Detection Job Data Structures"
        }, 
        {
            "location": "/CPP-Streaming-Component-API/index.html#mpfstreamingvideojob", 
            "text": "Structure containing information about a job to be performed on a video stream.   Constructor(s):   MPFStreamingVideoJob(\n  const string  job_name,\n  const string  run_directory,\n  const Properties  job_properties,\n  const Properties  media_properties)\n}   Members:      Member  Data Type  Description      job_name  const string    A specific name given to the job by the OpenMPF framework. This value may be used, for example, for logging and debugging purposes.    run_directory   const string    Contains the full path of the parent folder above where the component is installed. This parent folder is also known as the plugin folder.    job_properties   const Properties   Contains a map of  string, string  which represents the property name and the property value. The key corresponds to the property name specified in the component descriptor file described in  Packaging and Registering a Component . Values are determined when creating a pipeline or when submitting a job.   Note: The job_properties map may not contain the full set of job properties. For properties not contained in the map, the component must use a default value.    media_properties   const Properties   Contains a map of  string, string  of metadata about the media associated with the job. The entries in the map vary depending on the type of media. Refer to the type-specific job structures below.", 
            "title": "MPFStreamingVideoJob"
        }, 
        {
            "location": "/CPP-Streaming-Component-API/index.html#videosegmentinfo", 
            "text": "Structure containing information about a segment of a video stream to be processed. A segment is a subset of contiguous video frames.   Constructor(s):   VideoSegmentInfo(\n  int segment_number,\n  int start_frame,\n  int end_frame,\n  int frame_width,\n  int frame_height\n}   Members:      Member  Data Type  Description      segment_number  int  A unique segment number (0-based index).    start_frame  int  The frame number (0-based index) corresponding to the first frame in this segment.    end_frame  int  The frame number (0-based index) corresponding to the last frame in this segment.    frame_width  int  The height of each frame in this segment.    frame_height  int  The width of each frame in this segment.", 
            "title": "VideoSegmentInfo"
        }, 
        {
            "location": "/CPP-Streaming-Component-API/index.html#detection-job-result-classes", 
            "text": "", 
            "title": "Detection Job Result Classes"
        }, 
        {
            "location": "/CPP-Streaming-Component-API/index.html#mpfimagelocation", 
            "text": "Structure used to store the location of detected objects in a single video frame (image).   Constructor(s):   MPFImageLocation()\nMPFImageLocation(\n  int x_left_upper,\n  int y_left_upper,\n  int width,\n  int height,\n  float confidence = -1,\n  const Properties  detection_properties = {})   Members:      Member  Data Type  Description      x_left_upper  int  Upper left X coordinate of the detected object.    y_left_upper  int  Upper left Y coordinate of the detected object.    width  int  The width of the detected object.    height  int  The height of the detected object.    confidence  float  Represents the \"quality\" of the detection. The range depends on the detection algorithm. 0.0 is lowest quality. Higher values are higher quality. Using a standard range of [0.0 - 1.0] is advised. If the component is unable to supply a confidence value, it should return -1.0.    detection_properties  Properties   Optional additional information about the detected object. There is no restriction on the keys or the number of entries that can be added to the detection_properties map. For best practice, keys should be in all CAPS.      Example:   A component that performs generic object classification can add an entry to  detection_properties  where the key is  CLASSIFICATION  and the value is the type of object detected.  MPFImageLocation detection;\ndetection.x_left_upper = 0;\ndetection.y_left_upper = 0;\ndetection.width = 100;\ndetection.height = 100;\ndetection.confidence = 1.0;\ndetection.detection_properties[ CLASSIFICATION ] =  backpack ;", 
            "title": "MPFImageLocation"
        }, 
        {
            "location": "/CPP-Streaming-Component-API/index.html#mpfvideotrack", 
            "text": "Structure used to store the location of detected objects in a video file.   Constructor(s):   MPFVideoTrack()\nMPFVideoTrack(\n  int start_frame,\n  int stop_frame,\n  float confidence = -1,\n  map int, MPFImageLocation  frame_locations,\n  const Properties  detection_properties = {})   Members:      Member  Data Type  Description      start_frame  int  The first frame number (0-based index) that contained the detected object.    stop_frame  int  The last frame number (0-based index) that contained the detected object.    frame_locations  map int, MPFImageLocation  A map of individual detections. The key for each map entry is the frame number where the detection was generated, and the value is a  MPFImageLocation  calculated as if that frame was a still image. Note that a key-value pair is  not  required for every frame between the track start frame and track stop frame.    confidence  float  Represents the \"quality\" of the detection. The range depends on the detection algorithm. 0.0 is lowest quality. Higher values are higher quality. Using a standard range of [0.0 - 1.0] is advised. If the component is unable to supply a confidence value, it should return -1.0.    detection_properties  Properties   Optional additional information about the detected object. There is no restriction on the keys or the number of entries that can be added to the detection_properties map. For best practice, keys should be in all CAPS.      Example:    NOTE:  Currently,  MPFVideoTrack.detection_properties  do not show up in the JSON output object or are used by the WFM in any way.   A component that detects text can add an entry to  detection_properties  where the key is  TRANSCRIPT  and the value is a string representing the text found in the video segment.  MPFVideoTrack track;\ntrack.start_frame = 0;\ntrack.stop_frame = 5;\ntrack.confidence = 1.0;\ntrack.frame_locations = frame_locations;\ntrack.detection_properties[ TRANSCRIPT ] =  RE5ULTS FR0M A TEXT DETECTER ;", 
            "title": "MPFVideoTrack"
        }, 
        {
            "location": "/CPP-Streaming-Component-API/index.html#c-component-build-environment", 
            "text": "A C++ component library must be built for the same C++ compiler and Linux version that is used by the OpenMPF Component Executable. This is to ensure compatibility between the executable and the library functions at the Application Binary Interface (ABI) level. At this writing, the OpenMPF runs on CentOS 7.4.1708 (kernel version 3.10.0-693), and the OpenMPF C++ Component Executable is built with g++ (GCC) 4.8.5 20150623 (Red Hat 4.8.5-16).  Components should be supplied as a tar file, which includes not only the component library, but any other libraries or files needed for execution. This includes all other non-standard libraries used by the component (aside from the standard Linux and C++ libraries), and any configuration or data files.", 
            "title": "C++ Component Build Environment"
        }, 
        {
            "location": "/CPP-Streaming-Component-API/index.html#component-development-best-practices", 
            "text": "", 
            "title": "Component Development Best Practices"
        }, 
        {
            "location": "/CPP-Streaming-Component-API/index.html#throw-exceptions", 
            "text": "Unlike the  C++ Batch Component API , none of the the C++ Streaming Component API functions return an  MPFDetectionError . Instead, streaming components should throw an exception when a non-recoverable error occurs. The exception should be an instantiation or subclass of  std::exception  and provide a descriptive error message that can be retrieved using  what() . For example:  bool SampleComponent::ProcessFrame(const cv::Mat  frame, int frame_number) {\n    // Something bad happened\n    throw std::exception( Error: Cannot do X with value Y. );\n}  The exception will be handled by the Component Executable. It will immediately invoke  EndSegment()  to retrieve the current tracks. Then the component process and streaming job will be terminated.", 
            "title": "Throw Exceptions"
        }, 
        {
            "location": "/CPP-Streaming-Component-API/index.html#single-threaded-operation", 
            "text": "Implementations are encouraged to operate in single-threaded mode. OpenMPF will parallelize components through multiple instantiations of the component, each running as a separate service.", 
            "title": "Single-threaded Operation"
        }, 
        {
            "location": "/CPP-Streaming-Component-API/index.html#stateless-behavior", 
            "text": "OpenMPF components should be stateless in operation and give identical output for a provided input (i.e. when processing a segment with the same  VideoSegmentInfo ).", 
            "title": "Stateless Behavior"
        }, 
        {
            "location": "/CPP-Streaming-Component-API/index.html#gpu-support", 
            "text": "For components that want to take advantage of NVIDA GPU processors, please read the  GPU Support Guide . Also ensure that your build environment has the NVIDIA CUDA Toolkit installed, as described in the  Build Environment Setup Guide .", 
            "title": "GPU Support"
        }, 
        {
            "location": "/CPP-Streaming-Component-API/index.html#component-packaging", 
            "text": "It is recommended that C++ components are organized according to the following directory structure:  componentName\n\u251c\u2500\u2500 config - Logging and other component-specific configuration\n\u251c\u2500\u2500 descriptor\n\u2502   \u2514\u2500\u2500 descriptor.json\n\u2514\u2500\u2500 lib\n    \u2514\u2500\u2500libComponentName.so - Compiled component library  Once built, components should be packaged into a .tar.gz containing the contents of the directory shown above.", 
            "title": "Component Packaging"
        }, 
        {
            "location": "/CPP-Streaming-Component-API/index.html#logging", 
            "text": "It is recommended to use  Apache log4cxx  for OpenMPF Component logging.  Note that multiple instances of the same component can log to the same file. Also, logging content can span multiple lines.  Log files should be output to: ${MPF_LOG_PATH}/${THIS_MPF_NODE}/log/ componentName .log  Each log statement must take the form: DATE TIME LEVEL CONTENT  The following log LEVELs are supported: FATAL, ERROR, WARN,  INFO,  DEBUG, TRACE .  For example: 2016-02-09 13:42:42,341 INFO - Starting sample-component: [  OK  ]  The following configuration can be used to match the format of other OpenMPF logs:  log4j:configuration xmlns:log4j= http://jakarta.apache.org/log4j/ \n\n   !-- Output the log message to log file-- \n   appender name= SAMPLECOMPONENT-FILE  class= org.apache.log4j.DailyRollingFileAppender \n     param name= file  value= ${MPF_LOG_PATH}/${THIS_MPF_NODE}/log/ componentName .log  / \n     param name= DatePattern  value= '.'yyyy-MM-dd  / \n     layout class= org.apache.log4j.PatternLayout \n       param name= ConversionPattern  value= %d %p [%t] %c{36}:%L - %m%n  / \n     /layout \n   /appender \n\n   logger name=  SampleComponent  additivity= false \n     level value= INFO / \n     appender-ref ref= SAMPLECOMPONENT-FILE / \n   /logger  /log4j:configuration", 
            "title": "Logging"
        }, 
        {
            "location": "/Java-Batch-Component-API/index.html", 
            "text": "NOTICE:\n This software (or technical data) was produced for the U.S. Government under contract, and is subject to the Rights in Data-General Clause 52.227-14, Alt. IV (DEC 2007). Copyright 2019 The MITRE Corporation. All Rights Reserved.\n\n\n\n\nAPI Overview\n\n\nIn OpenMPF, a \ncomponent\n is a plugin that receives jobs (containing media), processes that  media, and returns results.\n\n\nThe OpenMPF Batch Component API currently supports the development of \ndetection components\n, which are used detect objects in image, video, audio, or other (generic) files that reside on disk.\n\n\nUsing this API, detection components can be built to provide:\n\n\n\n\nDetection (Localizing an object)\n\n\nTracking (Localizing an object across multiple frames)\n\n\nClassification (Detecting the type of object and optionally localizing that object)\n\n\nTranscription (Detecting speech and transcribing it into text)\n\n\n\n\nHow Components Integrate into OpenMPF\n\n\nComponents are integrated into OpenMPF through the use of OpenMPF's \nComponent Executor\n. Developers create component libraries that encapsulate the component detection logic. Each instance of the Component Executor loads one of these libraries and uses it to service job requests sent by the OpenMPF Workflow Manager (WFM).\n\n\nThe Component Executor:\n\n\n\n\nReceives and parses job requests from the WFM\n\n\nInvokes methods on the component library to obtain detection results\n\n\nPopulates and sends the respective responses to the WFM\n\n\n\n\nThe basic psuedocode for the Component Executor is as follows:\n\n\ncomponent.setRunDirectory(...)\ncomponent.init()\nwhile (true) {\n    job = ReceiveJob()\n    if (component.supports(job.dataType))\n        component.getDetections(...) // Component does the work here\n    }\ncomponent.close()\n\n\n\n\nEach instance of a Component Executor runs as a separate process.\n\n\nThe Component Executor receives and parses requests from the WFM, invokes methods on the Component Logic to get detection objects, and subsequently populates responses with the component output and sends them to the WFM.\n\n\nA component developer implements a detection component by extending \nMPFDetectionComponentBase\n.\n\n\nAs an alternative to extending \nMPFDetectionComponentBase\n directly, a developer may extend one of several convenience adapter classes provided by OpenMPF. See \nConvenience Adapters\n for more information.\n\n\nGetting Started\n\n\nThe quickest way to get started with the Java Batch Component API is to first read the \nOpenMPF Component API Overview\n and then \nreview the source\n for example OpenMPF Java detection components.\n\n\nDetection components are implemented by:\n\n\n\n\nExtending \nMPFDetectionComponentBase\n.\n\n\nBuilding the component into a jar. (See \nHelloWorldComponent pom.xml\n).\n\n\nPackaging the component into an OpenMPF-compliant .tar.gz file. (See \nComponent Packaging\n).\n\n\nRegistering the component with OpenMPF. (See \nPackaging and Registering a Component\n).\n\n\n\n\nAPI Specification\n\n\nThe figure below presents a high-level component diagram of the Java Batch Component API:\n\n\n\n\nThe API consists of \nComponent Interfaces\n, which provide interfaces and abstract classes for developing components; \nJob Definitions\n, which define the work to be performed by a component; \nJob Results\n, which define the results generated by the component; and \nComponent Adapters\n, which provide default implementations of several of the \nMPFDetectionComponentInterface\n methods (See the \nMPFAudioAndVideoDetectionComponentAdapter\n for an example; \nTODO: implement those shown in the diagram\n). In the future, the API will also include \nComponent Utilities\n, which perform actions such as image flipping, rotation, and cropping.\n\n\nComponent Interfaces\n\n\n\n\nMPFComponentInterface\n - Interface for all Java components that perform batch processing.\n\n\nMPFComponentBase\n - An abstract baseline for components. Provides default implementations for \nMPFComponentInterface\n.\n\n\n\n\nDetection Component Interfaces\n\n\n\n\nMPFDetectionComponentInterface\n - Baseline interface for detection components.\n\n\nMPFDetectionComponentBase\n - An abstract baseline for detection components. Provides default implementations for  \nMPFDetectionComponentInterface\n.\n\n\n\n\nJob Definitions\n\n\nThe following classes define the details about a specific job (work unit):\n\n\n\n\nMPFImageJob\n extends \nMPFJob\n\n\nMPFVideoJob\n extends \nMPFJob\n\n\nMPFAudioJob\n extends \nMPFJob\n\n\nMPFGenericJob\n extends \nMPFJob\n\n\n\n\nJob Results\n\n\nThe following classes define detection results:\n\n\n\n\nMPFImageLocation\n\n\nMPFVideoTrack\n\n\nMPFAudioTrack\n\n\nMPFGenericTrack\n\n\n\n\nComponent Interface\n\n\nThe OpenMPF Component class structure consists of:\n\n\n\n\nMPFComponentInterface\n - Interface for all OpenMPF Java components that perform batch processing.\n\n\nMPFComponentBase\n - An abstract baseline for components. Provides default implementations for \nMPFComponentInterface\n.\n\n\n\n\n\n\nIMPORTANT:\n This interface and abstract class should not be directly implemented because no mechanism exists for launching components based off of it. Instead, it defines the contract that components must follow. Currently, the only supported type of batch component is \"DETECTION\". Those components should extend \nMPFDetectionComponentBase\n\n\n\n\nSee the latest source here.\n\n\nsetRunDirectory(String)\n\n\nSets the value to the full path of the parent folder above where the component is installed.\n\n\n\n\nMethod Definition:\n\n\n\n\npublic void setRunDirectory(String runDirectory);\n\n\n\n\n\n\nParameters:\n\n\n\n\n\n\n\n\n\n\nParameter\n\n\nData Type\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nrunDirectory\n\n\nString\n\n\nFull path of the parent folder above where the component is installed.\n\n\n\n\n\n\n\n\n\n\nReturns: none\n\n\n\n\n\n\nIMPORTANT:\n \nsetRunDirectory\n is called by the Component Executor to set the correct path. It is not necessary to call this method in your component implementation.\n\n\n\n\ngetRunDirectory()\n\n\nReturns the full path of the parent folder above where the component is installed.\n\n\n\n\nMethod Definition:\n\n\n\n\npublic String getRunDirectory()\n\n\n\n\n\n\n\n\nParameters: none\n\n\n\n\n\n\nReturns: (\nString\n) Full path of the parent folder above where the component is installed.\n\n\n\n\n\n\ninit()\n\n\nPerforms any necessary startup tasks for the component. This will be executed once by the Component Executor, on component startup, before the first job, after \nsetRunDirectory\n.\n\n\n\n\nMethod Definition:\n\n\n\n\npublic void init()\n\n\n\n\n\n\n\n\nParameters: none\n\n\n\n\n\n\nReturns: none\n\n\n\n\n\n\nExample:\n\n\n\n\n\n\npublic void init() {\n    // Setup logger, Load data models, etc.\n}\n\n\n\n\nclose()\n\n\nPerforms any necessary shutdown tasks for the component. This will be executed once by the Component Executor, on component shutdown, usually after the last job.\n\n\n\n\nMethod Definition:\n\n\n\n\npublic void close()\n\n\n\n\n\n\n\n\nParameters: none\n\n\n\n\n\n\nReturns: none\n\n\n\n\n\n\nExample:\n\n\n\n\n\n\npublic void close() {\n    // Close file handlers, etc.\n}\n\n\n\n\ngetComponentType()\n\n\nAllows the Component API to determine the component \"type.\" Currently \nDETECTION\n is the only supported component type.\n\n\n\n\nMethod Definition:\n\n\n\n\npublic MPFComponentType getComponentType()\n\n\n\n\n\n\n\n\nParameters: none\n\n\n\n\n\n\nReturns: (\nMPFComponentType\n) Currently, \nDETECTION\n is the only supported return value.\n\n\n\n\n\n\nExample:\n\n\n\n\n\n\npublic MPFComponentType getComponentType() {\n    return MPFComponentType.DETECTION;\n}\n\n\n\n\nDetection Component Interface\n\n\nThe \nMPFDetectionComponentInterface\n must be utilized by all OpenMPF Java detection components that perform batch processing.\n\n\nEvery batch detection component must define a \ncomponent\n class which implements the MPFComponentInterface. This is typically performed by extending \nMPFDetectionComponentBase\n, which extends \nMPFComponentBase\n and implements \nMPFDetectionComponentInterface\n.\n\n\nTo designate the component class, every batch detection component should include an applicationContext.xml which defines the \ncomponent\n bean.  The \ncomponent\n bean class must implement \nMPFDetectionComponentInterface\n.\n\n\n\n\nIMPORTANT:\n Each batch detection component must implement all of the \ngetDetections()\n methods or extend from a superclass which provides implementations for them (see \nconvenience adapters\n).\n\n\nIf your component does not support a particular data type, it should simply:\n\n\nthrow new MPFComponentDetectionError(MPFDetectionError.MPF_UNSUPPORTED_DATA_TYPE);\n\n\n\n\nConvenience Adapters\n\n\nAs an alternative to extending \nMPFDetectionComponentBase\n directly, developers may extend a convenience adapter classes provided by OpenMPF.\n\n\nThese adapters provide default implementations of several methods in \nMPFDetectionComponentInterface\n and ensure that the component's logic properly extends from the Component API. This enables developers to concentrate on implementation of the detection algorithm.\n\n\nThe following adapter is provided:\n\n\n\n\nAudio And Video Detection Component Adapter (\nsource\n)\n\n\n\n\n\n\nExample: Using Adaptors to Provide Simple AudioVisual Handling:\n\nMany components designed to work on audio files, such as speech detection, are relevant to video files as well.  Some of the tools for these components, however, only function on audio files (such as .wav, .mp3) and not video files (.avi, .mov, etc).\n\n\nThe \nMPFAudioAndVideoDetectionComponentAdapter\n adapter class implements the \ngetDetections(MPFVideoJob)\n method by translating the video request into an audio request.  It builds a temporary audio file by ripping the audio from the video media input, translates the \nMPFVideoJob\n into an \nMPFAudioJob\n, and invokes \ngetDetections(MPFAudioJob)\n on the generated file.  Once processing is done, the adapter translates the \nMPFAudioTrack\n list into an \nMPFVideoTrack\n list.\n\n\nSince only audio and video files are relevant to this adapter, it provides a default implementation of the \ngetDetections(MPFImageJob)\n method which throws \nnew MPFComponentDetectionError(MPFDetectionError.MPF_UNSUPPORTED_DATA_TYPE)\n.\n\n\nThe Sphinx speech detection component uses this adapter to run Sphinx speech detection on video files.  Other components that need to process video files as audio may also use the adapter.\n\n\n\n\nsupports(MPFDataType)\n\n\nReturns the supported data types of the component.\n\n\n\n\nMethod Definition:\n\n\n\n\npublic boolean supports(MPFDataType dataType)\n\n\n\n\n\n\nParameters:\n\n\n\n\n\n\n\n\n\n\nParameter\n\n\nData Type\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\ndataType\n\n\nMPFDataType\n\n\nReturn true if the component supports IMAGE, VIDEO, AUDIO, and/or UNKNOWN (generic) processing.\n\n\n\n\n\n\n\n\n\n\n\n\nReturns: (\nboolean\n) True if the component supports the data type, otherwise false.\n\n\n\n\n\n\nExample:\n\n\n\n\n\n\n// Sample Component that supports only image and video files\npublic boolean supports(MPFDataType dataType) {\n    return dataType == MPFDataType.IMAGE || dataType == MPFDataType.VIDEO;\n}\n\n\n\n\ngetDetectionType()\n\n\nReturns the type of object detected by the component.\n\n\n\n\nMethod Definition:\n\n\n\n\npublic String getDetectionType()\n\n\n\n\n\n\n\n\nParameters: none\n\n\n\n\n\n\nReturns: (\nString\n) The type of object detected by the component. Should be in all CAPS. Examples include: \nFACE\n, \nMOTION\n, \nPERSON\n, \nSPEECH\n, \nCLASS\n (for object classification), or \nTEXT\n.\n\n\n\n\n\n\nExample:\n\n\n\n\n\n\npublic String getDetectionType() {\n    return \nFACE\n;\n}\n\n\n\n\ngetDetections(MPFImageJob)\n\n\nUsed to detect objects in image files. The MPFImageJob class contains the URI specifying the location of the image file.\n\n\nCurrently, the dataUri is always a local file path. For example, \"/opt/mpf/share/remote-media/test-file.jpg\". This is because all media is copied to the OpenMPF server before the job is executed.\n\n\n\n\nMethod Definition:\n\n\n\n\npublic List\nMPFImageLocation\n getDetections(MPFImageJob job)\n  throws MPFComponentDetectionError;\n\n\n\n\n\n\nParameters:\n\n\n\n\n\n\n\n\n\n\nParameter\n\n\nData Type\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\njob\n\n\nMPFImageJob\n\n\nClass containing details about the work to be performed. See \nMPFImageJob\n\n\n\n\n\n\n\n\n\n\n\n\nReturns: (\nList\nMPFImageLocation\n) The \nMPFImageLocation\n data for each detected object.\n\n\n\n\n\n\nExample:\n\n\n\n\n\n\npublic List\nMPFImageLocation\n getDetections(MPFImageJob job)\n  throws MPFComponentDetectionError {\n    // Component logic to generate image locations\n}\n\n\n\n\ngetDetections(MPFVideoJob)\n\n\nUsed to detect objects in a video.\n\n\nPrior to being sent to the component, videos are split into logical \"segments\" of video data and each segment (containing a range of frames) is assigned to a different job. Components are not guaranteed to receive requests in any order. For example, the first request processed by a component might receive a request for frames 300-399 of a Video A, while the next request may cover frames 900-999 of a Video B.\n\n\n\n\nMethod Definition:\n\n\n\n\npublic List\nMPFVideoTrack\n getDetections(MPFVideoJob job)\n  throws MPFComponentDetectionError;\n\n\n\n\n\n\nParameters:\n\n\n\n\n\n\n\n\n\n\nParameter\n\n\nData Type\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\njob\n\n\nMPFVideoJob\n\n\nClass containing details about the work to be performed. See \nMPFVideoJob\n\n\n\n\n\n\n\n\n\n\n\n\nReturns: (\nList\nMPFVideoTrack\n) The \nMPFVideoTrack\n data for each detected object.\n\n\n\n\n\n\nExample:\n\n\n\n\n\n\npublic List\nMPFVideoTrack\n getDetections(MPFVideoJob job)\n  throws MPFComponentDetectionError {\n    // Component logic to generate video tracks\n}\n\n\n\n\ngetDetections(MPFAudioJob)\n\n\nUsed to detect objects in audio files. Currently, audio files are not logically segmented, so a job will contain the entirety of the audio file.\n\n\n\n\nMethod Definition:\n\n\n\n\npublic List\nMPFAudioTrack\n getDetections(MPFAudioJob job)\n  throws MPFComponentDetectionError;\n\n\n\n\n\n\nParameters:\n\n\n\n\n\n\n\n\n\n\nParameter\n\n\nData Type\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\njob\n\n\nMPFAudioJob\n\n\nClass containing details about the work to be performed. See \nMPFAudioJob\n\n\n\n\n\n\n\n\n\n\n\n\nReturns: (\nList\nMPFAudioTrack\n) The \nMPFAudioTrack\n data for each detected object.\n\n\n\n\n\n\nExample:\n\n\n\n\n\n\npublic List\nMPFAudioTrack\n getDetections(MPFAudioJob job)\n  throws MPFComponentDetectionError {\n    // Component logic to generate audio tracks\n}\n\n\n\n\ngetDetections(MPFGenericJob)\n\n\nUsed to detect objects in files that aren't video, image, or audio files. Such files are of the UNKNOWN type and handled generically. These files are not logically segmented, so a job will contain the entirety of the file.\n\n\n\n\nMethod Definition:\n\n\n\n\npublic List\nMPFGenericTrack\n getDetections(MPFGenericJob job)\n  throws MPFComponentDetectionError;\n\n\n\n\n\n\nParameters:\n\n\n\n\n\n\n\n\n\n\nParameter\n\n\nData Type\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\njob\n\n\nMPFGenericJob\n\n\nClass containing details about the work to be performed. See \nMPFGenericJob\n\n\n\n\n\n\n\n\n\n\n\n\nReturns: (\nList\nMPFGenericTrack\n) The \nMPFGenericTrack\n data for each detected object.\n\n\n\n\n\n\nExample:\n\n\n\n\n\n\npublic List\nMPFGenericTrack\n getDetections(MPFGenericJob job)\n  throws MPFComponentDetectionError {\n    // Component logic to generate generic tracks\n}\n\n\n\n\nMPFComponentDetectionError\n\n\nAn exception that occurs in a component.  The exception must contain a reference to a valid \nMPFDetectionError\n.\n\n\n\n\nConstructor(s):\n\n\n\n\npublic MPFComponentDetectionError (\n  MPFDetectionError error,\n  String msg,\n  Exception e\n)\n\n\n\n\n\n\nParameters:\n\n\n\n\n\n\n\n\n\n\nParameter\n\n\nData Type\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nerror\n\n\nMPFDetectionError\n\n\nThe type of error generated by the component. See \nMPFDetectionError\n.\n\n\n\n\n\n\nmsg\n\n\nString\n\n\nThe detail message (which is saved for later retrieval by the \nThrowable.getMessage()\n method).\n\n\n\n\n\n\ne\n\n\nException\n\n\nThe cause (which is saved for later retrieval by the \nThrowable.getCause()\n method). A null value is permitted.\n\n\n\n\n\n\n\n\nDetection Job Classes\n\n\nThe following classes contain details about a specific job (work unit):\n\n\n\n\nMPFImageJob\n extends \nMPFJob\n\n\nMPFVideoJob\n extends \nMPFJob\n\n\nMPFAudioJob\n extends \nMPFJob\n\n\nMPFGenericJob\n extends \nMPFJob\n\n\n\n\nThe following classes define detection results:\n\n\n\n\nMPFImageLocation\n\n\nMPFVideoTrack\n\n\nMPFAudioTrack\n\n\nMPFGenericTrack\n\n\n\n\nMPFJob\n\n\nClass containing data used for detection of objects.\n\n\n\n\nConstructor(s):\n\n\n\n\nprotected MPFJob(\n  String jobName,\n  String dataUri,\n  final Map\nString, String\n jobProperties,\n  final Map\nString, String\n mediaProperties\n)\n\n\n\n\n\n\nMembers:\n\n\n\n\n\n\n\n\n\n\nMember\n\n\nData Type\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\njobName \n\n\nString\n\n\nA specific name given to the job by the OpenMPF Framework. This value may be used, for example, for logging and debugging purposes.\n\n\n\n\n\n\ndataUri \n\n\nString\n\n\nThe URI of the input media file to be processed. Currently, this is a file path. For example, \"/opt/mpf/share/remote-media/test-file.avi\".\n\n\n\n\n\n\njobProperties \n\n\nMap\nString, String\n\n\nThe key corresponds to the property name specified in the component descriptor file described in \"Installing and Registering a Component\". Values are determined by an end user when creating a pipeline. \n Note: Only those property values specified by the user will be in the jobProperties map; for properties not contained in the map, the component must use a default value.\n\n\n\n\n\n\nmediaProperties \n\n\nMap\nString, String\n\n\nMetadata about the media associated with the job. The key is the property name and value is the property value. The entries in the map vary depend on the job type. They are defined in the specific Job's API description.\n\n\n\n\n\n\n\n\nMPFImageJob\n\n\nExtends \nMPFJob\n\n\nClass containing data used for detection of objects in image files.\n\n\n\n\nConstructor(s):\n\n\n\n\npublic MPFImageJob(\n  String jobName,\n  String dataUri,\n  final Map\nString, String\n jobProperties,\n  final Map \nString, String\n mediaProperties)\n\n\n\n\npublic MPFImageJob(\n  String jobName,\n  String dataUri,\n  final Map\nString, String\n jobProperties,\n  final Map \nString, String\n mediaProperties,\n  MPFImageLocation location)\n\n\n\n\n\n\nMembers:\n\n\n\n\n\n  \n\n    \n\n      \nMember\n\n      \nData Type\n\n      \nDescription\n\n    \n\n  \n\n  \n\n    \n\n      \njobName\n\n      \nString\n\n      \nSee \nMPFJob.jobName\n for description.\n\n    \n\n    \n\n      \ndataUri\n\n      \nString\n\n      \nSee \nMPFJob.dataUri\n for description.\n\n    \n\n    \n\n      \njobProperties\n\n      \nMap\n, String\n\n      \nSee \nMPFJob.jobProperties\n for description.\n\n    \n\n    \n\n      \nmediaProperties\n\n      \nMap\n, String\n\n      \n\n        See \nMPFJob.mediaProperties\n for description.\n        \n\n        Includes the following key-value pairs:\n        \n\n          \nMIME_TYPE\n : the MIME type of the media\n\n          \nFRAME_WIDTH\n : the width of the image in pixels\n\n          \nFRAME_HEIGHT\n : the height of the image in pixels\n\n        \n\n        May include the following key-value pairs:\n        \n\n          \nROTATION\n : A floating point value in the interval \n[0.0, 360.0)\n indicating the orientation of the media in degrees in the counter-clockwise direction. In order to view the media in the upright orientation, it must be rotated the given number of degrees in the clockwise direction.\n\n          \nHORIZONTAL_FLIP\n : true if the image is mirrored across the Y-axis, otherwise false\n\n          \nEXIF_ORIENTATION\n : the standard EXIF orientation tag; a value between 1 and 8\n\n        \n\n      \n\n    \n\n    \n\n      \nlocation\n\n      \nMPFImageLocation\n\n      \nAn \nMPFImageLocation\n from the previous pipeline stage. Provided when feed forward is enabled. See \nFeed Forward Guide\n.\n\n    \n\n  \n\n\n\n\n\nMPFVideoJob\n\n\nExtends \nMPFJob\n\n\nClass containing data used for detection of objects in video files.\n\n\n\n\nConstructor(s):\n\n\n\n\npublic MPFVideoJob(\n  String jobName,\n  String dataUri,\n  final Map\nString, String\n jobProperties,\n  final Map\nString, String\n mediaProperties,\n  int startFrame,\n  int stopFrame)\n\n\n\n\npublic MPFVideoJob(\n  String jobName,\n  String dataUri,\n  final Map\nString, String\n jobProperties,\n  final Map\nString, String\n mediaProperties,\n  int startFrame,\n  int stopFrame,\n  MPFVideoTrack track)\n\n\n\n\n\n\nMembers:\n\n\n\n\n\n  \n\n    \n\n      \nMember\n\n      \nData Type\n\n      \nDescription\n\n    \n\n  \n\n  \n\n    \n\n      \njobName\n\n      \nString\n\n      \nSee \nMPFJob.jobName\n for description.\n\n    \n\n    \n\n      \ndataUri\n\n      \nString\n\n      \nSee \nMPFJob.dataUri\n for description.\n\n    \n\n    \n\n      \nstartFrame\n\n      \nint\n\n      \nThe first frame number (0-based index) of the video that should be processed to look for detections.\n\n    \n\n    \n\n      \nstopFrame\n\n      \nint\n\n      \nThe last frame number (0-based index) of the video that should be processed to look for detections.\n\n    \n        \n    \n\n      \njobProperties\n\n      \nMap\n, String\n\n      \nSee \nMPFJob.jobProperties\n for description.\n\n    \n\n    \n\n      \nmediaProperties\n\n      \nMap\n, String\n\n      \n\n        See \nMPFJob.mediaProperties\n for description.\n        \n\n        Includes the following key-value pairs:\n        \n\n          \nDURATION\n : length of video in milliseconds\n\n          \nFPS\n : frames per second (averaged for variable frame rate video)\n\n          \nFRAME_COUNT\n : the number of frames in the video\n\n          \nMIME_TYPE\n : the MIME type of the media\n\n          \nFRAME_WIDTH\n : the width of a frame in pixels\n\n          \nFRAME_HEIGHT\n : the height of a frame in pixels\n\n        \n\n        May include the following key-value pair:\n        \n\n          \nROTATION\n : A floating point value in the interval \n[0.0, 360.0)\n indicating the orientation of the media in degrees in the counter-clockwise direction. In order to view the media in the upright orientation, it must be rotated the given number of degrees in the clockwise direction.\n\n        \n\n      \n\n    \n\n    \n\n      \ntrack\n\n      \nMPFVideoTrack\n\n      \nAn \nMPFVideoTrack\n from the previous pipeline stage. Provided when feed forward is enabled. See \nFeed Forward Guide\n.\n\n    \n\n  \n\n\n\n\n\n\n\nIMPORTANT:\n \nFRAME_INTERVAL\n is a common job property that many components support. For frame intervals greater than 1, the component must look for detections starting with the first frame, and then skip frames as specified by the frame interval, until or before it reaches the stop frame. For example, given a start frame of 0, a stop frame of 99, and a frame interval of 2, then the detection component must look for objects in frames numbered 0, 2, 4, 6, ..., 98.\n\n\n\n\nMPFAudioJob\n\n\nExtends \nMPFJob\n\n\nClass containing data used for detection of objects in audio files.\n\n\n\n\nConstructor(s):\n\n\n\n\npublic MPFAudioJob(\n  String jobName,\n  String dataUri,\n  final Map\nString, String\n jobProperties,\n  final Map\nString, String\n mediaProperties,\n  int startTime,\n  int stopTime)\n\n\n\n\npublic MPFAudioJob(\n  String jobName,\n  String dataUri,\n  final Map\nString, String\n jobProperties,\n  final Map\nString, String\n mediaProperties,\n  int startTime,\n  int stopTime,\n  MPFAudioTrack track)\n\n\n\n\n\n\nMembers:\n\n\n\n\n\n  \n\n    \n\n      \nMember\n\n      \nData Type\n\n      \nDescription\n\n    \n\n  \n\n  \n\n    \n\n      \njobName\n\n      \nString\n\n      \nSee \nMPFJob.jobName\n for description.\n\n    \n\n    \n\n      \ndataUri\n\n      \nString\n\n      \nSee \nMPFJob.dataUri\n for description.\n\n    \n\n    \n\n      \nstartTime\n\n      \nint\n\n      \nThe time (0-based index, in ms) associated with the beginning of the segment of the audio file that should be processed to look for detections.\n\n    \n\n    \n\n      \nstopTime\n\n      \nint\n\n      \nThe time (0-based index, in ms) associated with the end of the segment of the audio file that should be processed to look for detections.\n\n    \n        \n    \n\n      \njobProperties\n\n      \nMap\n, String\n\n      \nSee \nMPFJob.jobProperties\n for description.\n\n    \n\n    \n\n      \nmediaProperties\n\n      \nMap\n, String\n\n      \n\n        See \nMPFJob.mediaProperties\n for description.\n        \n\n        Includes the following key-value pairs:\n        \n\n          \nDURATION\n : length of audio file in milliseconds\n\n          \nMIME_TYPE\n : the MIME type of the media\n\n        \n\n      \n\n    \n\n    \n\n      \ntrack\n\n      \nMPFAudioTrack\n\n      \nAn \nMPFAudioTrack\n from the previous pipeline stage. Provided when feed forward is enabled. See \nFeed Forward Guide\n.\n\n    \n\n  \n\n\n\n\n\nMPFGenericJob\n\n\nExtends \nMPFJob\n\n\nClass containing data used for detection of objects in a file that isn't a video, image, or audio file. The file is of the UNKNOWN type and handled generically. The file is not logically segmented, so a job will contain the entirety of the file.\n\n\n\n\nConstructor(s):\n\n\n\n\npublic MPGenericJob(\n  String jobName,\n  String dataUri,\n  final Map\nString, String\n jobProperties,\n  final Map\nString, String\n mediaProperties)\n\n\n\n\npublic MPFGenericJob(\n  String jobName,\n  String dataUri,\n  final Map\nString, String\n jobProperties,\n  final Map \nString, String\n mediaProperties,\n  MPFGenericTrack track) {\n\n\n\n\n\n\nMembers:\n\n\n\n\n\n  \n\n    \n\n      \nMember\n\n      \nData Type\n\n      \nDescription\n\n    \n\n  \n\n  \n\n    \n\n      \njobName\n\n      \nString\n\n      \nSee \nMPFJob.jobName\n for description.\n\n    \n\n    \n\n      \ndataUri\n\n      \nString\n\n      \nSee \nMPFJob.dataUri\n for description.\n\n    \n\n    \n\n      \nstartTime\n\n      \nint\n\n      \nThe time (0-based index, in ms) associated with the beginning of the segment of the audio file that should be processed to look for detections.\n\n    \n\n    \n\n      \nstopTime\n\n      \nint\n\n      \nThe time (0-based index, in ms) associated with the end of the segment of the audio file that should be processed to look for detections.\n\n    \n        \n    \n\n      \njobProperties\n\n      \nMap\n, String\n\n      \nSee \nMPFJob.jobProperties\n for description.\n\n    \n\n    \n\n      \nmediaProperties\n\n      \nMap\n, String\n\n      \n\n        See \nMPFJob.mediaProperties\n for description.\n        \n\n        Includes the following key-value pair:\n        \n\n          \nMIME_TYPE\n : the MIME type of the media\n\n        \n\n      \n\n    \n\n    \n\n      \ntrack\n\n      \nMPFGenericTrack\n\n      \nAn \nMPFGenericTrack\n from the previous pipeline stage. Provided when feed forward is enabled. See \nFeed Forward Guide\n.\n\n    \n\n  \n\n\n\n\n\nDetection Job Result Classes\n\n\nMPFImageLocation\n\n\nClass used to store the location of detected objects in an image.\n\n\n\n\nConstructor(s):\n\n\n\n\npublic MPFImageLocation(\n  int xLeftUpper,\n  int yLeftUpper,\n  int width,\n  int height,\n  float confidence,\n  Map\nString, String\n detectionProperties\n)\n\n\n\n\n\n\nMembers:\n\n\n\n\n\n\n\n\n\n\nMember\n\n\nData Type\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nxLeftUpper\n\n\nint\n\n\nUpper left X coordinate of the detected object.\n\n\n\n\n\n\nyLeftUpper\n\n\nint\n\n\nUpper left Y coordinate of the detected object.\n\n\n\n\n\n\nwidth\n\n\nint\n\n\nThe width of the detected object.\n\n\n\n\n\n\nheight\n\n\nint\n\n\nThe height of the detected object.\n\n\n\n\n\n\nconfidence\n\n\nfloat\n\n\nRepresents the \"quality\" of the detection. The range depends on the detection algorithm. 0.0 is lowest quality. Higher values are higher quality. Using a standard range of [0.0 - 1.0] is advised. If the component is unable to supply a confidence value, it should return -1.0.\n\n\n\n\n\n\ndetectionProperties\n\n\nMap\nString, String\n\n\nOptional additional information about the detected object. There is no restriction on the keys or the number of entries that can be added to the properties map. For best practice, keys should be in all CAPS.\n\n\n\n\n\n\n\n\n\n\nExample:\n\n\n\n\nA component that performs generic object classification can add an entry to \ndetection_properties\n where the key is \nCLASSIFICATION\n and the value is the type of object detected.\n\n\nMap\nString, String\n detectionProperties = new HashMap\nString, String\n();\ndetectionProperties.put(\nCLASSIFICATION\n, \nbackpack\n);\nMPFImageLocation imageLocation = new MPFImageLocation(0, 0, 100, 100, 1.0, detectionProperties);\n\n\n\n\nMPFVideoTrack\n\n\nClass used to store the location of detected objects in an image.\n\n\n\n\nConstructor(s):\n\n\n\n\npublic MPFVideoTrack(\n  int startFrame,\n  int stopFrame,\n  Map\nInteger, MPFImageLocation\n frameLocations,\n  float confidence,\n  Map\nString, String\n detectionProperties\n)\n\n\n\n\n\n\nMembers:\n\n\n\n\n\n\n\n\n\n\nMember\n\n\nData Type\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nstartFrame\n\n\nint\n\n\nThe first frame number (0-based index) that contained the detected object.\n\n\n\n\n\n\nstopFrame\n\n\nint\n\n\nThe last frame number (0-based index) that contained the detected object.\n\n\n\n\n\n\nframeLocations\n\n\nMap\nInteger, MPFImageLocation\n\n\nA map of individual detections. The key for each map entry is the frame number where the detection was generated, and the value is a \nMPFImageLocation\n calculated as if that frame was a still image. Note that a key-value pair is \nnot\n required for every frame between the track start frame and track stop frame. In some cases, frames are deliberately skipped, as when a FRAME_INTERVAL \n 1 is specified\n\n\n\n\n\n\nconfidence\n\n\nfloat\n\n\nRepresents the \"quality\" of the detection. The range depends on the detection algorithm. 0.0 is lowest quality. Higher values are higher quality. Using a standard range of [0.0 - 1.0] is advised. If the component is unable to supply a confidence value, it should return -1.0.\n\n\n\n\n\n\ndetectionProperties\n\n\nMap\nString, String\n\n\nOptional additional information about the detected object. There is no restriction on the keys or the number of entries that can be added to the properties map. For best practice, keys should be in all CAPS.\n\n\n\n\n\n\n\n\n\n\nExample:\n\n\n\n\n\n\nNOTE:\n Currently, \nMPFVideoTrack.detectionProperties\n do not show up in the JSON output object or are used by the WFM in any way.\n\n\n\n\nA component that detects text could add an entry to \ndetectionProperties\n where the key is \nTRANSCRIPT\n and the value is a string representing the text found in the video segment.\n\n\nMap\nString, String\n detectionProperties = new HashMap\nString, String\n();\ndetectionProperties.put(\nTRANSCRIPT\n, \nRE5ULTS FR0M A TEXT DETECTER\n);\nMPFVideoTrack videoTrack = new MPFVideoTrack(0, 5, frameLocations, 1.0, detectionProperties);\n\n\n\n\nMPFAudioTrack\n\n\nClass used to store the location of detected objects in an image.\n\n\n\n\nConstructor(s):\n\n\n\n\npublic MPFAudioTrack(\n  int startTime,\n  int stopTime,\n  float confidence,\n  Map\nString, String\n detectionProperties\n)\n\n\n\n\n\n\nMembers:\n\n\n\n\n\n\n\n\n\n\nMember\n\n\nData Type\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nstartTime\n\n\nint\n\n\nThe time (0-based index, in ms) when the audio detection event started.\n\n\n\n\n\n\nstopTime\n\n\nint\n\n\nThe time (0-based index, in ms) when the audio detection event stopped.\n\n\n\n\n\n\nconfidence\n\n\nfloat\n\n\nRepresents the \"quality\" of the detection. The range depends on the detection algorithm. 0.0 is lowest quality. Higher values are higher quality. Using a standard range of [0.0 - 1.0] is advised. If the component is unable to supply a confidence value, it should return -1.0.\n\n\n\n\n\n\ndetectionProperties\n\n\nMap\nString, String\n\n\nOptional additional information about the detection. There is no restriction on the keys or the number of entries that can be added to the properties map. For best practice, keys should be in all CAPS.\n\n\n\n\n\n\n\n\n\n\nNOTE:\n Currently, \nMPFAudioTrack.detectionProperties\n do not show up in the JSON output object or are used by the WFM in any way.\n\n\n\n\nMPFGenericTrack\n\n\nClass used to store the location of detected objects in a file that is not a video, image, or audio file. The file is of the UNKNOWN type and handled generically.\n\n\n\n\nConstructor(s):\n\n\n\n\npublic MPFGenericTrack(\n  float confidence,\n  Map\nString, String\n detectionProperties\n)\n\n\n\n\n\n\nMembers:\n\n\n\n\n\n\n\n\n\n\nMember\n\n\nData Type\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nconfidence\n\n\nfloat\n\n\nRepresents the \"quality\" of the detection. The range depends on the detection algorithm. 0.0 is lowest quality. Higher values are higher quality. Using a standard range of [0.0 - 1.0] is advised. If the component is unable to supply a confidence value, it should return -1.0.\n\n\n\n\n\n\ndetectionProperties\n\n\nMap\nString, String\n\n\nOptional additional information about the detection. There is no restriction on the keys or the number of entries that can be added to the properties map. For best practice, keys should be in all CAPS.\n\n\n\n\n\n\n\n\nEnumeration Types\n\n\nMPFDetectionError\n\n\nEnum used to indicate the status of \ngetDetections\n in a \nMPFComponentDetectionError\n. A component is not required to support all error types.\n\n\n\n\n\n\n\n\nENUM\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nMPF_OTHER_DETECTION_ERROR_TYPE\n\n\nThe component method has failed for a reason that is not captured by any of the other error codes.\n\n\n\n\n\n\nMPF_DETECTION_NOT_INITIALIZED\n\n\nThe initialization of the component, or the initialization of any of its dependencies, has failed for any reason.\n\n\n\n\n\n\nMPF_UNRECOGNIZED_DATA_TYPE\n\n\nThe media data type received by a component is not one of the values contained in the \nMPFDataType\n enum.  Note that this failure is normally caught by the Component Executor before a job is passed to the component logic.\n\n\n\n\n\n\nMPF_UNSUPPORTED_DATA_TYPE\n\n\nThe job passed to a component requests processing of a job of an unsupported type. For instance, a component that is only capable of processing audio files should return this error code if a video or image job request is received.\n\n\n\n\n\n\nMPF_INVALID_DATAFILE_URI\n\n\nThe string containing the URI location of the input data file is invalid or empty.\n\n\n\n\n\n\nMPF_COULD_NOT_OPEN_DATAFILE\n\n\nThe data file to be processed could not be opened for any reason, such as a permissions failure, or an unreachable URI.\n\n\n\n\n\n\nMPF_COULD_NOT_READ_DATAFILE\n\n\nThere is a failure reading data from a successfully opened input data file.\n\n\n\n\n\n\nMPF_FILE_WRITE_ERROR\n\n\nThe component received a failure for any reason when attempting to write to a file.\n\n\n\n\n\n\nMPF_IMAGE_READ_ERROR\n\n\nThe component failed to read the image provided by the URI.\n\n\n\n\n\n\nMPF_BAD_FRAME_SIZE\n\n\nThe frame data retrieved has an incorrect or invalid frame size.\n\n\n\n\n\n\nMPF_BOUNDING_BOX_SIZE_ERROR\n\n\nThe calculation of a detection location bounding box has failed. For example, a component may be using an external library to detect objects, but the bounding box returned by that library lies partially outside the frame boundaries.\n\n\n\n\n\n\nMPF_INVALID_FRAME_INTERVAL\n\n\nAn invalid or unsupported frame interval was received.\n\n\n\n\n\n\nMPF_INVALID_START_FRAME\n\n\nThe component received an invalid start frame number. For example, if the start frame is less than zero, or greater than the stop frame, this error code should be used.\n\n\n\n\n\n\nMPF_INVALID_STOP_FRAME\n\n\nThe component receives an invalid stop frame number. For example, if the stop frame is less than the start frame, or greater than the number of the last frame in a video segment, this error code should be used.\n\n\n\n\n\n\nMPF_DETECTION_FAILED\n\n\nGeneral failure of a detection algorithm.  This does not indicate a lack of detections found in the media, but rather a break down in the algorithm that makes it impossible to continue to try to detect objects.\n\n\n\n\n\n\nMPF_DETECTION_TRACKING_FAILED\n\n\nGeneral failure of a tracking algorithm.  This does not indicate a lack of tracks generated for the media, but rather a break down in the algorithm that makes it impossible to continue to try to track objects.\n\n\n\n\n\n\nMPF_INVALID_PROPERTY\n\n\nThe component received a property that is unrecognized or has an invalid/out-of-bounds value.\n\n\n\n\n\n\nMPF_MISSING_PROPERTY\n\n\nThe component received a job that is missing a required property.\n\n\n\n\n\n\nMPF_JOB_PROPERTY_IS_NOT_INT\n\n\nA job property is supposed to be an integer type, but it is of some other type, such as a boolean or a floating point value.\n\n\n\n\n\n\nMPF_JOB_PROPERTY_IS_NOT_FLOAT\n\n\nA job property is supposed to be a floating point type, but it is of some other type, such as a boolean value.\n\n\n\n\n\n\nMPF_INVALID_ROTATION\n\n\nThe component received a job that requests rotation of the media, but the rotation value given is not in the set of acceptable values.  The set of acceptable values is {0, 90, 180, 270}.\n\n\n\n\n\n\nMPF_MEMORY_ALLOCATION_FAILED\n\n\nThe component failed to allocate memory for any reason.\n\n\n\n\n\n\nMPF_GPU_ERROR\n\n\nThe job was configured to execute on a GPU, but there was an issue with the GPU or no GPU was detected.\n\n\n\n\n\n\n\n\nUtility Classes\n\n\nTODO: Implement Java utility classes\n\n\nJava Component Build Environment\n\n\nA Java Component must be built using a version of the Java SDK that is compatible with the one used to build the Java Component Executor. The OpenMPF Java Component Executor is currently built using Java version 1.8.0_144. In general, the Java SDK is backwards compatible.\n\n\nComponents should be supplied as a tar file, which includes not only the component library, but any other libraries or files needed for execution. This includes all other non-standard libraries used by the component (aside from the standard Linux and Java SDK libraries), and any configuration or data files.\n\n\nComponent Development Best Practices\n\n\nSingle-threaded Operation\n\n\nImplementations are encouraged to operate in single-threaded mode. OpenMPF will parallelize components through multiple instantiations of the component, each running as a separate service.\n\n\nStateless Behavior\n\n\nOpenMPF components should be stateless in operation and give identical output for a provided input (i.e. when processing the same \nMPFJob\n).\n\n\nComponent Packaging\n\n\nIt is recommended that Java components are organized according to the following directory structure:\n\n\ncomponentName\n\u251c\u2500\u2500 config - Other component-specific configuration\n\u251c\u2500\u2500 descriptor\n\u2502   \u2514\u2500\u2500 descriptor.json\n\u2514\u2500\u2500 lib - All libraries required by the component\n\u2514\u2500\u2500 libComponentName.jar - Compiled component library\n\n\n\n\nOnce built, components should be packaged into a .tar.gz containing the contents of the directory shown above.\n\n\nLogging\n\n\nIt is recommended to use \nslf4j\n with \nlog4j2\n for OpenMPF Java Component logging. Multiple instances of the same component can log to the same file. Logging content can span multiple lines.\n\n\nLog files should be output to:\n\n${MPF_LOG_PATH}/${THIS_MPF_NODE}/log/\ncomponentName\n.log\n\n\nEach log statement must take the form:\n\nDATE TIME LEVEL CONTENT\n\n\nThe following log LEVELs are supported:\n \nFATAL, ERROR, WARN,  INFO,  DEBUG, TRACE\n.\n\n\nFor example:\n\n2016-02-09 13:42:42,341 INFO - Starting sample-component: [  OK  ]\n\n\nThe following log4j2 configuration can be used to match the format of other OpenMPF logs:\n\n\nConfiguration status=\nWARN\n \n!-- status=\nWARN\n is the logging level for configuration issues in this file. --\n\n\n    \nProperties\n\n        \nProperty name=\nsampleComponentLogFile\n${env:MPF_LOG_PATH}/${env:THIS_MPF_NODE}/log/sample-component-detection.log\n/Property\n\n        \nProperty name=\nlayoutPattern\n%date %level [%thread] %logger{1.} - %msg%n\n/Property\n\n    \n/Properties\n\n\n    \nAppenders\n\n        \nConsole name=\nSTDOUT\n\n            \nPatternLayout pattern=\n${layoutPattern}\n/\n\n        \n/Console\n\n\n        \nRollingFile name=\nSAMPLE_COMPONENT_FILE\n fileName=\n${sampleComponentLogFile}\n\n                     filePattern=\n${sampleComponentLogFile}.%date{yyyy-MM-dd}.%i\n\n            \nPatternLayout pattern=\n${layoutPattern}\n/\n\n            \nPolicies\n\n                \n!-- Causes a rollover once the date/time pattern specified in filePattern no longer applies to the\n                     active file. --\n\n                \nTimeBasedTriggeringPolicy /\n\n                \nSizeBasedTriggeringPolicy size=\n50MB\n/\n\n            \n/Policies\n\n        \n/RollingFile\n\n\n    \n/Appenders\n\n\n    \nLoggers\n\n        \n!-- To change the verbosity of MPF's own logging, change the level in the XML element below. --\n\n        \nLogger name=\norg.mitre\n level=\nINFO\n /\n\n\n        \nRoot level=\nINFO\n\n            \nAppenderRef ref=\nSTDOUT\n/\n\n            \nAppenderRef ref=\nSAMPLE_COMPONENT_FILE\n/\n\n        \n/Root\n\n    \n/Loggers\n\n\n/Configuration", 
            "title": "Java Batch Component API"
        }, 
        {
            "location": "/Java-Batch-Component-API/index.html#api-overview", 
            "text": "In OpenMPF, a  component  is a plugin that receives jobs (containing media), processes that  media, and returns results.  The OpenMPF Batch Component API currently supports the development of  detection components , which are used detect objects in image, video, audio, or other (generic) files that reside on disk.  Using this API, detection components can be built to provide:   Detection (Localizing an object)  Tracking (Localizing an object across multiple frames)  Classification (Detecting the type of object and optionally localizing that object)  Transcription (Detecting speech and transcribing it into text)", 
            "title": "API Overview"
        }, 
        {
            "location": "/Java-Batch-Component-API/index.html#how-components-integrate-into-openmpf", 
            "text": "Components are integrated into OpenMPF through the use of OpenMPF's  Component Executor . Developers create component libraries that encapsulate the component detection logic. Each instance of the Component Executor loads one of these libraries and uses it to service job requests sent by the OpenMPF Workflow Manager (WFM).  The Component Executor:   Receives and parses job requests from the WFM  Invokes methods on the component library to obtain detection results  Populates and sends the respective responses to the WFM   The basic psuedocode for the Component Executor is as follows:  component.setRunDirectory(...)\ncomponent.init()\nwhile (true) {\n    job = ReceiveJob()\n    if (component.supports(job.dataType))\n        component.getDetections(...) // Component does the work here\n    }\ncomponent.close()  Each instance of a Component Executor runs as a separate process.  The Component Executor receives and parses requests from the WFM, invokes methods on the Component Logic to get detection objects, and subsequently populates responses with the component output and sends them to the WFM.  A component developer implements a detection component by extending  MPFDetectionComponentBase .  As an alternative to extending  MPFDetectionComponentBase  directly, a developer may extend one of several convenience adapter classes provided by OpenMPF. See  Convenience Adapters  for more information.", 
            "title": "How Components Integrate into OpenMPF"
        }, 
        {
            "location": "/Java-Batch-Component-API/index.html#getting-started", 
            "text": "The quickest way to get started with the Java Batch Component API is to first read the  OpenMPF Component API Overview  and then  review the source  for example OpenMPF Java detection components.  Detection components are implemented by:   Extending  MPFDetectionComponentBase .  Building the component into a jar. (See  HelloWorldComponent pom.xml ).  Packaging the component into an OpenMPF-compliant .tar.gz file. (See  Component Packaging ).  Registering the component with OpenMPF. (See  Packaging and Registering a Component ).", 
            "title": "Getting Started"
        }, 
        {
            "location": "/Java-Batch-Component-API/index.html#api-specification", 
            "text": "The figure below presents a high-level component diagram of the Java Batch Component API:   The API consists of  Component Interfaces , which provide interfaces and abstract classes for developing components;  Job Definitions , which define the work to be performed by a component;  Job Results , which define the results generated by the component; and  Component Adapters , which provide default implementations of several of the  MPFDetectionComponentInterface  methods (See the  MPFAudioAndVideoDetectionComponentAdapter  for an example;  TODO: implement those shown in the diagram ). In the future, the API will also include  Component Utilities , which perform actions such as image flipping, rotation, and cropping.  Component Interfaces   MPFComponentInterface  - Interface for all Java components that perform batch processing.  MPFComponentBase  - An abstract baseline for components. Provides default implementations for  MPFComponentInterface .   Detection Component Interfaces   MPFDetectionComponentInterface  - Baseline interface for detection components.  MPFDetectionComponentBase  - An abstract baseline for detection components. Provides default implementations for   MPFDetectionComponentInterface .   Job Definitions  The following classes define the details about a specific job (work unit):   MPFImageJob  extends  MPFJob  MPFVideoJob  extends  MPFJob  MPFAudioJob  extends  MPFJob  MPFGenericJob  extends  MPFJob   Job Results  The following classes define detection results:   MPFImageLocation  MPFVideoTrack  MPFAudioTrack  MPFGenericTrack", 
            "title": "API Specification"
        }, 
        {
            "location": "/Java-Batch-Component-API/index.html#component-interface", 
            "text": "The OpenMPF Component class structure consists of:   MPFComponentInterface  - Interface for all OpenMPF Java components that perform batch processing.  MPFComponentBase  - An abstract baseline for components. Provides default implementations for  MPFComponentInterface .    IMPORTANT:  This interface and abstract class should not be directly implemented because no mechanism exists for launching components based off of it. Instead, it defines the contract that components must follow. Currently, the only supported type of batch component is \"DETECTION\". Those components should extend  MPFDetectionComponentBase   See the latest source here.", 
            "title": "Component Interface"
        }, 
        {
            "location": "/Java-Batch-Component-API/index.html#setrundirectorystring", 
            "text": "Sets the value to the full path of the parent folder above where the component is installed.   Method Definition:   public void setRunDirectory(String runDirectory);   Parameters:      Parameter  Data Type  Description      runDirectory  String  Full path of the parent folder above where the component is installed.      Returns: none    IMPORTANT:   setRunDirectory  is called by the Component Executor to set the correct path. It is not necessary to call this method in your component implementation.", 
            "title": "setRunDirectory(String)"
        }, 
        {
            "location": "/Java-Batch-Component-API/index.html#getrundirectory", 
            "text": "Returns the full path of the parent folder above where the component is installed.   Method Definition:   public String getRunDirectory()    Parameters: none    Returns: ( String ) Full path of the parent folder above where the component is installed.", 
            "title": "getRunDirectory()"
        }, 
        {
            "location": "/Java-Batch-Component-API/index.html#init", 
            "text": "Performs any necessary startup tasks for the component. This will be executed once by the Component Executor, on component startup, before the first job, after  setRunDirectory .   Method Definition:   public void init()    Parameters: none    Returns: none    Example:    public void init() {\n    // Setup logger, Load data models, etc.\n}", 
            "title": "init()"
        }, 
        {
            "location": "/Java-Batch-Component-API/index.html#close", 
            "text": "Performs any necessary shutdown tasks for the component. This will be executed once by the Component Executor, on component shutdown, usually after the last job.   Method Definition:   public void close()    Parameters: none    Returns: none    Example:    public void close() {\n    // Close file handlers, etc.\n}", 
            "title": "close()"
        }, 
        {
            "location": "/Java-Batch-Component-API/index.html#getcomponenttype", 
            "text": "Allows the Component API to determine the component \"type.\" Currently  DETECTION  is the only supported component type.   Method Definition:   public MPFComponentType getComponentType()    Parameters: none    Returns: ( MPFComponentType ) Currently,  DETECTION  is the only supported return value.    Example:    public MPFComponentType getComponentType() {\n    return MPFComponentType.DETECTION;\n}", 
            "title": "getComponentType()"
        }, 
        {
            "location": "/Java-Batch-Component-API/index.html#detection-component-interface", 
            "text": "The  MPFDetectionComponentInterface  must be utilized by all OpenMPF Java detection components that perform batch processing.  Every batch detection component must define a  component  class which implements the MPFComponentInterface. This is typically performed by extending  MPFDetectionComponentBase , which extends  MPFComponentBase  and implements  MPFDetectionComponentInterface .  To designate the component class, every batch detection component should include an applicationContext.xml which defines the  component  bean.  The  component  bean class must implement  MPFDetectionComponentInterface .   IMPORTANT:  Each batch detection component must implement all of the  getDetections()  methods or extend from a superclass which provides implementations for them (see  convenience adapters ).  If your component does not support a particular data type, it should simply:  throw new MPFComponentDetectionError(MPFDetectionError.MPF_UNSUPPORTED_DATA_TYPE);", 
            "title": "Detection Component Interface"
        }, 
        {
            "location": "/Java-Batch-Component-API/index.html#convenience-adapters", 
            "text": "As an alternative to extending  MPFDetectionComponentBase  directly, developers may extend a convenience adapter classes provided by OpenMPF.  These adapters provide default implementations of several methods in  MPFDetectionComponentInterface  and ensure that the component's logic properly extends from the Component API. This enables developers to concentrate on implementation of the detection algorithm.  The following adapter is provided:   Audio And Video Detection Component Adapter ( source )    Example: Using Adaptors to Provide Simple AudioVisual Handling: \nMany components designed to work on audio files, such as speech detection, are relevant to video files as well.  Some of the tools for these components, however, only function on audio files (such as .wav, .mp3) and not video files (.avi, .mov, etc).  The  MPFAudioAndVideoDetectionComponentAdapter  adapter class implements the  getDetections(MPFVideoJob)  method by translating the video request into an audio request.  It builds a temporary audio file by ripping the audio from the video media input, translates the  MPFVideoJob  into an  MPFAudioJob , and invokes  getDetections(MPFAudioJob)  on the generated file.  Once processing is done, the adapter translates the  MPFAudioTrack  list into an  MPFVideoTrack  list.  Since only audio and video files are relevant to this adapter, it provides a default implementation of the  getDetections(MPFImageJob)  method which throws  new MPFComponentDetectionError(MPFDetectionError.MPF_UNSUPPORTED_DATA_TYPE) .  The Sphinx speech detection component uses this adapter to run Sphinx speech detection on video files.  Other components that need to process video files as audio may also use the adapter.", 
            "title": "Convenience Adapters"
        }, 
        {
            "location": "/Java-Batch-Component-API/index.html#supportsmpfdatatype", 
            "text": "Returns the supported data types of the component.   Method Definition:   public boolean supports(MPFDataType dataType)   Parameters:      Parameter  Data Type  Description      dataType  MPFDataType  Return true if the component supports IMAGE, VIDEO, AUDIO, and/or UNKNOWN (generic) processing.       Returns: ( boolean ) True if the component supports the data type, otherwise false.    Example:    // Sample Component that supports only image and video files\npublic boolean supports(MPFDataType dataType) {\n    return dataType == MPFDataType.IMAGE || dataType == MPFDataType.VIDEO;\n}", 
            "title": "supports(MPFDataType)"
        }, 
        {
            "location": "/Java-Batch-Component-API/index.html#getdetectiontype", 
            "text": "Returns the type of object detected by the component.   Method Definition:   public String getDetectionType()    Parameters: none    Returns: ( String ) The type of object detected by the component. Should be in all CAPS. Examples include:  FACE ,  MOTION ,  PERSON ,  SPEECH ,  CLASS  (for object classification), or  TEXT .    Example:    public String getDetectionType() {\n    return  FACE ;\n}", 
            "title": "getDetectionType()"
        }, 
        {
            "location": "/Java-Batch-Component-API/index.html#getdetectionsmpfimagejob", 
            "text": "Used to detect objects in image files. The MPFImageJob class contains the URI specifying the location of the image file.  Currently, the dataUri is always a local file path. For example, \"/opt/mpf/share/remote-media/test-file.jpg\". This is because all media is copied to the OpenMPF server before the job is executed.   Method Definition:   public List MPFImageLocation  getDetections(MPFImageJob job)\n  throws MPFComponentDetectionError;   Parameters:      Parameter  Data Type  Description      job  MPFImageJob  Class containing details about the work to be performed. See  MPFImageJob       Returns: ( List MPFImageLocation ) The  MPFImageLocation  data for each detected object.    Example:    public List MPFImageLocation  getDetections(MPFImageJob job)\n  throws MPFComponentDetectionError {\n    // Component logic to generate image locations\n}", 
            "title": "getDetections(MPFImageJob)"
        }, 
        {
            "location": "/Java-Batch-Component-API/index.html#getdetectionsmpfvideojob", 
            "text": "Used to detect objects in a video.  Prior to being sent to the component, videos are split into logical \"segments\" of video data and each segment (containing a range of frames) is assigned to a different job. Components are not guaranteed to receive requests in any order. For example, the first request processed by a component might receive a request for frames 300-399 of a Video A, while the next request may cover frames 900-999 of a Video B.   Method Definition:   public List MPFVideoTrack  getDetections(MPFVideoJob job)\n  throws MPFComponentDetectionError;   Parameters:      Parameter  Data Type  Description      job  MPFVideoJob  Class containing details about the work to be performed. See  MPFVideoJob       Returns: ( List MPFVideoTrack ) The  MPFVideoTrack  data for each detected object.    Example:    public List MPFVideoTrack  getDetections(MPFVideoJob job)\n  throws MPFComponentDetectionError {\n    // Component logic to generate video tracks\n}", 
            "title": "getDetections(MPFVideoJob)"
        }, 
        {
            "location": "/Java-Batch-Component-API/index.html#getdetectionsmpfaudiojob", 
            "text": "Used to detect objects in audio files. Currently, audio files are not logically segmented, so a job will contain the entirety of the audio file.   Method Definition:   public List MPFAudioTrack  getDetections(MPFAudioJob job)\n  throws MPFComponentDetectionError;   Parameters:      Parameter  Data Type  Description      job  MPFAudioJob  Class containing details about the work to be performed. See  MPFAudioJob       Returns: ( List MPFAudioTrack ) The  MPFAudioTrack  data for each detected object.    Example:    public List MPFAudioTrack  getDetections(MPFAudioJob job)\n  throws MPFComponentDetectionError {\n    // Component logic to generate audio tracks\n}", 
            "title": "getDetections(MPFAudioJob)"
        }, 
        {
            "location": "/Java-Batch-Component-API/index.html#getdetectionsmpfgenericjob", 
            "text": "Used to detect objects in files that aren't video, image, or audio files. Such files are of the UNKNOWN type and handled generically. These files are not logically segmented, so a job will contain the entirety of the file.   Method Definition:   public List MPFGenericTrack  getDetections(MPFGenericJob job)\n  throws MPFComponentDetectionError;   Parameters:      Parameter  Data Type  Description      job  MPFGenericJob  Class containing details about the work to be performed. See  MPFGenericJob       Returns: ( List MPFGenericTrack ) The  MPFGenericTrack  data for each detected object.    Example:    public List MPFGenericTrack  getDetections(MPFGenericJob job)\n  throws MPFComponentDetectionError {\n    // Component logic to generate generic tracks\n}", 
            "title": "getDetections(MPFGenericJob)"
        }, 
        {
            "location": "/Java-Batch-Component-API/index.html#mpfcomponentdetectionerror", 
            "text": "An exception that occurs in a component.  The exception must contain a reference to a valid  MPFDetectionError .   Constructor(s):   public MPFComponentDetectionError (\n  MPFDetectionError error,\n  String msg,\n  Exception e\n)   Parameters:      Parameter  Data Type  Description      error  MPFDetectionError  The type of error generated by the component. See  MPFDetectionError .    msg  String  The detail message (which is saved for later retrieval by the  Throwable.getMessage()  method).    e  Exception  The cause (which is saved for later retrieval by the  Throwable.getCause()  method). A null value is permitted.", 
            "title": "MPFComponentDetectionError"
        }, 
        {
            "location": "/Java-Batch-Component-API/index.html#detection-job-classes", 
            "text": "The following classes contain details about a specific job (work unit):   MPFImageJob  extends  MPFJob  MPFVideoJob  extends  MPFJob  MPFAudioJob  extends  MPFJob  MPFGenericJob  extends  MPFJob   The following classes define detection results:   MPFImageLocation  MPFVideoTrack  MPFAudioTrack  MPFGenericTrack", 
            "title": "Detection Job Classes"
        }, 
        {
            "location": "/Java-Batch-Component-API/index.html#mpfjob", 
            "text": "Class containing data used for detection of objects.   Constructor(s):   protected MPFJob(\n  String jobName,\n  String dataUri,\n  final Map String, String  jobProperties,\n  final Map String, String  mediaProperties\n)   Members:      Member  Data Type  Description      jobName   String  A specific name given to the job by the OpenMPF Framework. This value may be used, for example, for logging and debugging purposes.    dataUri   String  The URI of the input media file to be processed. Currently, this is a file path. For example, \"/opt/mpf/share/remote-media/test-file.avi\".    jobProperties   Map String, String  The key corresponds to the property name specified in the component descriptor file described in \"Installing and Registering a Component\". Values are determined by an end user when creating a pipeline.   Note: Only those property values specified by the user will be in the jobProperties map; for properties not contained in the map, the component must use a default value.    mediaProperties   Map String, String  Metadata about the media associated with the job. The key is the property name and value is the property value. The entries in the map vary depend on the job type. They are defined in the specific Job's API description.", 
            "title": "MPFJob"
        }, 
        {
            "location": "/Java-Batch-Component-API/index.html#mpfimagejob", 
            "text": "Extends  MPFJob  Class containing data used for detection of objects in image files.   Constructor(s):   public MPFImageJob(\n  String jobName,\n  String dataUri,\n  final Map String, String  jobProperties,\n  final Map  String, String  mediaProperties)  public MPFImageJob(\n  String jobName,\n  String dataUri,\n  final Map String, String  jobProperties,\n  final Map  String, String  mediaProperties,\n  MPFImageLocation location)   Members:   \n   \n     \n       Member \n       Data Type \n       Description \n     \n   \n   \n     \n       jobName \n       String \n       See  MPFJob.jobName  for description. \n     \n     \n       dataUri \n       String \n       See  MPFJob.dataUri  for description. \n     \n     \n       jobProperties \n       Map , String \n       See  MPFJob.jobProperties  for description. \n     \n     \n       mediaProperties \n       Map , String \n       \n        See  MPFJob.mediaProperties  for description.\n         \n        Includes the following key-value pairs:\n         \n           MIME_TYPE  : the MIME type of the media \n           FRAME_WIDTH  : the width of the image in pixels \n           FRAME_HEIGHT  : the height of the image in pixels \n         \n        May include the following key-value pairs:\n         \n           ROTATION  : A floating point value in the interval  [0.0, 360.0)  indicating the orientation of the media in degrees in the counter-clockwise direction. In order to view the media in the upright orientation, it must be rotated the given number of degrees in the clockwise direction. \n           HORIZONTAL_FLIP  : true if the image is mirrored across the Y-axis, otherwise false \n           EXIF_ORIENTATION  : the standard EXIF orientation tag; a value between 1 and 8 \n         \n       \n     \n     \n       location \n       MPFImageLocation \n       An  MPFImageLocation  from the previous pipeline stage. Provided when feed forward is enabled. See  Feed Forward Guide .", 
            "title": "MPFImageJob"
        }, 
        {
            "location": "/Java-Batch-Component-API/index.html#mpfvideojob", 
            "text": "Extends  MPFJob  Class containing data used for detection of objects in video files.   Constructor(s):   public MPFVideoJob(\n  String jobName,\n  String dataUri,\n  final Map String, String  jobProperties,\n  final Map String, String  mediaProperties,\n  int startFrame,\n  int stopFrame)  public MPFVideoJob(\n  String jobName,\n  String dataUri,\n  final Map String, String  jobProperties,\n  final Map String, String  mediaProperties,\n  int startFrame,\n  int stopFrame,\n  MPFVideoTrack track)   Members:   \n   \n     \n       Member \n       Data Type \n       Description \n     \n   \n   \n     \n       jobName \n       String \n       See  MPFJob.jobName  for description. \n     \n     \n       dataUri \n       String \n       See  MPFJob.dataUri  for description. \n     \n     \n       startFrame \n       int \n       The first frame number (0-based index) of the video that should be processed to look for detections. \n     \n     \n       stopFrame \n       int \n       The last frame number (0-based index) of the video that should be processed to look for detections. \n             \n     \n       jobProperties \n       Map , String \n       See  MPFJob.jobProperties  for description. \n     \n     \n       mediaProperties \n       Map , String \n       \n        See  MPFJob.mediaProperties  for description.\n         \n        Includes the following key-value pairs:\n         \n           DURATION  : length of video in milliseconds \n           FPS  : frames per second (averaged for variable frame rate video) \n           FRAME_COUNT  : the number of frames in the video \n           MIME_TYPE  : the MIME type of the media \n           FRAME_WIDTH  : the width of a frame in pixels \n           FRAME_HEIGHT  : the height of a frame in pixels \n         \n        May include the following key-value pair:\n         \n           ROTATION  : A floating point value in the interval  [0.0, 360.0)  indicating the orientation of the media in degrees in the counter-clockwise direction. In order to view the media in the upright orientation, it must be rotated the given number of degrees in the clockwise direction. \n         \n       \n     \n     \n       track \n       MPFVideoTrack \n       An  MPFVideoTrack  from the previous pipeline stage. Provided when feed forward is enabled. See  Feed Forward Guide . \n     \n      IMPORTANT:   FRAME_INTERVAL  is a common job property that many components support. For frame intervals greater than 1, the component must look for detections starting with the first frame, and then skip frames as specified by the frame interval, until or before it reaches the stop frame. For example, given a start frame of 0, a stop frame of 99, and a frame interval of 2, then the detection component must look for objects in frames numbered 0, 2, 4, 6, ..., 98.", 
            "title": "MPFVideoJob"
        }, 
        {
            "location": "/Java-Batch-Component-API/index.html#mpfaudiojob", 
            "text": "Extends  MPFJob  Class containing data used for detection of objects in audio files.   Constructor(s):   public MPFAudioJob(\n  String jobName,\n  String dataUri,\n  final Map String, String  jobProperties,\n  final Map String, String  mediaProperties,\n  int startTime,\n  int stopTime)  public MPFAudioJob(\n  String jobName,\n  String dataUri,\n  final Map String, String  jobProperties,\n  final Map String, String  mediaProperties,\n  int startTime,\n  int stopTime,\n  MPFAudioTrack track)   Members:   \n   \n     \n       Member \n       Data Type \n       Description \n     \n   \n   \n     \n       jobName \n       String \n       See  MPFJob.jobName  for description. \n     \n     \n       dataUri \n       String \n       See  MPFJob.dataUri  for description. \n     \n     \n       startTime \n       int \n       The time (0-based index, in ms) associated with the beginning of the segment of the audio file that should be processed to look for detections. \n     \n     \n       stopTime \n       int \n       The time (0-based index, in ms) associated with the end of the segment of the audio file that should be processed to look for detections. \n             \n     \n       jobProperties \n       Map , String \n       See  MPFJob.jobProperties  for description. \n     \n     \n       mediaProperties \n       Map , String \n       \n        See  MPFJob.mediaProperties  for description.\n         \n        Includes the following key-value pairs:\n         \n           DURATION  : length of audio file in milliseconds \n           MIME_TYPE  : the MIME type of the media \n         \n       \n     \n     \n       track \n       MPFAudioTrack \n       An  MPFAudioTrack  from the previous pipeline stage. Provided when feed forward is enabled. See  Feed Forward Guide .", 
            "title": "MPFAudioJob"
        }, 
        {
            "location": "/Java-Batch-Component-API/index.html#mpfgenericjob", 
            "text": "Extends  MPFJob  Class containing data used for detection of objects in a file that isn't a video, image, or audio file. The file is of the UNKNOWN type and handled generically. The file is not logically segmented, so a job will contain the entirety of the file.   Constructor(s):   public MPGenericJob(\n  String jobName,\n  String dataUri,\n  final Map String, String  jobProperties,\n  final Map String, String  mediaProperties)  public MPFGenericJob(\n  String jobName,\n  String dataUri,\n  final Map String, String  jobProperties,\n  final Map  String, String  mediaProperties,\n  MPFGenericTrack track) {   Members:   \n   \n     \n       Member \n       Data Type \n       Description \n     \n   \n   \n     \n       jobName \n       String \n       See  MPFJob.jobName  for description. \n     \n     \n       dataUri \n       String \n       See  MPFJob.dataUri  for description. \n     \n     \n       startTime \n       int \n       The time (0-based index, in ms) associated with the beginning of the segment of the audio file that should be processed to look for detections. \n     \n     \n       stopTime \n       int \n       The time (0-based index, in ms) associated with the end of the segment of the audio file that should be processed to look for detections. \n             \n     \n       jobProperties \n       Map , String \n       See  MPFJob.jobProperties  for description. \n     \n     \n       mediaProperties \n       Map , String \n       \n        See  MPFJob.mediaProperties  for description.\n         \n        Includes the following key-value pair:\n         \n           MIME_TYPE  : the MIME type of the media \n         \n       \n     \n     \n       track \n       MPFGenericTrack \n       An  MPFGenericTrack  from the previous pipeline stage. Provided when feed forward is enabled. See  Feed Forward Guide .", 
            "title": "MPFGenericJob"
        }, 
        {
            "location": "/Java-Batch-Component-API/index.html#detection-job-result-classes", 
            "text": "", 
            "title": "Detection Job Result Classes"
        }, 
        {
            "location": "/Java-Batch-Component-API/index.html#mpfimagelocation", 
            "text": "Class used to store the location of detected objects in an image.   Constructor(s):   public MPFImageLocation(\n  int xLeftUpper,\n  int yLeftUpper,\n  int width,\n  int height,\n  float confidence,\n  Map String, String  detectionProperties\n)   Members:      Member  Data Type  Description      xLeftUpper  int  Upper left X coordinate of the detected object.    yLeftUpper  int  Upper left Y coordinate of the detected object.    width  int  The width of the detected object.    height  int  The height of the detected object.    confidence  float  Represents the \"quality\" of the detection. The range depends on the detection algorithm. 0.0 is lowest quality. Higher values are higher quality. Using a standard range of [0.0 - 1.0] is advised. If the component is unable to supply a confidence value, it should return -1.0.    detectionProperties  Map String, String  Optional additional information about the detected object. There is no restriction on the keys or the number of entries that can be added to the properties map. For best practice, keys should be in all CAPS.      Example:   A component that performs generic object classification can add an entry to  detection_properties  where the key is  CLASSIFICATION  and the value is the type of object detected.  Map String, String  detectionProperties = new HashMap String, String ();\ndetectionProperties.put( CLASSIFICATION ,  backpack );\nMPFImageLocation imageLocation = new MPFImageLocation(0, 0, 100, 100, 1.0, detectionProperties);", 
            "title": "MPFImageLocation"
        }, 
        {
            "location": "/Java-Batch-Component-API/index.html#mpfvideotrack", 
            "text": "Class used to store the location of detected objects in an image.   Constructor(s):   public MPFVideoTrack(\n  int startFrame,\n  int stopFrame,\n  Map Integer, MPFImageLocation  frameLocations,\n  float confidence,\n  Map String, String  detectionProperties\n)   Members:      Member  Data Type  Description      startFrame  int  The first frame number (0-based index) that contained the detected object.    stopFrame  int  The last frame number (0-based index) that contained the detected object.    frameLocations  Map Integer, MPFImageLocation  A map of individual detections. The key for each map entry is the frame number where the detection was generated, and the value is a  MPFImageLocation  calculated as if that frame was a still image. Note that a key-value pair is  not  required for every frame between the track start frame and track stop frame. In some cases, frames are deliberately skipped, as when a FRAME_INTERVAL   1 is specified    confidence  float  Represents the \"quality\" of the detection. The range depends on the detection algorithm. 0.0 is lowest quality. Higher values are higher quality. Using a standard range of [0.0 - 1.0] is advised. If the component is unable to supply a confidence value, it should return -1.0.    detectionProperties  Map String, String  Optional additional information about the detected object. There is no restriction on the keys or the number of entries that can be added to the properties map. For best practice, keys should be in all CAPS.      Example:    NOTE:  Currently,  MPFVideoTrack.detectionProperties  do not show up in the JSON output object or are used by the WFM in any way.   A component that detects text could add an entry to  detectionProperties  where the key is  TRANSCRIPT  and the value is a string representing the text found in the video segment.  Map String, String  detectionProperties = new HashMap String, String ();\ndetectionProperties.put( TRANSCRIPT ,  RE5ULTS FR0M A TEXT DETECTER );\nMPFVideoTrack videoTrack = new MPFVideoTrack(0, 5, frameLocations, 1.0, detectionProperties);", 
            "title": "MPFVideoTrack"
        }, 
        {
            "location": "/Java-Batch-Component-API/index.html#mpfaudiotrack", 
            "text": "Class used to store the location of detected objects in an image.   Constructor(s):   public MPFAudioTrack(\n  int startTime,\n  int stopTime,\n  float confidence,\n  Map String, String  detectionProperties\n)   Members:      Member  Data Type  Description      startTime  int  The time (0-based index, in ms) when the audio detection event started.    stopTime  int  The time (0-based index, in ms) when the audio detection event stopped.    confidence  float  Represents the \"quality\" of the detection. The range depends on the detection algorithm. 0.0 is lowest quality. Higher values are higher quality. Using a standard range of [0.0 - 1.0] is advised. If the component is unable to supply a confidence value, it should return -1.0.    detectionProperties  Map String, String  Optional additional information about the detection. There is no restriction on the keys or the number of entries that can be added to the properties map. For best practice, keys should be in all CAPS.      NOTE:  Currently,  MPFAudioTrack.detectionProperties  do not show up in the JSON output object or are used by the WFM in any way.", 
            "title": "MPFAudioTrack"
        }, 
        {
            "location": "/Java-Batch-Component-API/index.html#mpfgenerictrack", 
            "text": "Class used to store the location of detected objects in a file that is not a video, image, or audio file. The file is of the UNKNOWN type and handled generically.   Constructor(s):   public MPFGenericTrack(\n  float confidence,\n  Map String, String  detectionProperties\n)   Members:      Member  Data Type  Description      confidence  float  Represents the \"quality\" of the detection. The range depends on the detection algorithm. 0.0 is lowest quality. Higher values are higher quality. Using a standard range of [0.0 - 1.0] is advised. If the component is unable to supply a confidence value, it should return -1.0.    detectionProperties  Map String, String  Optional additional information about the detection. There is no restriction on the keys or the number of entries that can be added to the properties map. For best practice, keys should be in all CAPS.", 
            "title": "MPFGenericTrack"
        }, 
        {
            "location": "/Java-Batch-Component-API/index.html#enumeration-types", 
            "text": "", 
            "title": "Enumeration Types"
        }, 
        {
            "location": "/Java-Batch-Component-API/index.html#mpfdetectionerror", 
            "text": "Enum used to indicate the status of  getDetections  in a  MPFComponentDetectionError . A component is not required to support all error types.     ENUM  Description      MPF_OTHER_DETECTION_ERROR_TYPE  The component method has failed for a reason that is not captured by any of the other error codes.    MPF_DETECTION_NOT_INITIALIZED  The initialization of the component, or the initialization of any of its dependencies, has failed for any reason.    MPF_UNRECOGNIZED_DATA_TYPE  The media data type received by a component is not one of the values contained in the  MPFDataType  enum.  Note that this failure is normally caught by the Component Executor before a job is passed to the component logic.    MPF_UNSUPPORTED_DATA_TYPE  The job passed to a component requests processing of a job of an unsupported type. For instance, a component that is only capable of processing audio files should return this error code if a video or image job request is received.    MPF_INVALID_DATAFILE_URI  The string containing the URI location of the input data file is invalid or empty.    MPF_COULD_NOT_OPEN_DATAFILE  The data file to be processed could not be opened for any reason, such as a permissions failure, or an unreachable URI.    MPF_COULD_NOT_READ_DATAFILE  There is a failure reading data from a successfully opened input data file.    MPF_FILE_WRITE_ERROR  The component received a failure for any reason when attempting to write to a file.    MPF_IMAGE_READ_ERROR  The component failed to read the image provided by the URI.    MPF_BAD_FRAME_SIZE  The frame data retrieved has an incorrect or invalid frame size.    MPF_BOUNDING_BOX_SIZE_ERROR  The calculation of a detection location bounding box has failed. For example, a component may be using an external library to detect objects, but the bounding box returned by that library lies partially outside the frame boundaries.    MPF_INVALID_FRAME_INTERVAL  An invalid or unsupported frame interval was received.    MPF_INVALID_START_FRAME  The component received an invalid start frame number. For example, if the start frame is less than zero, or greater than the stop frame, this error code should be used.    MPF_INVALID_STOP_FRAME  The component receives an invalid stop frame number. For example, if the stop frame is less than the start frame, or greater than the number of the last frame in a video segment, this error code should be used.    MPF_DETECTION_FAILED  General failure of a detection algorithm.  This does not indicate a lack of detections found in the media, but rather a break down in the algorithm that makes it impossible to continue to try to detect objects.    MPF_DETECTION_TRACKING_FAILED  General failure of a tracking algorithm.  This does not indicate a lack of tracks generated for the media, but rather a break down in the algorithm that makes it impossible to continue to try to track objects.    MPF_INVALID_PROPERTY  The component received a property that is unrecognized or has an invalid/out-of-bounds value.    MPF_MISSING_PROPERTY  The component received a job that is missing a required property.    MPF_JOB_PROPERTY_IS_NOT_INT  A job property is supposed to be an integer type, but it is of some other type, such as a boolean or a floating point value.    MPF_JOB_PROPERTY_IS_NOT_FLOAT  A job property is supposed to be a floating point type, but it is of some other type, such as a boolean value.    MPF_INVALID_ROTATION  The component received a job that requests rotation of the media, but the rotation value given is not in the set of acceptable values.  The set of acceptable values is {0, 90, 180, 270}.    MPF_MEMORY_ALLOCATION_FAILED  The component failed to allocate memory for any reason.    MPF_GPU_ERROR  The job was configured to execute on a GPU, but there was an issue with the GPU or no GPU was detected.", 
            "title": "MPFDetectionError"
        }, 
        {
            "location": "/Java-Batch-Component-API/index.html#utility-classes", 
            "text": "TODO: Implement Java utility classes", 
            "title": "Utility Classes"
        }, 
        {
            "location": "/Java-Batch-Component-API/index.html#java-component-build-environment", 
            "text": "A Java Component must be built using a version of the Java SDK that is compatible with the one used to build the Java Component Executor. The OpenMPF Java Component Executor is currently built using Java version 1.8.0_144. In general, the Java SDK is backwards compatible.  Components should be supplied as a tar file, which includes not only the component library, but any other libraries or files needed for execution. This includes all other non-standard libraries used by the component (aside from the standard Linux and Java SDK libraries), and any configuration or data files.", 
            "title": "Java Component Build Environment"
        }, 
        {
            "location": "/Java-Batch-Component-API/index.html#component-development-best-practices", 
            "text": "", 
            "title": "Component Development Best Practices"
        }, 
        {
            "location": "/Java-Batch-Component-API/index.html#single-threaded-operation", 
            "text": "Implementations are encouraged to operate in single-threaded mode. OpenMPF will parallelize components through multiple instantiations of the component, each running as a separate service.", 
            "title": "Single-threaded Operation"
        }, 
        {
            "location": "/Java-Batch-Component-API/index.html#stateless-behavior", 
            "text": "OpenMPF components should be stateless in operation and give identical output for a provided input (i.e. when processing the same  MPFJob ).", 
            "title": "Stateless Behavior"
        }, 
        {
            "location": "/Java-Batch-Component-API/index.html#component-packaging", 
            "text": "It is recommended that Java components are organized according to the following directory structure:  componentName\n\u251c\u2500\u2500 config - Other component-specific configuration\n\u251c\u2500\u2500 descriptor\n\u2502   \u2514\u2500\u2500 descriptor.json\n\u2514\u2500\u2500 lib - All libraries required by the component\n\u2514\u2500\u2500 libComponentName.jar - Compiled component library  Once built, components should be packaged into a .tar.gz containing the contents of the directory shown above.", 
            "title": "Component Packaging"
        }, 
        {
            "location": "/Java-Batch-Component-API/index.html#logging", 
            "text": "It is recommended to use  slf4j  with  log4j2  for OpenMPF Java Component logging. Multiple instances of the same component can log to the same file. Logging content can span multiple lines.  Log files should be output to: ${MPF_LOG_PATH}/${THIS_MPF_NODE}/log/ componentName .log  Each log statement must take the form: DATE TIME LEVEL CONTENT  The following log LEVELs are supported:\n  FATAL, ERROR, WARN,  INFO,  DEBUG, TRACE .  For example: 2016-02-09 13:42:42,341 INFO - Starting sample-component: [  OK  ]  The following log4j2 configuration can be used to match the format of other OpenMPF logs:  Configuration status= WARN   !-- status= WARN  is the logging level for configuration issues in this file. -- \n\n     Properties \n         Property name= sampleComponentLogFile ${env:MPF_LOG_PATH}/${env:THIS_MPF_NODE}/log/sample-component-detection.log /Property \n         Property name= layoutPattern %date %level [%thread] %logger{1.} - %msg%n /Property \n     /Properties \n\n     Appenders \n         Console name= STDOUT \n             PatternLayout pattern= ${layoutPattern} / \n         /Console \n\n         RollingFile name= SAMPLE_COMPONENT_FILE  fileName= ${sampleComponentLogFile} \n                     filePattern= ${sampleComponentLogFile}.%date{yyyy-MM-dd}.%i \n             PatternLayout pattern= ${layoutPattern} / \n             Policies \n                 !-- Causes a rollover once the date/time pattern specified in filePattern no longer applies to the\n                     active file. -- \n                 TimeBasedTriggeringPolicy / \n                 SizeBasedTriggeringPolicy size= 50MB / \n             /Policies \n         /RollingFile \n\n     /Appenders \n\n     Loggers \n         !-- To change the verbosity of MPF's own logging, change the level in the XML element below. -- \n         Logger name= org.mitre  level= INFO  / \n\n         Root level= INFO \n             AppenderRef ref= STDOUT / \n             AppenderRef ref= SAMPLE_COMPONENT_FILE / \n         /Root \n     /Loggers  /Configuration", 
            "title": "Logging"
        }, 
        {
            "location": "/Python-Batch-Component-API/index.html", 
            "text": "NOTICE:\n This software (or technical data) was produced for the U.S. Government under contract, and is subject to\nthe Rights in Data-General Clause 52.227-14, Alt. IV (DEC 2007).\nCopyright 2019 The MITRE Corporation. All Rights Reserved.\n\n\n\n\nAPI Overview\n\n\nIn OpenMPF, a \ncomponent\n is a plugin that receives jobs (containing media), processes that  media, and returns results.\n\n\nThe OpenMPF Batch Component API currently supports the development of \ndetection components\n, which are used detect\nobjects in image, video, audio, or other (generic) files that reside on disk.\n\n\nUsing this API, detection components can be built to provide:\n\n\n\n\nDetection (Localizing an object)\n\n\nTracking (Localizing an object across multiple frames)\n\n\nClassification (Detecting the type of object and optionally localizing that object)\n\n\nTranscription (Detecting speech and transcribing it into text)\n\n\n\n\nHow Components Integrate into OpenMPF\n\n\nComponents are integrated into OpenMPF through the use of OpenMPF's \nComponent Executable\n.\nDevelopers create component libraries that encapsulate the component detection logic.\nEach instance of the Component Executable loads one of these libraries and uses it to service job requests\nsent by the OpenMPF Workflow Manager (WFM).\n\n\nThe Component Executable:\n\n\n\n\nReceives and parses job requests from the WFM\n\n\nInvokes methods on the component library to obtain detection results\n\n\nPopulates and sends the respective responses to the WFM\n\n\n\n\nThe basic psuedocode for the Component Executable is as follows:\n\n\ncomponent_cls = locate_component_class()\ncomponent = component_cls()\ndetection_type = component.detection_type\n\nwhile True:\n    job = receive_job()\n\n    if is_image_job(job) and hasattr(component, 'get_detections_from_image'):\n        detections = component.get_detections_from_image(job)\n        send_job_response(detections)\n\n    elif is_video_job(job) and hasattr(component, 'get_detections_from_video'):\n        detections = component.get_detections_from_video(job)\n        send_job_response(detections)\n\n    elif is_audio_job(job) and hasattr(component, 'get_detections_from_audio'):\n        detections = component.get_detections_from_audio(job)\n        send_job_response(detections)\n\n    elif is_generic_job(job) and hasattr(component, 'get_detections_from_generic'):\n        detections = component.get_detections_from_generic(job)\n        send_job_response(detections)\n\n\n\n\nEach instance of a Component Executable runs as a separate process.\n\n\nThe Component Executable receives and parses requests from the WFM, invokes methods on the Component Logic to get\ndetection objects, and subsequently populates responses with the component output and sends them to the WFM.\n\n\nA component developer implements a detection component by creating a class that defines one or more of the\nget_detections_from_* methods and has a \ndetection_type\n field.\nSee the \nAPI Specification\n for more information.\n\n\nThe figures below present high-level component diagrams of the Python Batch Component API.\nThis figure shows the basic structure:\n\n\n\n\nThe figure above shows the Node Manager starting the Detection Component Executable.\nThe Detection Component Executable determines that it is running a Python component so it creates an instance of the\n\nPythonComponentHandle\n\nclass. The \nPythonComponentHandle\n class creates an instance of the component class and calls one of the\n\nget_detections_from_*\n methods on the component instance. The example\nabove is an image component, so \nPythonComponentHandle\n calls \nExampleImageFaceDetection.get_detections_from_image\n\non the component instance. The component instance creates an instance of\n\nmpf_component_util.ImageReader\n to access the image. Components that support video\nwould implement \nget_detections_from_video\n and use\n\nmpf_component_util.VideoCapture\n instead.\n\n\nThis figure show the structure when the mixin classes are used:\n\n\n\n\nThe figure above shows a video component, \nExampleVideoFaceDetection\n, that extends the\n\nmpf_component_util.VideoCaptureMixin\n class. \nPythonComponentHandle\n will\ncall \nget_detections_from_video\n on an instance of \nExampleVideoFaceDetection\n. \nExampleVideoFaceDetection\n does not\nimplement \nget_detections_from_video\n, so the implementation inherited from \nmpf_component_util.VideoCaptureMixin\n\ngets called. \nmpf_component_util.VideoCaptureMixin.get_detections_from_video\n creates an instance of\n\nmpf_component_util.VideoCapture\n and calls\n\nExampleVideoFaceDetection.get_detections_from_video_capture\n, passing in the \nmpf_component_util.VideoCapture\n it\njust created. \nExampleVideoFaceDetection.get_detections_from_video_capture\n is where the component reads the video\nusing the passed-in \nmpf_component_util.VideoCapture\n and attempts to find detections. Components that support images\nwould extend \nmpf_component_util.ImageReaderMixin\n, implement\n\nget_detections_from_image_reader\n, and access the image using the passed-in\n\nmpf_component_util.ImageReader\n.\n\n\nDuring component registration a \nvirtualenv\n is created for each component.\nThe virtualenv has access to the built-in Python libraries, but does not have access to any third party packages\nthat might be installed on the system. When creating the virtualenv for a setuptools-based component the only packages\nthat get installed are the component itself and any dependencies specified in the setup.py\nfile (including their transitive dependencies). When creating the virtualenv for a basic Python component the only\npackage that gets installed is \nmpf_component_api\n. \nmpf_component_api\n is the package containing the job classes\n(e.g. \nmpf_component_api.ImageJob\n,\n\nmpf_component_api.VideoJob\n) and detection result classes\n(e.g. \nmpf_component_api.ImageLocation\n,\n\nmpf_component_api.VideoTrack\n).\n\n\nHow to Create a Python Component\n\n\nThere are two types of Python components that are supported, setuptools-based components and basic Python components.\nBasic Python components are quicker to set up, but have no built-in support for dependency management.\nAll dependencies must be handled by the developer. Setuptools-based components are recommended since they use\nsetuptools and pip for dependency management.\n\n\nGet openmpf-python-component-sdk\n\n\nIn order to create a Python component you will need to clone the\n\nopenmpf-python-component-sdk repository\n if you don't\nalready have it. While not technically required, it is recommended to also clone the\n\nopenmpf-build-tools repository\n.\nThe rest of the steps assume you cloned openmpf-python-component-sdk to\n\n~/openmpf-projects/openmpf-python-component-sdk\n. The rest of the steps also assume that if you cloned the\nopenmpf-build-tools repository, you cloned it to \n~/openmpf-projects/openmpf-build-tools\n.\n\n\nSetup Python Component Libraries\n\n\nThe component packaging steps require that wheel files for \nmpf_component_api\n, \nmpf_component_util\n, and\ntheir dependencies are available in the \n~/mpf-sdk-install/python/wheelhouse\n directory.\n\n\nIf you have openmpf-build-tools, then you can run:\n\n\n~/openmpf-projects/openmpf-build-tools/build-openmpf-components/build_components.py -psdk ~/openmpf-projects/openmpf-python-component-sdk\n\n\n\n\nTo setup the libraries manually you can run:\n\n\npip3 wheel -w ~/mpf-sdk-install/python/wheelhouse ~/openmpf-projects/openmpf-python-component-sdk/detection/api\npip3 wheel -w ~/mpf-sdk-install/python/wheelhouse ~/openmpf-projects/openmpf-python-component-sdk/detection/component_util\n\n\n\n\nHow to Create a Setuptools-based Python Component\n\n\nIn this example we create a setuptools-based video component named \"MyComponent\". An example of a setuptools-based\nPython component can be found\n\nhere\n.\n\n\nThis is the recommended project structure:\n\n\nComponentName\n\u251c\u2500\u2500 setup.py\n\u251c\u2500\u2500 component_name\n\u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u2514\u2500\u2500 component_name.py\n\u2514\u2500\u2500 plugin-files\n    \u251c\u2500\u2500 descriptor\n    \u2502   \u2514\u2500\u2500 descriptor.json\n    \u2514\u2500\u2500 wheelhouse  # optional\n        \u2514\u2500\u2500 my_prebuilt_lib-0.1-py3-none-any.whl\n\n\n\n\n1. Create directory structure:\n\n\nmkdir MyComponent\nmkdir MyComponent/my_component\nmkdir -p MyComponent/plugin-files/descriptor\ntouch MyComponent/setup.py\ntouch MyComponent/my_component/__init__.py\ntouch MyComponent/my_component/my_component.py\ntouch MyComponent/plugin-files/descriptor/descriptor.json\n\n\n\n\n2. Create setup.py file in project's top-level directory:\n\n\nExample of a minimal setup.py file:\n\n\nimport setuptools\n\nsetuptools.setup(\n    name='MyComponent',\n    version='0.1',\n    packages=setuptools.find_packages(),\n    install_requires=(\n        'mpf_component_api\n=0.1',\n        'mpf_component_util\n=0.1'\n    ),\n    entry_points={\n        'mpf.exported_component': 'component = my_component.my_component:MyComponent'\n    }\n\n)\n\n\n\n\nThe \nname\n parameter defines the distribution name. Typically the distribution name matches the component name.\n\n\nAny dependencies that component requires should be listed in the \ninstall_requires\n field.\n\n\nThe component executor looks in the \nentry_points\n element and uses the \nmpf.exported_component\n field to determine\nthe component class. The right hand side of \ncomponent =\n should be the dotted module name, followed by a \n:\n,\nfollowed by the name of the class. The general pattern is\n\n'mpf.exported_component': 'component = \npackage_name\n.\nmodule_name\n:\nclass_name\n'\n. In the above example,\n\nMyComponent\n is the class name. The module is listed as \nmy_component.my_component\n because the \nmy_component\n\npackage contains the \nmy_component.py\n file and the \nmy_component.py\n file contains the \nMyComponent\n class.\n\n\n3. Create descriptor.json file in MyComponent/plugin-files/descriptor:\n\n\nThe \nbatchLibrary\n field should match the distribution name from the setup.py file. In this example the\nfield should be: \n\"batchLibrary\" : \"MyComponent\"\n.\nSee \nPackaging and Registering a Component\n for details about\nthe descriptor format.\n\n\n4. Implement your component class:\n\n\nBelow is an example of the structure of a simple component. This component extends\n\nmpf_component_util.VideoCaptureMixin\n to simplify the use of\n\nmpf_component_util.VideoCapture\n. You would replace the call to\n\nrun_detection_algorithm_on_frame\n with your component-specific logic.\n\n\nimport mpf_component_api as mpf\nimport mpf_component_util as mpf_util\n\nlogger = mpf.configure_logging('my-component.log', __name__ == '__main__')\n\nclass MyComponent(mpf_util.VideoCaptureMixin, object):\n    detection_type = 'FACE'\n\n    @staticmethod\n    def get_detections_from_video_capture(video_job, video_capture):\n        logger.info('[%s] Received video job: %s', video_job.job_name, video_job)\n        # If frame index is not required, you can just loop over video_capture directly\n        for frame_index, frame in enumerate(video_capture):\n            for result_track in run_detection_algorithm_on_frame(frame_index, frame):\n                # Alternatively, while iterating through the video, add tracks to a list. When done, return that list.\n                yield result_track\n\n\n\n\n5. Optional: Add prebuilt wheel files if not available on PyPi:\n\n\nIf your component depends on Python libraries that are not available on PyPi, the libraries can be manually added to\nyour project. The prebuilt libraries must be placed in your project's \nplugin-files/wheelhouse\n directory.\nThe prebuilt library names must be listed in your \nsetup.py\n file's \ninstall_requires\n field.\nIf any of the prebuilt libraries have transitive dependencies that are not available on PyPi, then those libraries\nmust also be added to your project's \nplugin-files/wheelhouse\n directory.\n\n\n6. Create the plugin package:\n\n\nThe directory structure of the .tar.gz file will be:\n\n\nMyComponent\n\u251c\u2500\u2500 descriptor\n\u2502   \u2514\u2500\u2500 descriptor.json\n\u2514\u2500\u2500 wheelhouse\n    \u251c\u2500\u2500 MyComponent-0.1-py3-none-any.whl\n    \u251c\u2500\u2500 mpf_component_api-0.1-py3-none-any.whl\n    \u251c\u2500\u2500 mpf_component_util-0.1-py3-none-any.whl\n    \u251c\u2500\u2500 numpy-1.18.4-cp38-cp38-manylinux1_x86_64.whl\n    \u2514\u2500\u2500 opencv_python-4.2.0.34-cp38-cp38-manylinux1_x86_64.whl\n\n\n\n\nTo create the plugin packages you can run the build script as follows:\n\n\n~/openmpf-projects/openmpf-build-tools/build-openmpf-components/build_components.py -psdk ~/openmpf-projects/openmpf-python-component-sdk -c MyComponent\n\n\n\n\nThe plugin package can also be built manually using the following commands:\n\n\nmkdir -p plugin-packages/MyComponent/wheelhouse\ncp -r MyComponent/plugin-files/* plugin-packages/MyComponent/\npip3 wheel -w plugin-packages/MyComponent/wheelhouse -f ~/mpf-sdk-install/python/wheelhouse -f plugin-packages/MyComponent/wheelhouse ./MyComponent/\ncd plugin-packages\ntar -zcf MyComponent.tar.gz MyComponent\n\n\n\n\nHow to Create a Basic Python Component\n\n\nIn this example we create a basic Python component that supports video. An example of a basic Python component can be\nfound\n\nhere\n.\n\n\nThis is the recommended project structure:\n\n\nComponentName\n\u251c\u2500\u2500 component_name.py\n\u251c\u2500\u2500 dependency.py\n\u2514\u2500\u2500 descriptor\n    \u2514\u2500\u2500 descriptor.json\n\n\n\n\n1. Create directory structure:\n\n\nmkdir MyComponent\nmkdir MyComponent/descriptor\ntouch MyComponent/descriptor/descriptor.json\ntouch MyComponent/my_component.py\n\n\n\n\n2. Create descriptor.json file in MyComponent/descriptor:\n\n\nThe \nbatchLibrary\n field should be the full path to the Python file containing your component class.\nIn this example the field should be: \n\"batchLibrary\" : \"${MPF_HOME}/plugins/MyComponent/my_component.py\"\n.\nSee \nPackaging and Registering a Component\n for details about\nthe descriptor format.\n\n\n3. Implement your component class:\n\n\nBelow is an example of the structure of a simple component that does not use\n\nmpf_component_util.VideoCaptureMixin\n. You would replace the call to\n\nrun_detection_algorithm\n with your component-specific logic.\n\n\nimport mpf_component_api as mpf\n\nlogger = mpf.configure_logging('my-component.log', __name__ == '__main__')\n\nclass MyComponent(object):\n    detection_type = 'FACE'\n\n    @staticmethod\n    def get_detections_from_video(video_job):\n        logger.info('[%s] Received video job: %s', video_job.job_name, video_job)\n        return run_detection_algorithm(video_job)\n\nEXPORT_MPF_COMPONENT = MyComponent\n\n\n\n\nThe component executor looks for a module-level variable named \nEXPORT_MPF_COMPONENT\n to specify which class\nis the component.\n\n\n4. Create the plugin package:\n\n\nThe directory structure of the .tar.gz file will be:\n\n\nComponentName\n\u251c\u2500\u2500 component_name.py\n\u251c\u2500\u2500 dependency.py\n\u2514\u2500\u2500 descriptor\n    \u2514\u2500\u2500 descriptor.json\n\n\n\n\nTo create the plugin packages you can run the build script as follows:\n\n\n~/openmpf-projects/openmpf-build-tools/build-openmpf-components/build_components.py -c MyComponent\n\n\n\n\nThe plugin package can also be built manually using the following command:\n\n\ntar -zcf MyComponent.tar.gz MyComponent\n\n\n\n\nAPI Specification\n\n\nAn OpenMPF Python component is a class that defines one or more of the get_detections_from_* methods and has a\n\ndetection_type\n field.\n\n\ncomponent.get_detections_from_* methods\n\n\nAll get_detections_from_* methods are invoked through an instance of the component class. The only parameter passed\nin is an appropriate job object (e.g. \nmpf_component_api.ImageJob\n, \nmpf_component_api.VideoJob\n). Since the methods\nare invoked through an instance, instance methods and class methods end up with two arguments, the first is either the\ninstance or the class, respectively. All get_detections_from_* methods can be implemented either as an instance method,\na static method, or a class method.\nFor example:\n\n\ninstance method:\n\n\nclass MyComponent(object):\n    def get_detections_from_image(self, image_job):\n        return [mpf_component_api.ImageLocation(...), ...]\n\n\n\n\nstatic method:\n\n\nclass MyComponent(object):\n    @staticmethod\n    def get_detections_from_image(image_job):\n        return [mpf_component_api.ImageLocation(...), ...]\n\n\n\n\nclass method:\n\n\nclass MyComponent(object):\n    @classmethod\n    def get_detections_from_image(cls, image_job):\n        return [mpf_component_api.ImageLocation(...), ...]\n\n\n\n\nAll get_detections_from_* methods must return an iterable of the appropriate detection type\n(e.g. \nmpf_component_api.ImageLocation\n, \nmpf_component_api.VideoTrack\n). The return value is normally a list or generator,\nbut any iterable can be used.\n\n\ncomponent.detection_type\n\n\n\n\nstr\n field describing the type of object that is detected by the component. Should be in all CAPS.\nExamples include: \nFACE\n, \nMOTION\n, \nPERSON\n, \nSPEECH\n, \nCLASS\n (for object classification), or \nTEXT\n.\n\n\nExample:\n\n\n\n\nclass MyComponent(object):\n    detection_type = 'FACE'\n\n\n\n\n\nImage API\n\n\ncomponent.get_detections_from_image(image_job)\n\n\nUsed to detect objects in an image file.\n\n\n\n\nMethod Definition:\n\n\n\n\nclass MyComponent(object):\n    def get_detections_from_image(self, image_job):\n        return [mpf_component_api.ImageLocation(...), ...]\n\n\n\n\nget_detections_from_image\n, like all get_detections_from_* methods, can be implemented either as an instance method,\na static method, or a class method.\n\n\n\n\nParameters:\n\n\n\n\n\n\n\n\n\n\nParameter\n\n\nData Type\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nimage_job\n\n\nmpf_component_api.ImageJob\n\n\nObject containing details about the work to be performed.\n\n\n\n\n\n\n\n\n\n\nReturns: An iterable of \nmpf_component_api.ImageLocation\n\n\n\n\nmpf_component_api.ImageJob\n\n\nClass containing data used for detection of objects in an image file.\n\n\n\n\nMembers:\n\n\n\n\n\n  \n\n    \n\n      \nMember\n\n      \nData Type\n\n      \nDescription\n\n    \n\n  \n\n  \n\n    \n\n      \njob_name\n\n      \nstr\n\n      \nA specific name given to the job by the OpenMPF framework. This value may be used, for example, for logging and debugging purposes.\n\n    \n\n    \n\n      \ndata_uri\n\n      \nstr\n\n      \nThe URI of the input media file to be processed. Currently, this is a file path. For example, \"/opt/mpf/share/remote-media/test-file.jpg\".\n\n    \n\n    \n\n      \njob_properties\n\n      \ndict[str, str]\n\n      \n\n        Contains a dict with keys and values of type \nstr\n which represent the property name and the property value. The key corresponds to the property name specified in the component descriptor file described in \nPackaging and Registering a Component\n. Values are determined when creating a pipeline or when submitting a job.\n        \n\n        Note: The job_properties dict may not contain the full set of job properties. For properties not contained in the dict, the component must use a default value.\n      \n\n    \n\n    \n\n      \nmedia_properties\n\n      \ndict[str, str]\n\n      \n\n        Contains a dict with keys and values of type \nstr\n of metadata about the media associated with the job.\n        \n\n        Includes the following key-value pairs:\n        \n\n          \nMIME_TYPE\n : the MIME type of the media\n\n          \nFRAME_WIDTH\n : the width of the image in pixels\n\n          \nFRAME_HEIGHT\n : the height of the image in pixels\n\n        \n\n        May include the following key-value pairs:\n        \n\n          \nROTATION\n : A floating point value in the interval \n[0.0, 360.0)\n indicating the orientation of the media in degrees in the counter-clockwise direction. In order to view the media in the upright orientation, it must be rotated the given number of degrees in the clockwise direction.\n\n          \nHORIZONTAL_FLIP\n : true if the image is mirrored across the Y-axis, otherwise false\n\n          \nEXIF_ORIENTATION\n : the standard EXIF orientation tag; a value between 1 and 8\n\n        \n\n      \n\n    \n\n    \n\n      \nfeed_forward_location\n\n      \nNone\n or \nmpf_component_api.ImageLocation\n\n      \nAn \nmpf_component_api.ImageLocation\n from the previous pipeline stage. Provided when feed forward is enabled. See \nFeed Forward Guide\n.\n\n    \n\n  \n\n\n\n\n\nmpf_component_api.ImageLocation\n\n\nClass used to store the location of detected objects in a image file.\n\n\n\n\nConstructor:\n\n\n\n\ndef __init__(self, x_left_upper, y_left_upper, width, height, confidence=-1.0, detection_properties=None):\n    ...\n\n\n\n\n\n\nMembers:\n\n\n\n\n\n\n\n\n\n\nMember\n\n\nData Type\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nx_left_upper\n\n\nint\n\n\nUpper left X coordinate of the detected object.\n\n\n\n\n\n\ny_left_upper\n\n\nint\n\n\nUpper left Y coordinate of the detected object.\n\n\n\n\n\n\nwidth\n\n\nint\n\n\nThe width of the detected object.\n\n\n\n\n\n\nheight\n\n\nint\n\n\nThe height of the detected object.\n\n\n\n\n\n\nconfidence\n\n\nfloat\n\n\nRepresents the \"quality\" of the detection. The range depends on the detection algorithm. 0.0 is lowest quality. Higher values are higher quality. Using a standard range of [0.0 - 1.0] is advised. If the component is unable to supply a confidence value, it should return -1.0.\n\n\n\n\n\n\ndetection_properties\n\n\ndict[str, str]\n\n\nA dict with keys and values of type \nstr\n containing optional additional information about the detected object. For best practice, keys should be in all CAPS.\n\n\n\n\n\n\n\n\nSee here for information about rotation and horizontal flipping.\n\n\n\n\nExample:\n\n\n\n\nA component that performs generic object classification can add an entry to \ndetection_properties\n where the key is\n\nCLASSIFICATION\n and the value is the type of object detected.\n\n\nmpf_component_api.ImageLocation(0, 0, 100, 100, 1.0, {'CLASSIFICATION': 'backpack'})\n\n\n\n\nmpf_component_util.ImageReader\n\n\nmpf_component_util.ImageReader\n is a utility class for accessing images. It is the image equivalent to\n\nmpf_component_util.VideoCapture\n. Like \nmpf_component_util.VideoCapture\n,\nit may modify the read-in frame data based on job_properties. From the point of view of someone using\n\nmpf_component_util.ImageReader\n, these modifications are mostly transparent. \nmpf_component_util.ImageReader\n makes\nit look like you are reading the original image file as though it has already been rotated, flipped, cropped, etc.\n\n\nOne issue with this approach is that the detection bounding boxes will be relative to the\nmodified frame data, not the original. To make the detections relative to the original image\nthe \nmpf_component_util.ImageReader.reverse_transform(image_location)\n method must be called on each\n\nmpf_component_api.ImageLocation\n. Since the use of \nmpf_component_util.ImageReader\n is optional, the framework\ncannot automatically perform the reverse transform for the developer.\n\n\nThe general pattern for using \nmpf_component_util.ImageReader\n is as follows:\n\n\nclass MyComponent(object):\n\n    @staticmethod\n    def get_detections_from_image(image_job):\n        image_reader = mpf_component_util.ImageReader(image_job)\n        image = image_reader.get_image()\n        # run_component_specific_algorithm is a placeholder for this example.\n        # Replace run_component_specific_algorithm with your component's detection logic\n        result_image_locations = run_component_specific_algorithm(image)\n        for result in result_image_locations:\n            image_reader.reverse_transform(result)\n            yield result\n\n\n\n\nAlternatively, see the documentation for \nmpf_component_util.ImageReaderMixin\n for a more concise way to use\n\nmpf_component_util.ImageReader\n below.\n\n\nmpf_component_util.ImageReaderMixin\n\n\nA mixin class that can be used to simplify the usage of \nmpf_component_util.ImageReader\n.\n\nmpf_component_util.ImageReaderMixin\n takes care of initializing a \nmpf_component_util.ImageReader\n and\nperforming the reverse transform.\n\n\nThere are some requirements to properly use \nmpf_component_util.ImageReaderMixin\n:\n\n\n\n\nThe component must extend \nmpf_component_util.ImageReaderMixin\n.\n\n\nThe component must implement \nget_detections_from_image_reader(image_job, image_reader)\n.\n\n\nThe component must read the image using the \nmpf_component_util.ImageReader\n\n  that is passed in to \nget_detections_from_image_reader(image_job, image_reader)\n.\n\n\nThe component must NOT implement \nget_detections_from_image(image_job)\n.\n\n\nThe component must NOT call \nmpf_component_util.ImageReader.reverse_transform\n.\n\n\n\n\nThe general pattern for using \nmpf_component_util.ImageReaderMixin\n is as follows:\n\n\nclass MyComponent(mpf_component_util.ImageReaderMixin, object):\n\n    @staticmethod # Can also be a regular instance method or a class method\n    def get_detections_from_image_reader(image_job, image_reader):\n        image = image_reader.get_image()\n\n        # run_component_specific_algorithm is a placeholder for this example.\n        # Replace run_component_specific_algorithm with your component's detection logic\n        return run_component_specific_algorithm(image)\n\n\n\n\nmpf_component_util.ImageReaderMixin\n is a mixin class so it is designed in a way that does not prevent the subclass\nfrom extending other classes. If a component supports both videos and images, and it uses\n\nmpf_component_util.VideoCaptureMixin\n, it should also use\n\nmpf_component_util.ImageReaderMixin\n.\n\n\nVideo API\n\n\ncomponent.get_detections_from_video(video_job)\n\n\nUsed to detect objects in a video file. Prior to being sent to the component, videos are split into logical \"segments\"\nof video data and each segment (containing a range of frames) is assigned to a different job. Components are not\nguaranteed to receive requests in any order. For example, the first request processed by a component might receive a\nrequest for frames 300-399 of a Video A, while the next request may cover frames 900-999 of a Video B.\n\n\n\n\nMethod Definition:\n\n\n\n\nclass MyComponent(object):\n    def get_detections_from_video(self, video_job):\n        return [mpf_component_api.VideoTrack(...), ...]\n\n\n\n\nget_detections_from_video\n, like all get_detections_from_* methods, can be implemented either as an instance method,\na static method, or a class method.\n\n\n\n\nParameters:\n\n\n\n\n\n\n\n\n\n\nParameter\n\n\nData Type\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nvideo_job\n\n\nmpf_component_api.VideoJob\n\n\nObject containing details about the work to be performed.\n\n\n\n\n\n\n\n\n\n\nReturns: An iterable of \nmpf_component_api.VideoTrack\n\n\n\n\nmpf_component_api.VideoJob\n\n\nClass containing data used for detection of objects in a video file.\n\n\n\n\nMembers:\n\n\n\n\n\n  \n\n    \n\n      \nMember\n\n      \nData Type\n\n      \nDescription\n\n    \n\n  \n\n  \n\n    \n\n      \njob_name\n\n      \nstr\n\n      \nA specific name given to the job by the OpenMPF framework. This value may be used, for example, for logging and debugging purposes.\n\n    \n\n    \n\n      \ndata_uri\n\n      \nstr\n\n      \nThe URI of the input media file to be processed. Currently, this is a file path. For example, \"/opt/mpf/share/remote-media/test-file.avi\".\n\n    \n\n    \n\n      \nstart_frame\n\n      \nint\n\n      \nThe first frame number (0-based index) of the video that should be processed to look for detections.\n\n    \n    \n    \n\n      \nstop_frame\n\n      \nint\n\n      \nThe last frame number (0-based index) of the video that should be processed to look for detections.\n\n    \n    \n    \n\n      \njob_properties\n\n      \ndict[str, str]\n\n      \n\n        Contains a dict with keys and values of type \nstr\n which represent the property name and the property value. The key corresponds to the property name specified in the component descriptor file described in \nPackaging and Registering a Component\n. Values are determined when creating a pipeline or when submitting a job.\n        \n\n        Note: The job_properties dict may not contain the full set of job properties. For properties not contained in the dict, the component must use a default value.\n      \n\n    \n\n    \n\n      \nmedia_properties\n\n      \ndict[str, str]\n\n      \n\n        Contains a dict with keys and values of type \nstr\n of metadata about the media associated with the job.\n        \n\n        Includes the following key-value pairs:\n        \n\n          \nDURATION\n : length of video in milliseconds\n\n          \nFPS\n : frames per second (averaged for variable frame rate video)\n\n          \nFRAME_COUNT\n : the number of frames in the video\n\n          \nMIME_TYPE\n : the MIME type of the media\n\n          \nFRAME_WIDTH\n : the width of a frame in pixels\n\n          \nFRAME_HEIGHT\n : the height of a frame in pixels\n\n        \n\n        May include the following key-value pair:\n        \n\n          \nROTATION\n : A floating point value in the interval \n[0.0, 360.0)\n indicating the orientation of the media in degrees in the counter-clockwise direction. In order to view the media in the upright orientation, it must be rotated the given number of degrees in the clockwise direction.\n\n        \n\n      \n\n    \n\n    \n\n      \nfeed_forward_track\n\n      \nNone\n or \nmpf_component_api.VideoTrack\n\n      \nAn \nmpf_component_api.VideoTrack\n from the previous pipeline stage. Provided when feed forward is enabled. See \nFeed Forward Guide\n.\n\n    \n\n  \n\n\n\n\n\n\n\nIMPORTANT:\n \nFRAME_INTERVAL\n is a common job property that many components support.\nFor frame intervals greater than 1, the component must look for detections starting with the first\nframe, and then skip frames as specified by the frame interval, until or before it reaches the stop frame.\nFor example, given a start frame of 0, a stop frame of 99, and a frame interval of 2, then the detection component\nmust look for objects in frames numbered 0, 2, 4, 6, ..., 98.\n\n\n\n\nmpf_component_api.VideoTrack\n\n\nClass used to store the location of detected objects in a video file.\n\n\n\n\nConstructor:\n\n\n\n\ndef __init__(self, start_frame, stop_frame, confidence=-1.0, frame_locations=None, detection_properties=None):\n    ...\n\n\n\n\n\n\nMembers:\n\n\n\n\n\n\n\n\n\n\nMember\n\n\nData Type\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nstart_frame\n\n\nint\n\n\nThe first frame number (0-based index) that contained the detected object.\n\n\n\n\n\n\nstop_frame\n\n\nint\n\n\nThe last frame number (0-based index) that contained the detected object.\n\n\n\n\n\n\nconfidence\n\n\nfloat\n\n\nRepresents the \"quality\" of the detection. The range depends on the detection algorithm. 0.0 is lowest quality. Higher values are higher quality. Using a standard range of [0.0 - 1.0] is advised. If the component is unable to supply a confidence value, it should return -1.0.\n\n\n\n\n\n\nframe_locations\n\n\ndict[int, mpf_component_api.ImageLocation]\n\n\nA dict of individual detections. The key for each entry is the frame number where the detection was generated, and the value is a \nmpf_component_api.ImageLocation\n calculated as if that frame was a still image. Note that a key-value pair is \nnot\n required for every frame between the track start frame and track stop frame.\n\n\n\n\n\n\ndetection_properties\n\n\ndict[str, str]\n\n\nA dict with keys and values of type \nstr\n containing optional additional information about the detected object. For best practice, keys should be in all CAPS.\n\n\n\n\n\n\n\n\n\n\nNOTE:\n Currently, \nmpf_component_api.VideoTrack.detection_properties\n do not show up in the JSON output object or\nare used by the WFM in any way.\n\n\n\n\n\n\nExample:\n\n\n\n\nA component that performs generic object classification can add an entry to \ndetection_properties\n where the key is\n\nCLASSIFICATION\n and the value is the type of object detected.\n\n\ntrack = mpf_component_api.VideoTrack(0, 1)\ntrack.frame_locations[0] = mpf_component_api.ImageLocation(0, 0, 100, 100, 0.75, {'CLASSIFICATION': 'backpack'})\ntrack.frame_locations[1] = mpf_component_api.ImageLocation(10, 10, 110, 110, 0.95, {'CLASSIFICATION': 'backpack'})\ntrack.confidence = max(il.confidence for il in track.frame_locations.itervalues())\n\n\n\n\nmpf_component_util.VideoCapture\n\n\nmpf_component_util.VideoCapture\n is a utility class for reading videos. \nmpf_component_util.VideoCapture\n works very\nsimilarly to \ncv2.VideoCapture\n, except that it might modify the video frames based on job properties. From the point\nof view of someone using \nmpf_component_util.VideoCapture\n, these modifications are mostly transparent.\n\nmpf_component_util.VideoCapture\n makes it look like you are reading the original video file as though it has already\nbeen rotated, flipped, cropped, etc. Also, if frame skipping is enabled, such as by setting the value of the\n\nFRAME_INTERVAL\n job property, it makes it look like you are reading the video as though it never contained the\nskipped frames.\n\n\nOne issue with this approach is that the detection frame numbers and bounding box will be relative to the\nmodified video, not the original. To make the detections relative to the original video\nthe \nmpf_component_util.VideoCapture.reverse_transform(video_track)\n method must be called on each\n\nmpf_component_api.VideoTrack\n. Since the use of \nmpf_component_util.VideoCapture\n is optional, the framework\ncannot automatically perform the reverse transform for the developer.\n\n\nThe general pattern for using \nmpf_component_util.VideoCapture\n is as follows:\n\n\nclass MyComponent(object):\n\n    @staticmethod\n    def get_detections_from_video(video_job):\n        video_capture = mpf_component_util.VideoCapture(video_job)\n        # If frame index is not required, you can just loop over video_capture directly\n        for frame_index, frame in enumerate(video_capture):\n            # run_component_specific_algorithm is a placeholder for this example.\n            # Replace run_component_specific_algorithm with your component's detection logic\n            result_tracks = run_component_specific_algorithm(frame_index, frame)\n            for track in result_tracks:\n                video_capture.reverse_transform(track)\n                yield track\n\n\n\n\nAlternatively, see the documentation for \nmpf_component_util.VideoCaptureMixin\n for a more concise way to use\n\nmpf_component_util.VideoCapture\n below.\n\n\nmpf_component_util.VideoCaptureMixin\n\n\nA mixin class that can be used to simplify the usage of \nmpf_component_util.VideoCapture\n.\n\nmpf_component_util.VideoCaptureMixin\n takes care of initializing a \nmpf_component_util.VideoCapture\n and\nperforming the reverse transform.\n\n\nThere are some requirements to properly use \nmpf_component_util.VideoCaptureMixin\n:\n\n\n\n\nThe component must extend \nmpf_component_util.VideoCaptureMixin\n.\n\n\nThe component must implement \nget_detections_from_video_capture(video_job, video_capture)\n.\n\n\nThe component must read the video using the \nmpf_component_util.VideoCapture\n\n  that is passed in to \nget_detections_from_video_capture(video_job, video_capture)\n.\n\n\nThe component must NOT implement \nget_detections_from_video(video_job)\n.\n\n\nThe component must NOT call \nmpf_component_util.VideoCapture.reverse_transform\n.\n\n\n\n\nThe general pattern for using \nmpf_component_util.VideoCaptureMixin\n is as follows:\n\n\nclass MyComponent(mpf_component_util.VideoCaptureMixin, object):\n\n    @staticmethod # Can also be a regular instance method or a class method\n    def get_detections_from_video_capture(video_job, video_capture):\n        # If frame index is not required, you can just loop over video_capture directly\n        for frame_index, frame in enumerate(video_capture):\n            # run_component_specific_algorithm is a placeholder for this example.\n            # Replace run_component_specific_algorithm with your component's detection logic\n            result_tracks = run_component_specific_algorithm(frame_index, frame)\n            for track in result_tracks:\n                # Alternatively, while iterating through the video, add tracks to a list. When done, return that list.\n                yield track\n\n\n\n\nmpf_component_util.VideoCaptureMixin\n is a mixin class so it is designed in a way that does not prevent the subclass\nfrom extending other classes. If a component supports both videos and images, and it uses\n\nmpf_component_util.VideoCaptureMixin\n, it should also use\n\nmpf_component_util.ImageReaderMixin\n.\nFor example:\n\n\nclass MyComponent(mpf_component_util.VideoCaptureMixin, mpf_component_util.ImageReaderMixin, object):\n\n    @staticmethod\n    def get_detections_from_video_capture(video_job, video_capture):\n        ...\n\n    @staticmethod\n    def get_detections_from_image_reader(image_job, image_reader):\n       ...\n\n\n\n\nAudio API\n\n\ncomponent.get_detections_from_audio(audio_job)\n\n\nUsed to detect objects in an audio file.\n\n\n\n\nMethod Definition:\n\n\n\n\nclass MyComponent(object):\n    def get_detections_from_audio(self, audio_job):\n        return [mpf_component_api.AudioTrack(...), ...]\n\n\n\n\nget_detections_from_audio\n, like all get_detections_from_* methods, can be implemented either as an instance method,\na static method, or a class method.\n\n\n\n\nParameters:\n\n\n\n\n\n\n\n\n\n\nParameter\n\n\nData Type\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\naudio_job\n\n\nmpf_component_api.AudioJob\n\n\nObject containing details about the work to be performed.\n\n\n\n\n\n\n\n\n\n\nReturns: An iterable of \nmpf_component_api.AudioTrack\n\n\n\n\nmpf_component_api.AudioJob\n\n\nClass containing data used for detection of objects in an audio file.\nCurrently, audio files are not logically segmented, so a job will contain the entirety of the audio file.\n\n\n\n\nMembers:\n\n\n\n\n\n  \n\n    \n\n      \nMember\n\n      \nData Type\n\n      \nDescription\n\n    \n\n  \n\n  \n\n    \n\n      \njob_name\n\n      \nstr\n\n      \nA specific name given to the job by the OpenMPF framework. This value may be used, for example, for logging and debugging purposes.\n\n    \n\n    \n\n      \ndata_uri\n\n      \nstr\n\n      \nThe URI of the input media file to be processed. Currently, this is a file path. For example, \"/opt/mpf/share/remote-media/test-file.mp3\".\n\n    \n\n    \n\n      \nstart_time\n\n      \nint\n\n      \nThe time (0-based index, in milliseconds) associated with the beginning of the segment of the audio file that should be processed to look for detections.\n\n    \n\n    \n\n      \nstop_time\n\n      \nint\n\n      \nThe time (0-based index, in milliseconds) associated with the end of the segment of the audio file that should be processed to look for detections.\n\n    \n        \n    \n\n      \njob_properties\n\n      \ndict[str, str]\n\n      \n\n        Contains a dict with keys and values of type \nstr\n which represent the property name and the property value. The key corresponds to the property name specified in the component descriptor file described in \nPackaging and Registering a Component\n. Values are determined when creating a pipeline or when submitting a job.\n        \n\n        Note: The job_properties dict may not contain the full set of job properties. For properties not contained in the dict, the component must use a default value.\n      \n\n    \n\n    \n\n      \nmedia_properties\n\n      \ndict[str, str]\n\n      \n\n        Contains a dict with keys and values of type \nstr\n of metadata about the media associated with the job.\n        \n\n        Includes the following key-value pairs:\n        \n\n          \nDURATION\n : length of audio file in milliseconds\n\n          \nMIME_TYPE\n : the MIME type of the media\n\n        \n\n      \n\n    \n\n    \n\n      \nfeed_forward_track\n\n      \nNone\n or \nmpf_component_api.AudioTrack\n\n      \nAn \nmpf_component_api.AudioTrack\n from the previous pipeline stage. Provided when feed forward is enabled. See \nFeed Forward Guide\n.\n\n    \n\n  \n\n\n\n\n\nmpf_component_api.AudioTrack\n\n\nClass used to store the location of detected objects in an audio file.\n\n\n\n\nConstructor:\n\n\n\n\ndef __init__(self, start_time, stop_time, confidence, detection_properties=None):\n    ...\n\n\n\n\n\n\nMembers:\n\n\n\n\n\n\n\n\n\n\nMember\n\n\nData Type\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nstart_time\n\n\nint\n\n\nThe time (0-based index, in ms) when the audio detection event started.\n\n\n\n\n\n\nstop_time\n\n\nint\n\n\nThe time (0-based index, in ms) when the audio detection event stopped.\n\n\n\n\n\n\nconfidence\n\n\nfloat\n\n\nRepresents the \"quality\" of the detection. The range depends on the detection algorithm. 0.0 is lowest quality. Higher values are higher quality. Using a standard range of [0.0 - 1.0] is advised. If the component is unable to supply a confidence value, it should return -1.0.\n\n\n\n\n\n\ndetection_properties\n\n\ndict[str, str]\n\n\nA dict with keys and values of type \nstr\n containing optional additional information about the detected object. For best practice, keys should be in all CAPS.\n\n\n\n\n\n\n\n\n\n\nNOTE:\n Currently, \nmpf_component_api.AudioTrack.detection_properties\n do not show up in the JSON output object or\nare used by the WFM in any way.\n\n\n\n\nGeneric API\n\n\ncomponent.get_detections_from_generic(generic_job)\n\n\nUsed to detect objects in files that are not video, image, or audio files. Such files are of the UNKNOWN type and\nhandled generically.\n\n\n\n\nMethod Definition:\n\n\n\n\nclass MyComponent(object):\n    def get_detections_from_generic(self, generic_job):\n        return [mpf_component_api.GenericTrack(...), ...]\n\n\n\n\nget_detections_from_generic\n, like all get_detections_from_* methods, can be implemented either as an instance method,\na static method, or a class method.\n\n\n\n\nParameters:\n\n\n\n\n\n\n\n\n\n\nParameter\n\n\nData Type\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\ngeneric_job\n\n\nmpf_component_api.GenericJob\n\n\nObject containing details about the work to be performed.\n\n\n\n\n\n\n\n\n\n\nReturns: An iterable of \nmpf_component_api.GenericTrack\n\n\n\n\nmpf_component_api.GenericJob\n\n\nClass containing data used for detection of objects in a file that isn't a video, image, or audio file. The file is not\nlogically segmented, so a job will contain the entirety of the file.\n\n\n\n\nMembers:\n\n\n\n\n\n  \n\n    \n\n      \nMember\n\n      \nData Type\n\n      \nDescription\n\n    \n\n  \n\n  \n\n    \n\n      \njob_name\n\n      \nstr\n\n      \nA specific name given to the job by the OpenMPF framework. This value may be used, for example, for logging and debugging purposes.\n\n    \n\n    \n\n      \ndata_uri\n\n      \nstr\n\n      \nThe URI of the input media file to be processed. Currently, this is a file path. For example, \"/opt/mpf/share/remote-media/test-file.txt\".\n\n    \n\n    \n\n      \njob_properties\n\n      \ndict[str, str]\n\n      \n\n        Contains a dict with keys and values of type \nstr\n which represent the property name and the property value. The key corresponds to the property name specified in the component descriptor file described in \nPackaging and Registering a Component\n. Values are determined when creating a pipeline or when submitting a job.\n        \n\n        Note: The job_properties dict may not contain the full set of job properties. For properties not contained in the dict, the component must use a default value.\n      \n\n    \n\n    \n\n      \nmedia_properties\n\n      \ndict[str, str]\n\n      \n\n        Contains a dict with keys and values of type \nstr\n of metadata about the media associated with the job.\n        \n\n        Includes the following key-value pair:\n        \n\n          \nMIME_TYPE\n : the MIME type of the media\n\n        \n\n      \n\n    \n\n    \n\n      \nfeed_forward_track\n\n      \nNone\n or \nmpf_component_api.GenericTrack\n\n      \nAn \nmpf_component_api.GenericTrack\n from the previous pipeline stage. Provided when feed forward is enabled. See \nFeed Forward Guide\n.\n\n    \n\n  \n\n\n\n\n\nmpf_component_api.GenericTrack\n\n\nClass used to store the location of detected objects in a file that is not a video, image, or audio file.\n\n\n\n\nConstructor:\n\n\n\n\ndef __init__(self, confidence=-1.0, detection_properties=None):\n    ...\n\n\n\n\n\n\nMembers:\n\n\n\n\n\n\n\n\n\n\nMember\n\n\nData Type\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nconfidence\n\n\nfloat\n\n\nRepresents the \"quality\" of the detection. The range depends on the detection algorithm. 0.0 is lowest quality. Higher values are higher quality. Using a standard range of [0.0 - 1.0] is advised. If the component is unable to supply a confidence value, it should return -1.0.\n\n\n\n\n\n\ndetection_properties\n\n\ndict[str, str]\n\n\nA dict with keys and values of type \nstr\n containing optional additional information about the detected object. For best practice, keys should be in all CAPS.\n\n\n\n\n\n\n\n\nPython Component Build Environment\n\n\nAll Python components must work with CPython 3.8.2. Also, Python components must work with the Linux version that is\nused by the OpenMPF Component Executable. At this writing, OpenMPF runs on CentOS 7.4.1708 (kernel version 3.10.0-693).\nPure Python code should work on any OS, but incompatibility issues can arise when using Python libraries that include\ncompiled extension modules. Python libraries are typically distributed as wheel files. The wheel format requires that\nthe file name follows the pattern of\n\ndist_name\n-\nversion\n-\npython_tag\n-\nabi_tag\n-\nplatform_tag\n.whl\n. \npython_tag\n-\nabi_tag\n-\nplatform_tag\n are called\n\ncompatibility tags\n. For example, \nmpf_component_api\n is pure Python,\nso the name of its wheel file is \nmpf_component_api-0.1-py3-none-any.whl\n. \npy3\n means it will work with any Python 3\nimplementation because it does not use any implementation-specific features. \nnone\n means that it does not use the\nPython ABI. \nany\n means it will work on any platform.\n\n\nThe following combinations of compatibility tags are supported:\n\n\n\n\ncp32-abi3-linux_x86_64\n\n\ncp32-abi3-manylinux1_x86_64\n\n\ncp32-abi3-manylinux2010_x86_64\n\n\ncp32-abi3-manylinux2014_x86_64\n\n\ncp33-abi3-linux_x86_64\n\n\ncp33-abi3-manylinux1_x86_64\n\n\ncp33-abi3-manylinux2010_x86_64\n\n\ncp33-abi3-manylinux2014_x86_64\n\n\ncp34-abi3-linux_x86_64\n\n\ncp34-abi3-manylinux1_x86_64\n\n\ncp34-abi3-manylinux2010_x86_64\n\n\ncp34-abi3-manylinux2014_x86_64\n\n\ncp35-abi3-linux_x86_64\n\n\ncp35-abi3-manylinux1_x86_64\n\n\ncp35-abi3-manylinux2010_x86_64\n\n\ncp35-abi3-manylinux2014_x86_64\n\n\ncp36-abi3-linux_x86_64\n\n\ncp36-abi3-manylinux1_x86_64\n\n\ncp36-abi3-manylinux2010_x86_64\n\n\ncp36-abi3-manylinux2014_x86_64\n\n\ncp37-abi3-linux_x86_64\n\n\ncp37-abi3-manylinux1_x86_64\n\n\ncp37-abi3-manylinux2010_x86_64\n\n\ncp37-abi3-manylinux2014_x86_64\n\n\ncp38-abi3-linux_x86_64\n\n\ncp38-abi3-manylinux1_x86_64\n\n\ncp38-abi3-manylinux2010_x86_64\n\n\ncp38-abi3-manylinux2014_x86_64\n\n\ncp38-cp38-linux_x86_64\n\n\ncp38-cp38-manylinux1_x86_64\n\n\ncp38-cp38-manylinux2010_x86_64\n\n\ncp38-cp38-manylinux2014_x86_64\n\n\ncp38-none-any\n\n\ncp38-none-linux_x86_64\n\n\ncp38-none-manylinux1_x86_64\n\n\ncp38-none-manylinux2010_x86_64\n\n\ncp38-none-manylinux2014_x86_64\n\n\npy30-none-any\n\n\npy30-none-linux_x86_64\n\n\npy30-none-manylinux1_x86_64\n\n\npy30-none-manylinux2010_x86_64\n\n\npy30-none-manylinux2014_x86_64\n\n\npy31-none-any\n\n\npy31-none-linux_x86_64\n\n\npy31-none-manylinux1_x86_64\n\n\npy31-none-manylinux2010_x86_64\n\n\npy31-none-manylinux2014_x86_64\n\n\npy32-none-any\n\n\npy32-none-linux_x86_64\n\n\npy32-none-manylinux1_x86_64\n\n\npy32-none-manylinux2010_x86_64\n\n\npy32-none-manylinux2014_x86_64\n\n\npy33-none-any\n\n\npy33-none-linux_x86_64\n\n\npy33-none-manylinux1_x86_64\n\n\npy33-none-manylinux2010_x86_64\n\n\npy33-none-manylinux2014_x86_64\n\n\npy34-none-any\n\n\npy34-none-linux_x86_64\n\n\npy34-none-manylinux1_x86_64\n\n\npy34-none-manylinux2010_x86_64\n\n\npy34-none-manylinux2014_x86_64\n\n\npy35-none-any\n\n\npy35-none-linux_x86_64\n\n\npy35-none-manylinux1_x86_64\n\n\npy35-none-manylinux2010_x86_64\n\n\npy35-none-manylinux2014_x86_64\n\n\npy36-none-any\n\n\npy36-none-linux_x86_64\n\n\npy36-none-manylinux1_x86_64\n\n\npy36-none-manylinux2010_x86_64\n\n\npy36-none-manylinux2014_x86_64\n\n\npy37-none-any\n\n\npy37-none-linux_x86_64\n\n\npy37-none-manylinux1_x86_64\n\n\npy37-none-manylinux2010_x86_64\n\n\npy37-none-manylinux2014_x86_64\n\n\npy38-none-any\n\n\npy38-none-linux_x86_64\n\n\npy38-none-manylinux1_x86_64\n\n\npy38-none-manylinux2010_x86_64\n\n\npy38-none-manylinux2014_x86_64\n\n\npy3-none-any\n\n\npy3-none-linux_x86_64\n\n\npy3-none-manylinux1_x86_64\n\n\npy3-none-manylinux2010_x86_64\n\n\npy3-none-manylinux2014_x86_64\n\n\n\n\nComponents should be supplied as a tar file, which includes not only the component library, but any other libraries or\nfiles needed for execution. This includes all other non-standard libraries used by the component\n(aside from the standard Python libraries), and any configuration or data files.\n\n\nComponent Development Best Practices\n\n\nSingle-threaded Operation\n\n\nImplementations are encouraged to operate in single-threaded mode. OpenMPF will parallelize components through\nmultiple instantiations of the component, each running as a separate service.\n\n\nStateless Behavior\n\n\nOpenMPF components should be stateless in operation and give identical output for a provided input\n(i.e. when processing the same job).\n\n\nLogging\n\n\nIt recommended that components use the logger returned from:\n\n \nmpf_component_api.configure_logging(log_file_name, debug=False, replace_existing_config=True)\n. \nThe logger will write log messages to standard out. When \ndebug\n is false, the log messages will also be written to \n${MPF_LOG_PATH}/${THIS_MPF_NODE}/log/\nlog_file_name\n.log\n Note that multiple instances of the same component \ncan log to the same file. Also, logging content can span multiple lines. The following log levels are supported: \n\nFATAL, ERROR, WARN, INFO, DEBUG\n.\n\n\nThe format of the log messages is:\n\n\nDATE TIME LEVEL [SOURCE_FILE:LINE_NUMBER] - MESSAGE\n\n\n\n\nFor example:\n\n\n2018-05-03 14:41:11,703 INFO  [test_component.py:44] - Logged message", 
            "title": "Python Batch Component API"
        }, 
        {
            "location": "/Python-Batch-Component-API/index.html#api-overview", 
            "text": "In OpenMPF, a  component  is a plugin that receives jobs (containing media), processes that  media, and returns results.  The OpenMPF Batch Component API currently supports the development of  detection components , which are used detect\nobjects in image, video, audio, or other (generic) files that reside on disk.  Using this API, detection components can be built to provide:   Detection (Localizing an object)  Tracking (Localizing an object across multiple frames)  Classification (Detecting the type of object and optionally localizing that object)  Transcription (Detecting speech and transcribing it into text)", 
            "title": "API Overview"
        }, 
        {
            "location": "/Python-Batch-Component-API/index.html#how-components-integrate-into-openmpf", 
            "text": "Components are integrated into OpenMPF through the use of OpenMPF's  Component Executable .\nDevelopers create component libraries that encapsulate the component detection logic.\nEach instance of the Component Executable loads one of these libraries and uses it to service job requests\nsent by the OpenMPF Workflow Manager (WFM).  The Component Executable:   Receives and parses job requests from the WFM  Invokes methods on the component library to obtain detection results  Populates and sends the respective responses to the WFM   The basic psuedocode for the Component Executable is as follows:  component_cls = locate_component_class()\ncomponent = component_cls()\ndetection_type = component.detection_type\n\nwhile True:\n    job = receive_job()\n\n    if is_image_job(job) and hasattr(component, 'get_detections_from_image'):\n        detections = component.get_detections_from_image(job)\n        send_job_response(detections)\n\n    elif is_video_job(job) and hasattr(component, 'get_detections_from_video'):\n        detections = component.get_detections_from_video(job)\n        send_job_response(detections)\n\n    elif is_audio_job(job) and hasattr(component, 'get_detections_from_audio'):\n        detections = component.get_detections_from_audio(job)\n        send_job_response(detections)\n\n    elif is_generic_job(job) and hasattr(component, 'get_detections_from_generic'):\n        detections = component.get_detections_from_generic(job)\n        send_job_response(detections)  Each instance of a Component Executable runs as a separate process.  The Component Executable receives and parses requests from the WFM, invokes methods on the Component Logic to get\ndetection objects, and subsequently populates responses with the component output and sends them to the WFM.  A component developer implements a detection component by creating a class that defines one or more of the\nget_detections_from_* methods and has a  detection_type  field.\nSee the  API Specification  for more information.  The figures below present high-level component diagrams of the Python Batch Component API.\nThis figure shows the basic structure:   The figure above shows the Node Manager starting the Detection Component Executable.\nThe Detection Component Executable determines that it is running a Python component so it creates an instance of the PythonComponentHandle \nclass. The  PythonComponentHandle  class creates an instance of the component class and calls one of the get_detections_from_*  methods on the component instance. The example\nabove is an image component, so  PythonComponentHandle  calls  ExampleImageFaceDetection.get_detections_from_image \non the component instance. The component instance creates an instance of mpf_component_util.ImageReader  to access the image. Components that support video\nwould implement  get_detections_from_video  and use mpf_component_util.VideoCapture  instead.  This figure show the structure when the mixin classes are used:   The figure above shows a video component,  ExampleVideoFaceDetection , that extends the mpf_component_util.VideoCaptureMixin  class.  PythonComponentHandle  will\ncall  get_detections_from_video  on an instance of  ExampleVideoFaceDetection .  ExampleVideoFaceDetection  does not\nimplement  get_detections_from_video , so the implementation inherited from  mpf_component_util.VideoCaptureMixin \ngets called.  mpf_component_util.VideoCaptureMixin.get_detections_from_video  creates an instance of mpf_component_util.VideoCapture  and calls ExampleVideoFaceDetection.get_detections_from_video_capture , passing in the  mpf_component_util.VideoCapture  it\njust created.  ExampleVideoFaceDetection.get_detections_from_video_capture  is where the component reads the video\nusing the passed-in  mpf_component_util.VideoCapture  and attempts to find detections. Components that support images\nwould extend  mpf_component_util.ImageReaderMixin , implement get_detections_from_image_reader , and access the image using the passed-in mpf_component_util.ImageReader .  During component registration a  virtualenv  is created for each component.\nThe virtualenv has access to the built-in Python libraries, but does not have access to any third party packages\nthat might be installed on the system. When creating the virtualenv for a setuptools-based component the only packages\nthat get installed are the component itself and any dependencies specified in the setup.py\nfile (including their transitive dependencies). When creating the virtualenv for a basic Python component the only\npackage that gets installed is  mpf_component_api .  mpf_component_api  is the package containing the job classes\n(e.g.  mpf_component_api.ImageJob , mpf_component_api.VideoJob ) and detection result classes\n(e.g.  mpf_component_api.ImageLocation , mpf_component_api.VideoTrack ).", 
            "title": "How Components Integrate into OpenMPF"
        }, 
        {
            "location": "/Python-Batch-Component-API/index.html#how-to-create-a-python-component", 
            "text": "There are two types of Python components that are supported, setuptools-based components and basic Python components.\nBasic Python components are quicker to set up, but have no built-in support for dependency management.\nAll dependencies must be handled by the developer. Setuptools-based components are recommended since they use\nsetuptools and pip for dependency management.", 
            "title": "How to Create a Python Component"
        }, 
        {
            "location": "/Python-Batch-Component-API/index.html#get-openmpf-python-component-sdk", 
            "text": "In order to create a Python component you will need to clone the openmpf-python-component-sdk repository  if you don't\nalready have it. While not technically required, it is recommended to also clone the openmpf-build-tools repository .\nThe rest of the steps assume you cloned openmpf-python-component-sdk to ~/openmpf-projects/openmpf-python-component-sdk . The rest of the steps also assume that if you cloned the\nopenmpf-build-tools repository, you cloned it to  ~/openmpf-projects/openmpf-build-tools .", 
            "title": "Get openmpf-python-component-sdk"
        }, 
        {
            "location": "/Python-Batch-Component-API/index.html#setup-python-component-libraries", 
            "text": "The component packaging steps require that wheel files for  mpf_component_api ,  mpf_component_util , and\ntheir dependencies are available in the  ~/mpf-sdk-install/python/wheelhouse  directory.  If you have openmpf-build-tools, then you can run:  ~/openmpf-projects/openmpf-build-tools/build-openmpf-components/build_components.py -psdk ~/openmpf-projects/openmpf-python-component-sdk  To setup the libraries manually you can run:  pip3 wheel -w ~/mpf-sdk-install/python/wheelhouse ~/openmpf-projects/openmpf-python-component-sdk/detection/api\npip3 wheel -w ~/mpf-sdk-install/python/wheelhouse ~/openmpf-projects/openmpf-python-component-sdk/detection/component_util", 
            "title": "Setup Python Component Libraries"
        }, 
        {
            "location": "/Python-Batch-Component-API/index.html#how-to-create-a-setuptools-based-python-component", 
            "text": "In this example we create a setuptools-based video component named \"MyComponent\". An example of a setuptools-based\nPython component can be found here .  This is the recommended project structure:  ComponentName\n\u251c\u2500\u2500 setup.py\n\u251c\u2500\u2500 component_name\n\u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u2514\u2500\u2500 component_name.py\n\u2514\u2500\u2500 plugin-files\n    \u251c\u2500\u2500 descriptor\n    \u2502   \u2514\u2500\u2500 descriptor.json\n    \u2514\u2500\u2500 wheelhouse  # optional\n        \u2514\u2500\u2500 my_prebuilt_lib-0.1-py3-none-any.whl  1. Create directory structure:  mkdir MyComponent\nmkdir MyComponent/my_component\nmkdir -p MyComponent/plugin-files/descriptor\ntouch MyComponent/setup.py\ntouch MyComponent/my_component/__init__.py\ntouch MyComponent/my_component/my_component.py\ntouch MyComponent/plugin-files/descriptor/descriptor.json  2. Create setup.py file in project's top-level directory:  Example of a minimal setup.py file:  import setuptools\n\nsetuptools.setup(\n    name='MyComponent',\n    version='0.1',\n    packages=setuptools.find_packages(),\n    install_requires=(\n        'mpf_component_api =0.1',\n        'mpf_component_util =0.1'\n    ),\n    entry_points={\n        'mpf.exported_component': 'component = my_component.my_component:MyComponent'\n    }\n\n)  The  name  parameter defines the distribution name. Typically the distribution name matches the component name.  Any dependencies that component requires should be listed in the  install_requires  field.  The component executor looks in the  entry_points  element and uses the  mpf.exported_component  field to determine\nthe component class. The right hand side of  component =  should be the dotted module name, followed by a  : ,\nfollowed by the name of the class. The general pattern is 'mpf.exported_component': 'component =  package_name . module_name : class_name ' . In the above example, MyComponent  is the class name. The module is listed as  my_component.my_component  because the  my_component \npackage contains the  my_component.py  file and the  my_component.py  file contains the  MyComponent  class.  3. Create descriptor.json file in MyComponent/plugin-files/descriptor:  The  batchLibrary  field should match the distribution name from the setup.py file. In this example the\nfield should be:  \"batchLibrary\" : \"MyComponent\" .\nSee  Packaging and Registering a Component  for details about\nthe descriptor format.  4. Implement your component class:  Below is an example of the structure of a simple component. This component extends mpf_component_util.VideoCaptureMixin  to simplify the use of mpf_component_util.VideoCapture . You would replace the call to run_detection_algorithm_on_frame  with your component-specific logic.  import mpf_component_api as mpf\nimport mpf_component_util as mpf_util\n\nlogger = mpf.configure_logging('my-component.log', __name__ == '__main__')\n\nclass MyComponent(mpf_util.VideoCaptureMixin, object):\n    detection_type = 'FACE'\n\n    @staticmethod\n    def get_detections_from_video_capture(video_job, video_capture):\n        logger.info('[%s] Received video job: %s', video_job.job_name, video_job)\n        # If frame index is not required, you can just loop over video_capture directly\n        for frame_index, frame in enumerate(video_capture):\n            for result_track in run_detection_algorithm_on_frame(frame_index, frame):\n                # Alternatively, while iterating through the video, add tracks to a list. When done, return that list.\n                yield result_track  5. Optional: Add prebuilt wheel files if not available on PyPi:  If your component depends on Python libraries that are not available on PyPi, the libraries can be manually added to\nyour project. The prebuilt libraries must be placed in your project's  plugin-files/wheelhouse  directory.\nThe prebuilt library names must be listed in your  setup.py  file's  install_requires  field.\nIf any of the prebuilt libraries have transitive dependencies that are not available on PyPi, then those libraries\nmust also be added to your project's  plugin-files/wheelhouse  directory.  6. Create the plugin package:  The directory structure of the .tar.gz file will be:  MyComponent\n\u251c\u2500\u2500 descriptor\n\u2502   \u2514\u2500\u2500 descriptor.json\n\u2514\u2500\u2500 wheelhouse\n    \u251c\u2500\u2500 MyComponent-0.1-py3-none-any.whl\n    \u251c\u2500\u2500 mpf_component_api-0.1-py3-none-any.whl\n    \u251c\u2500\u2500 mpf_component_util-0.1-py3-none-any.whl\n    \u251c\u2500\u2500 numpy-1.18.4-cp38-cp38-manylinux1_x86_64.whl\n    \u2514\u2500\u2500 opencv_python-4.2.0.34-cp38-cp38-manylinux1_x86_64.whl  To create the plugin packages you can run the build script as follows:  ~/openmpf-projects/openmpf-build-tools/build-openmpf-components/build_components.py -psdk ~/openmpf-projects/openmpf-python-component-sdk -c MyComponent  The plugin package can also be built manually using the following commands:  mkdir -p plugin-packages/MyComponent/wheelhouse\ncp -r MyComponent/plugin-files/* plugin-packages/MyComponent/\npip3 wheel -w plugin-packages/MyComponent/wheelhouse -f ~/mpf-sdk-install/python/wheelhouse -f plugin-packages/MyComponent/wheelhouse ./MyComponent/\ncd plugin-packages\ntar -zcf MyComponent.tar.gz MyComponent", 
            "title": "How to Create a Setuptools-based Python Component"
        }, 
        {
            "location": "/Python-Batch-Component-API/index.html#how-to-create-a-basic-python-component", 
            "text": "In this example we create a basic Python component that supports video. An example of a basic Python component can be\nfound here .  This is the recommended project structure:  ComponentName\n\u251c\u2500\u2500 component_name.py\n\u251c\u2500\u2500 dependency.py\n\u2514\u2500\u2500 descriptor\n    \u2514\u2500\u2500 descriptor.json  1. Create directory structure:  mkdir MyComponent\nmkdir MyComponent/descriptor\ntouch MyComponent/descriptor/descriptor.json\ntouch MyComponent/my_component.py  2. Create descriptor.json file in MyComponent/descriptor:  The  batchLibrary  field should be the full path to the Python file containing your component class.\nIn this example the field should be:  \"batchLibrary\" : \"${MPF_HOME}/plugins/MyComponent/my_component.py\" .\nSee  Packaging and Registering a Component  for details about\nthe descriptor format.  3. Implement your component class:  Below is an example of the structure of a simple component that does not use mpf_component_util.VideoCaptureMixin . You would replace the call to run_detection_algorithm  with your component-specific logic.  import mpf_component_api as mpf\n\nlogger = mpf.configure_logging('my-component.log', __name__ == '__main__')\n\nclass MyComponent(object):\n    detection_type = 'FACE'\n\n    @staticmethod\n    def get_detections_from_video(video_job):\n        logger.info('[%s] Received video job: %s', video_job.job_name, video_job)\n        return run_detection_algorithm(video_job)\n\nEXPORT_MPF_COMPONENT = MyComponent  The component executor looks for a module-level variable named  EXPORT_MPF_COMPONENT  to specify which class\nis the component.  4. Create the plugin package:  The directory structure of the .tar.gz file will be:  ComponentName\n\u251c\u2500\u2500 component_name.py\n\u251c\u2500\u2500 dependency.py\n\u2514\u2500\u2500 descriptor\n    \u2514\u2500\u2500 descriptor.json  To create the plugin packages you can run the build script as follows:  ~/openmpf-projects/openmpf-build-tools/build-openmpf-components/build_components.py -c MyComponent  The plugin package can also be built manually using the following command:  tar -zcf MyComponent.tar.gz MyComponent", 
            "title": "How to Create a Basic Python Component"
        }, 
        {
            "location": "/Python-Batch-Component-API/index.html#api-specification", 
            "text": "An OpenMPF Python component is a class that defines one or more of the get_detections_from_* methods and has a detection_type  field.", 
            "title": "API Specification"
        }, 
        {
            "location": "/Python-Batch-Component-API/index.html#componentget_detections_from_42-methods", 
            "text": "All get_detections_from_* methods are invoked through an instance of the component class. The only parameter passed\nin is an appropriate job object (e.g.  mpf_component_api.ImageJob ,  mpf_component_api.VideoJob ). Since the methods\nare invoked through an instance, instance methods and class methods end up with two arguments, the first is either the\ninstance or the class, respectively. All get_detections_from_* methods can be implemented either as an instance method,\na static method, or a class method.\nFor example:  instance method:  class MyComponent(object):\n    def get_detections_from_image(self, image_job):\n        return [mpf_component_api.ImageLocation(...), ...]  static method:  class MyComponent(object):\n    @staticmethod\n    def get_detections_from_image(image_job):\n        return [mpf_component_api.ImageLocation(...), ...]  class method:  class MyComponent(object):\n    @classmethod\n    def get_detections_from_image(cls, image_job):\n        return [mpf_component_api.ImageLocation(...), ...]  All get_detections_from_* methods must return an iterable of the appropriate detection type\n(e.g.  mpf_component_api.ImageLocation ,  mpf_component_api.VideoTrack ). The return value is normally a list or generator,\nbut any iterable can be used.", 
            "title": "component.get_detections_from_* methods"
        }, 
        {
            "location": "/Python-Batch-Component-API/index.html#componentdetection_type", 
            "text": "str  field describing the type of object that is detected by the component. Should be in all CAPS.\nExamples include:  FACE ,  MOTION ,  PERSON ,  SPEECH ,  CLASS  (for object classification), or  TEXT .  Example:   class MyComponent(object):\n    detection_type = 'FACE'", 
            "title": "component.detection_type"
        }, 
        {
            "location": "/Python-Batch-Component-API/index.html#image-api", 
            "text": "", 
            "title": "Image API"
        }, 
        {
            "location": "/Python-Batch-Component-API/index.html#componentget_detections_from_imageimage_job", 
            "text": "Used to detect objects in an image file.   Method Definition:   class MyComponent(object):\n    def get_detections_from_image(self, image_job):\n        return [mpf_component_api.ImageLocation(...), ...]  get_detections_from_image , like all get_detections_from_* methods, can be implemented either as an instance method,\na static method, or a class method.   Parameters:      Parameter  Data Type  Description      image_job  mpf_component_api.ImageJob  Object containing details about the work to be performed.      Returns: An iterable of  mpf_component_api.ImageLocation", 
            "title": "component.get_detections_from_image(image_job)"
        }, 
        {
            "location": "/Python-Batch-Component-API/index.html#mpf_component_apiimagejob", 
            "text": "Class containing data used for detection of objects in an image file.   Members:   \n   \n     \n       Member \n       Data Type \n       Description \n     \n   \n   \n     \n       job_name \n       str \n       A specific name given to the job by the OpenMPF framework. This value may be used, for example, for logging and debugging purposes. \n     \n     \n       data_uri \n       str \n       The URI of the input media file to be processed. Currently, this is a file path. For example, \"/opt/mpf/share/remote-media/test-file.jpg\". \n     \n     \n       job_properties \n       dict[str, str] \n       \n        Contains a dict with keys and values of type  str  which represent the property name and the property value. The key corresponds to the property name specified in the component descriptor file described in  Packaging and Registering a Component . Values are determined when creating a pipeline or when submitting a job.\n         \n        Note: The job_properties dict may not contain the full set of job properties. For properties not contained in the dict, the component must use a default value.\n       \n     \n     \n       media_properties \n       dict[str, str] \n       \n        Contains a dict with keys and values of type  str  of metadata about the media associated with the job.\n         \n        Includes the following key-value pairs:\n         \n           MIME_TYPE  : the MIME type of the media \n           FRAME_WIDTH  : the width of the image in pixels \n           FRAME_HEIGHT  : the height of the image in pixels \n         \n        May include the following key-value pairs:\n         \n           ROTATION  : A floating point value in the interval  [0.0, 360.0)  indicating the orientation of the media in degrees in the counter-clockwise direction. In order to view the media in the upright orientation, it must be rotated the given number of degrees in the clockwise direction. \n           HORIZONTAL_FLIP  : true if the image is mirrored across the Y-axis, otherwise false \n           EXIF_ORIENTATION  : the standard EXIF orientation tag; a value between 1 and 8 \n         \n       \n     \n     \n       feed_forward_location \n       None  or  mpf_component_api.ImageLocation \n       An  mpf_component_api.ImageLocation  from the previous pipeline stage. Provided when feed forward is enabled. See  Feed Forward Guide .", 
            "title": "mpf_component_api.ImageJob"
        }, 
        {
            "location": "/Python-Batch-Component-API/index.html#mpf_component_apiimagelocation", 
            "text": "Class used to store the location of detected objects in a image file.   Constructor:   def __init__(self, x_left_upper, y_left_upper, width, height, confidence=-1.0, detection_properties=None):\n    ...   Members:      Member  Data Type  Description      x_left_upper  int  Upper left X coordinate of the detected object.    y_left_upper  int  Upper left Y coordinate of the detected object.    width  int  The width of the detected object.    height  int  The height of the detected object.    confidence  float  Represents the \"quality\" of the detection. The range depends on the detection algorithm. 0.0 is lowest quality. Higher values are higher quality. Using a standard range of [0.0 - 1.0] is advised. If the component is unable to supply a confidence value, it should return -1.0.    detection_properties  dict[str, str]  A dict with keys and values of type  str  containing optional additional information about the detected object. For best practice, keys should be in all CAPS.     See here for information about rotation and horizontal flipping.   Example:   A component that performs generic object classification can add an entry to  detection_properties  where the key is CLASSIFICATION  and the value is the type of object detected.  mpf_component_api.ImageLocation(0, 0, 100, 100, 1.0, {'CLASSIFICATION': 'backpack'})", 
            "title": "mpf_component_api.ImageLocation"
        }, 
        {
            "location": "/Python-Batch-Component-API/index.html#mpf_component_utilimagereader", 
            "text": "mpf_component_util.ImageReader  is a utility class for accessing images. It is the image equivalent to mpf_component_util.VideoCapture . Like  mpf_component_util.VideoCapture ,\nit may modify the read-in frame data based on job_properties. From the point of view of someone using mpf_component_util.ImageReader , these modifications are mostly transparent.  mpf_component_util.ImageReader  makes\nit look like you are reading the original image file as though it has already been rotated, flipped, cropped, etc.  One issue with this approach is that the detection bounding boxes will be relative to the\nmodified frame data, not the original. To make the detections relative to the original image\nthe  mpf_component_util.ImageReader.reverse_transform(image_location)  method must be called on each mpf_component_api.ImageLocation . Since the use of  mpf_component_util.ImageReader  is optional, the framework\ncannot automatically perform the reverse transform for the developer.  The general pattern for using  mpf_component_util.ImageReader  is as follows:  class MyComponent(object):\n\n    @staticmethod\n    def get_detections_from_image(image_job):\n        image_reader = mpf_component_util.ImageReader(image_job)\n        image = image_reader.get_image()\n        # run_component_specific_algorithm is a placeholder for this example.\n        # Replace run_component_specific_algorithm with your component's detection logic\n        result_image_locations = run_component_specific_algorithm(image)\n        for result in result_image_locations:\n            image_reader.reverse_transform(result)\n            yield result  Alternatively, see the documentation for  mpf_component_util.ImageReaderMixin  for a more concise way to use mpf_component_util.ImageReader  below.", 
            "title": "mpf_component_util.ImageReader"
        }, 
        {
            "location": "/Python-Batch-Component-API/index.html#mpf_component_utilimagereadermixin", 
            "text": "A mixin class that can be used to simplify the usage of  mpf_component_util.ImageReader . mpf_component_util.ImageReaderMixin  takes care of initializing a  mpf_component_util.ImageReader  and\nperforming the reverse transform.  There are some requirements to properly use  mpf_component_util.ImageReaderMixin :   The component must extend  mpf_component_util.ImageReaderMixin .  The component must implement  get_detections_from_image_reader(image_job, image_reader) .  The component must read the image using the  mpf_component_util.ImageReader \n  that is passed in to  get_detections_from_image_reader(image_job, image_reader) .  The component must NOT implement  get_detections_from_image(image_job) .  The component must NOT call  mpf_component_util.ImageReader.reverse_transform .   The general pattern for using  mpf_component_util.ImageReaderMixin  is as follows:  class MyComponent(mpf_component_util.ImageReaderMixin, object):\n\n    @staticmethod # Can also be a regular instance method or a class method\n    def get_detections_from_image_reader(image_job, image_reader):\n        image = image_reader.get_image()\n\n        # run_component_specific_algorithm is a placeholder for this example.\n        # Replace run_component_specific_algorithm with your component's detection logic\n        return run_component_specific_algorithm(image)  mpf_component_util.ImageReaderMixin  is a mixin class so it is designed in a way that does not prevent the subclass\nfrom extending other classes. If a component supports both videos and images, and it uses mpf_component_util.VideoCaptureMixin , it should also use mpf_component_util.ImageReaderMixin .", 
            "title": "mpf_component_util.ImageReaderMixin"
        }, 
        {
            "location": "/Python-Batch-Component-API/index.html#video-api", 
            "text": "", 
            "title": "Video API"
        }, 
        {
            "location": "/Python-Batch-Component-API/index.html#componentget_detections_from_videovideo_job", 
            "text": "Used to detect objects in a video file. Prior to being sent to the component, videos are split into logical \"segments\"\nof video data and each segment (containing a range of frames) is assigned to a different job. Components are not\nguaranteed to receive requests in any order. For example, the first request processed by a component might receive a\nrequest for frames 300-399 of a Video A, while the next request may cover frames 900-999 of a Video B.   Method Definition:   class MyComponent(object):\n    def get_detections_from_video(self, video_job):\n        return [mpf_component_api.VideoTrack(...), ...]  get_detections_from_video , like all get_detections_from_* methods, can be implemented either as an instance method,\na static method, or a class method.   Parameters:      Parameter  Data Type  Description      video_job  mpf_component_api.VideoJob  Object containing details about the work to be performed.      Returns: An iterable of  mpf_component_api.VideoTrack", 
            "title": "component.get_detections_from_video(video_job)"
        }, 
        {
            "location": "/Python-Batch-Component-API/index.html#mpf_component_apivideojob", 
            "text": "Class containing data used for detection of objects in a video file.   Members:   \n   \n     \n       Member \n       Data Type \n       Description \n     \n   \n   \n     \n       job_name \n       str \n       A specific name given to the job by the OpenMPF framework. This value may be used, for example, for logging and debugging purposes. \n     \n     \n       data_uri \n       str \n       The URI of the input media file to be processed. Currently, this is a file path. For example, \"/opt/mpf/share/remote-media/test-file.avi\". \n     \n     \n       start_frame \n       int \n       The first frame number (0-based index) of the video that should be processed to look for detections. \n         \n     \n       stop_frame \n       int \n       The last frame number (0-based index) of the video that should be processed to look for detections. \n         \n     \n       job_properties \n       dict[str, str] \n       \n        Contains a dict with keys and values of type  str  which represent the property name and the property value. The key corresponds to the property name specified in the component descriptor file described in  Packaging and Registering a Component . Values are determined when creating a pipeline or when submitting a job.\n         \n        Note: The job_properties dict may not contain the full set of job properties. For properties not contained in the dict, the component must use a default value.\n       \n     \n     \n       media_properties \n       dict[str, str] \n       \n        Contains a dict with keys and values of type  str  of metadata about the media associated with the job.\n         \n        Includes the following key-value pairs:\n         \n           DURATION  : length of video in milliseconds \n           FPS  : frames per second (averaged for variable frame rate video) \n           FRAME_COUNT  : the number of frames in the video \n           MIME_TYPE  : the MIME type of the media \n           FRAME_WIDTH  : the width of a frame in pixels \n           FRAME_HEIGHT  : the height of a frame in pixels \n         \n        May include the following key-value pair:\n         \n           ROTATION  : A floating point value in the interval  [0.0, 360.0)  indicating the orientation of the media in degrees in the counter-clockwise direction. In order to view the media in the upright orientation, it must be rotated the given number of degrees in the clockwise direction. \n         \n       \n     \n     \n       feed_forward_track \n       None  or  mpf_component_api.VideoTrack \n       An  mpf_component_api.VideoTrack  from the previous pipeline stage. Provided when feed forward is enabled. See  Feed Forward Guide . \n     \n      IMPORTANT:   FRAME_INTERVAL  is a common job property that many components support.\nFor frame intervals greater than 1, the component must look for detections starting with the first\nframe, and then skip frames as specified by the frame interval, until or before it reaches the stop frame.\nFor example, given a start frame of 0, a stop frame of 99, and a frame interval of 2, then the detection component\nmust look for objects in frames numbered 0, 2, 4, 6, ..., 98.", 
            "title": "mpf_component_api.VideoJob"
        }, 
        {
            "location": "/Python-Batch-Component-API/index.html#mpf_component_apivideotrack", 
            "text": "Class used to store the location of detected objects in a video file.   Constructor:   def __init__(self, start_frame, stop_frame, confidence=-1.0, frame_locations=None, detection_properties=None):\n    ...   Members:      Member  Data Type  Description      start_frame  int  The first frame number (0-based index) that contained the detected object.    stop_frame  int  The last frame number (0-based index) that contained the detected object.    confidence  float  Represents the \"quality\" of the detection. The range depends on the detection algorithm. 0.0 is lowest quality. Higher values are higher quality. Using a standard range of [0.0 - 1.0] is advised. If the component is unable to supply a confidence value, it should return -1.0.    frame_locations  dict[int, mpf_component_api.ImageLocation]  A dict of individual detections. The key for each entry is the frame number where the detection was generated, and the value is a  mpf_component_api.ImageLocation  calculated as if that frame was a still image. Note that a key-value pair is  not  required for every frame between the track start frame and track stop frame.    detection_properties  dict[str, str]  A dict with keys and values of type  str  containing optional additional information about the detected object. For best practice, keys should be in all CAPS.      NOTE:  Currently,  mpf_component_api.VideoTrack.detection_properties  do not show up in the JSON output object or\nare used by the WFM in any way.    Example:   A component that performs generic object classification can add an entry to  detection_properties  where the key is CLASSIFICATION  and the value is the type of object detected.  track = mpf_component_api.VideoTrack(0, 1)\ntrack.frame_locations[0] = mpf_component_api.ImageLocation(0, 0, 100, 100, 0.75, {'CLASSIFICATION': 'backpack'})\ntrack.frame_locations[1] = mpf_component_api.ImageLocation(10, 10, 110, 110, 0.95, {'CLASSIFICATION': 'backpack'})\ntrack.confidence = max(il.confidence for il in track.frame_locations.itervalues())", 
            "title": "mpf_component_api.VideoTrack"
        }, 
        {
            "location": "/Python-Batch-Component-API/index.html#mpf_component_utilvideocapture", 
            "text": "mpf_component_util.VideoCapture  is a utility class for reading videos.  mpf_component_util.VideoCapture  works very\nsimilarly to  cv2.VideoCapture , except that it might modify the video frames based on job properties. From the point\nof view of someone using  mpf_component_util.VideoCapture , these modifications are mostly transparent. mpf_component_util.VideoCapture  makes it look like you are reading the original video file as though it has already\nbeen rotated, flipped, cropped, etc. Also, if frame skipping is enabled, such as by setting the value of the FRAME_INTERVAL  job property, it makes it look like you are reading the video as though it never contained the\nskipped frames.  One issue with this approach is that the detection frame numbers and bounding box will be relative to the\nmodified video, not the original. To make the detections relative to the original video\nthe  mpf_component_util.VideoCapture.reverse_transform(video_track)  method must be called on each mpf_component_api.VideoTrack . Since the use of  mpf_component_util.VideoCapture  is optional, the framework\ncannot automatically perform the reverse transform for the developer.  The general pattern for using  mpf_component_util.VideoCapture  is as follows:  class MyComponent(object):\n\n    @staticmethod\n    def get_detections_from_video(video_job):\n        video_capture = mpf_component_util.VideoCapture(video_job)\n        # If frame index is not required, you can just loop over video_capture directly\n        for frame_index, frame in enumerate(video_capture):\n            # run_component_specific_algorithm is a placeholder for this example.\n            # Replace run_component_specific_algorithm with your component's detection logic\n            result_tracks = run_component_specific_algorithm(frame_index, frame)\n            for track in result_tracks:\n                video_capture.reverse_transform(track)\n                yield track  Alternatively, see the documentation for  mpf_component_util.VideoCaptureMixin  for a more concise way to use mpf_component_util.VideoCapture  below.", 
            "title": "mpf_component_util.VideoCapture"
        }, 
        {
            "location": "/Python-Batch-Component-API/index.html#mpf_component_utilvideocapturemixin", 
            "text": "A mixin class that can be used to simplify the usage of  mpf_component_util.VideoCapture . mpf_component_util.VideoCaptureMixin  takes care of initializing a  mpf_component_util.VideoCapture  and\nperforming the reverse transform.  There are some requirements to properly use  mpf_component_util.VideoCaptureMixin :   The component must extend  mpf_component_util.VideoCaptureMixin .  The component must implement  get_detections_from_video_capture(video_job, video_capture) .  The component must read the video using the  mpf_component_util.VideoCapture \n  that is passed in to  get_detections_from_video_capture(video_job, video_capture) .  The component must NOT implement  get_detections_from_video(video_job) .  The component must NOT call  mpf_component_util.VideoCapture.reverse_transform .   The general pattern for using  mpf_component_util.VideoCaptureMixin  is as follows:  class MyComponent(mpf_component_util.VideoCaptureMixin, object):\n\n    @staticmethod # Can also be a regular instance method or a class method\n    def get_detections_from_video_capture(video_job, video_capture):\n        # If frame index is not required, you can just loop over video_capture directly\n        for frame_index, frame in enumerate(video_capture):\n            # run_component_specific_algorithm is a placeholder for this example.\n            # Replace run_component_specific_algorithm with your component's detection logic\n            result_tracks = run_component_specific_algorithm(frame_index, frame)\n            for track in result_tracks:\n                # Alternatively, while iterating through the video, add tracks to a list. When done, return that list.\n                yield track  mpf_component_util.VideoCaptureMixin  is a mixin class so it is designed in a way that does not prevent the subclass\nfrom extending other classes. If a component supports both videos and images, and it uses mpf_component_util.VideoCaptureMixin , it should also use mpf_component_util.ImageReaderMixin .\nFor example:  class MyComponent(mpf_component_util.VideoCaptureMixin, mpf_component_util.ImageReaderMixin, object):\n\n    @staticmethod\n    def get_detections_from_video_capture(video_job, video_capture):\n        ...\n\n    @staticmethod\n    def get_detections_from_image_reader(image_job, image_reader):\n       ...", 
            "title": "mpf_component_util.VideoCaptureMixin"
        }, 
        {
            "location": "/Python-Batch-Component-API/index.html#audio-api", 
            "text": "", 
            "title": "Audio API"
        }, 
        {
            "location": "/Python-Batch-Component-API/index.html#componentget_detections_from_audioaudio_job", 
            "text": "Used to detect objects in an audio file.   Method Definition:   class MyComponent(object):\n    def get_detections_from_audio(self, audio_job):\n        return [mpf_component_api.AudioTrack(...), ...]  get_detections_from_audio , like all get_detections_from_* methods, can be implemented either as an instance method,\na static method, or a class method.   Parameters:      Parameter  Data Type  Description      audio_job  mpf_component_api.AudioJob  Object containing details about the work to be performed.      Returns: An iterable of  mpf_component_api.AudioTrack", 
            "title": "component.get_detections_from_audio(audio_job)"
        }, 
        {
            "location": "/Python-Batch-Component-API/index.html#mpf_component_apiaudiojob", 
            "text": "Class containing data used for detection of objects in an audio file.\nCurrently, audio files are not logically segmented, so a job will contain the entirety of the audio file.   Members:   \n   \n     \n       Member \n       Data Type \n       Description \n     \n   \n   \n     \n       job_name \n       str \n       A specific name given to the job by the OpenMPF framework. This value may be used, for example, for logging and debugging purposes. \n     \n     \n       data_uri \n       str \n       The URI of the input media file to be processed. Currently, this is a file path. For example, \"/opt/mpf/share/remote-media/test-file.mp3\". \n     \n     \n       start_time \n       int \n       The time (0-based index, in milliseconds) associated with the beginning of the segment of the audio file that should be processed to look for detections. \n     \n     \n       stop_time \n       int \n       The time (0-based index, in milliseconds) associated with the end of the segment of the audio file that should be processed to look for detections. \n             \n     \n       job_properties \n       dict[str, str] \n       \n        Contains a dict with keys and values of type  str  which represent the property name and the property value. The key corresponds to the property name specified in the component descriptor file described in  Packaging and Registering a Component . Values are determined when creating a pipeline or when submitting a job.\n         \n        Note: The job_properties dict may not contain the full set of job properties. For properties not contained in the dict, the component must use a default value.\n       \n     \n     \n       media_properties \n       dict[str, str] \n       \n        Contains a dict with keys and values of type  str  of metadata about the media associated with the job.\n         \n        Includes the following key-value pairs:\n         \n           DURATION  : length of audio file in milliseconds \n           MIME_TYPE  : the MIME type of the media \n         \n       \n     \n     \n       feed_forward_track \n       None  or  mpf_component_api.AudioTrack \n       An  mpf_component_api.AudioTrack  from the previous pipeline stage. Provided when feed forward is enabled. See  Feed Forward Guide .", 
            "title": "mpf_component_api.AudioJob"
        }, 
        {
            "location": "/Python-Batch-Component-API/index.html#mpf_component_apiaudiotrack", 
            "text": "Class used to store the location of detected objects in an audio file.   Constructor:   def __init__(self, start_time, stop_time, confidence, detection_properties=None):\n    ...   Members:      Member  Data Type  Description      start_time  int  The time (0-based index, in ms) when the audio detection event started.    stop_time  int  The time (0-based index, in ms) when the audio detection event stopped.    confidence  float  Represents the \"quality\" of the detection. The range depends on the detection algorithm. 0.0 is lowest quality. Higher values are higher quality. Using a standard range of [0.0 - 1.0] is advised. If the component is unable to supply a confidence value, it should return -1.0.    detection_properties  dict[str, str]  A dict with keys and values of type  str  containing optional additional information about the detected object. For best practice, keys should be in all CAPS.      NOTE:  Currently,  mpf_component_api.AudioTrack.detection_properties  do not show up in the JSON output object or\nare used by the WFM in any way.", 
            "title": "mpf_component_api.AudioTrack"
        }, 
        {
            "location": "/Python-Batch-Component-API/index.html#generic-api", 
            "text": "", 
            "title": "Generic API"
        }, 
        {
            "location": "/Python-Batch-Component-API/index.html#componentget_detections_from_genericgeneric_job", 
            "text": "Used to detect objects in files that are not video, image, or audio files. Such files are of the UNKNOWN type and\nhandled generically.   Method Definition:   class MyComponent(object):\n    def get_detections_from_generic(self, generic_job):\n        return [mpf_component_api.GenericTrack(...), ...]  get_detections_from_generic , like all get_detections_from_* methods, can be implemented either as an instance method,\na static method, or a class method.   Parameters:      Parameter  Data Type  Description      generic_job  mpf_component_api.GenericJob  Object containing details about the work to be performed.      Returns: An iterable of  mpf_component_api.GenericTrack", 
            "title": "component.get_detections_from_generic(generic_job)"
        }, 
        {
            "location": "/Python-Batch-Component-API/index.html#mpf_component_apigenericjob", 
            "text": "Class containing data used for detection of objects in a file that isn't a video, image, or audio file. The file is not\nlogically segmented, so a job will contain the entirety of the file.   Members:   \n   \n     \n       Member \n       Data Type \n       Description \n     \n   \n   \n     \n       job_name \n       str \n       A specific name given to the job by the OpenMPF framework. This value may be used, for example, for logging and debugging purposes. \n     \n     \n       data_uri \n       str \n       The URI of the input media file to be processed. Currently, this is a file path. For example, \"/opt/mpf/share/remote-media/test-file.txt\". \n     \n     \n       job_properties \n       dict[str, str] \n       \n        Contains a dict with keys and values of type  str  which represent the property name and the property value. The key corresponds to the property name specified in the component descriptor file described in  Packaging and Registering a Component . Values are determined when creating a pipeline or when submitting a job.\n         \n        Note: The job_properties dict may not contain the full set of job properties. For properties not contained in the dict, the component must use a default value.\n       \n     \n     \n       media_properties \n       dict[str, str] \n       \n        Contains a dict with keys and values of type  str  of metadata about the media associated with the job.\n         \n        Includes the following key-value pair:\n         \n           MIME_TYPE  : the MIME type of the media \n         \n       \n     \n     \n       feed_forward_track \n       None  or  mpf_component_api.GenericTrack \n       An  mpf_component_api.GenericTrack  from the previous pipeline stage. Provided when feed forward is enabled. See  Feed Forward Guide .", 
            "title": "mpf_component_api.GenericJob"
        }, 
        {
            "location": "/Python-Batch-Component-API/index.html#mpf_component_apigenerictrack", 
            "text": "Class used to store the location of detected objects in a file that is not a video, image, or audio file.   Constructor:   def __init__(self, confidence=-1.0, detection_properties=None):\n    ...   Members:      Member  Data Type  Description      confidence  float  Represents the \"quality\" of the detection. The range depends on the detection algorithm. 0.0 is lowest quality. Higher values are higher quality. Using a standard range of [0.0 - 1.0] is advised. If the component is unable to supply a confidence value, it should return -1.0.    detection_properties  dict[str, str]  A dict with keys and values of type  str  containing optional additional information about the detected object. For best practice, keys should be in all CAPS.", 
            "title": "mpf_component_api.GenericTrack"
        }, 
        {
            "location": "/Python-Batch-Component-API/index.html#python-component-build-environment", 
            "text": "All Python components must work with CPython 3.8.2. Also, Python components must work with the Linux version that is\nused by the OpenMPF Component Executable. At this writing, OpenMPF runs on CentOS 7.4.1708 (kernel version 3.10.0-693).\nPure Python code should work on any OS, but incompatibility issues can arise when using Python libraries that include\ncompiled extension modules. Python libraries are typically distributed as wheel files. The wheel format requires that\nthe file name follows the pattern of dist_name - version - python_tag - abi_tag - platform_tag .whl .  python_tag - abi_tag - platform_tag  are called compatibility tags . For example,  mpf_component_api  is pure Python,\nso the name of its wheel file is  mpf_component_api-0.1-py3-none-any.whl .  py3  means it will work with any Python 3\nimplementation because it does not use any implementation-specific features.  none  means that it does not use the\nPython ABI.  any  means it will work on any platform.  The following combinations of compatibility tags are supported:   cp32-abi3-linux_x86_64  cp32-abi3-manylinux1_x86_64  cp32-abi3-manylinux2010_x86_64  cp32-abi3-manylinux2014_x86_64  cp33-abi3-linux_x86_64  cp33-abi3-manylinux1_x86_64  cp33-abi3-manylinux2010_x86_64  cp33-abi3-manylinux2014_x86_64  cp34-abi3-linux_x86_64  cp34-abi3-manylinux1_x86_64  cp34-abi3-manylinux2010_x86_64  cp34-abi3-manylinux2014_x86_64  cp35-abi3-linux_x86_64  cp35-abi3-manylinux1_x86_64  cp35-abi3-manylinux2010_x86_64  cp35-abi3-manylinux2014_x86_64  cp36-abi3-linux_x86_64  cp36-abi3-manylinux1_x86_64  cp36-abi3-manylinux2010_x86_64  cp36-abi3-manylinux2014_x86_64  cp37-abi3-linux_x86_64  cp37-abi3-manylinux1_x86_64  cp37-abi3-manylinux2010_x86_64  cp37-abi3-manylinux2014_x86_64  cp38-abi3-linux_x86_64  cp38-abi3-manylinux1_x86_64  cp38-abi3-manylinux2010_x86_64  cp38-abi3-manylinux2014_x86_64  cp38-cp38-linux_x86_64  cp38-cp38-manylinux1_x86_64  cp38-cp38-manylinux2010_x86_64  cp38-cp38-manylinux2014_x86_64  cp38-none-any  cp38-none-linux_x86_64  cp38-none-manylinux1_x86_64  cp38-none-manylinux2010_x86_64  cp38-none-manylinux2014_x86_64  py30-none-any  py30-none-linux_x86_64  py30-none-manylinux1_x86_64  py30-none-manylinux2010_x86_64  py30-none-manylinux2014_x86_64  py31-none-any  py31-none-linux_x86_64  py31-none-manylinux1_x86_64  py31-none-manylinux2010_x86_64  py31-none-manylinux2014_x86_64  py32-none-any  py32-none-linux_x86_64  py32-none-manylinux1_x86_64  py32-none-manylinux2010_x86_64  py32-none-manylinux2014_x86_64  py33-none-any  py33-none-linux_x86_64  py33-none-manylinux1_x86_64  py33-none-manylinux2010_x86_64  py33-none-manylinux2014_x86_64  py34-none-any  py34-none-linux_x86_64  py34-none-manylinux1_x86_64  py34-none-manylinux2010_x86_64  py34-none-manylinux2014_x86_64  py35-none-any  py35-none-linux_x86_64  py35-none-manylinux1_x86_64  py35-none-manylinux2010_x86_64  py35-none-manylinux2014_x86_64  py36-none-any  py36-none-linux_x86_64  py36-none-manylinux1_x86_64  py36-none-manylinux2010_x86_64  py36-none-manylinux2014_x86_64  py37-none-any  py37-none-linux_x86_64  py37-none-manylinux1_x86_64  py37-none-manylinux2010_x86_64  py37-none-manylinux2014_x86_64  py38-none-any  py38-none-linux_x86_64  py38-none-manylinux1_x86_64  py38-none-manylinux2010_x86_64  py38-none-manylinux2014_x86_64  py3-none-any  py3-none-linux_x86_64  py3-none-manylinux1_x86_64  py3-none-manylinux2010_x86_64  py3-none-manylinux2014_x86_64   Components should be supplied as a tar file, which includes not only the component library, but any other libraries or\nfiles needed for execution. This includes all other non-standard libraries used by the component\n(aside from the standard Python libraries), and any configuration or data files.", 
            "title": "Python Component Build Environment"
        }, 
        {
            "location": "/Python-Batch-Component-API/index.html#component-development-best-practices", 
            "text": "", 
            "title": "Component Development Best Practices"
        }, 
        {
            "location": "/Python-Batch-Component-API/index.html#single-threaded-operation", 
            "text": "Implementations are encouraged to operate in single-threaded mode. OpenMPF will parallelize components through\nmultiple instantiations of the component, each running as a separate service.", 
            "title": "Single-threaded Operation"
        }, 
        {
            "location": "/Python-Batch-Component-API/index.html#stateless-behavior", 
            "text": "OpenMPF components should be stateless in operation and give identical output for a provided input\n(i.e. when processing the same job).", 
            "title": "Stateless Behavior"
        }, 
        {
            "location": "/Python-Batch-Component-API/index.html#logging", 
            "text": "It recommended that components use the logger returned from:   mpf_component_api.configure_logging(log_file_name, debug=False, replace_existing_config=True) . \nThe logger will write log messages to standard out. When  debug  is false, the log messages will also be written to  ${MPF_LOG_PATH}/${THIS_MPF_NODE}/log/ log_file_name .log  Note that multiple instances of the same component \ncan log to the same file. Also, logging content can span multiple lines. The following log levels are supported:  FATAL, ERROR, WARN, INFO, DEBUG .  The format of the log messages is:  DATE TIME LEVEL [SOURCE_FILE:LINE_NUMBER] - MESSAGE  For example:  2018-05-03 14:41:11,703 INFO  [test_component.py:44] - Logged message", 
            "title": "Logging"
        }, 
        {
            "location": "/GPU-Support-Guide/index.html", 
            "text": "NOTICE:\n This software (or technical data) was produced for the U.S. Government under contract, \nand is subject to the Rights in Data-General Clause 52.227-14, Alt. IV (DEC 2007). \nCopyright 2019 The MITRE Corporation. All Rights Reserved.\n\n\n\n\nIntroduction\n\n\nA subset of OpenMPF components are capable of running on NVIDIA GPUs. GPU support is through the NVIDIA CUDA libraries \nand runtime. This guide provides information needed for new component developers that would like to use NVIDIA GPUs \nto accelerate their component processing, and for users of the existing components that provide GPU support.\n\n\nBuilding a Component\n\n\nOpenMPF components that use GPUs are built with the NVIDIA nvcc compiler. Information about the nvcc compiler can be \nfound \nhere\n. The compiler accepts a number of \nflags to optimize the code generated, and the output of the compiler is called a \"fatbin\", since it may contain \nversions of the CUDA code compiled for multiple GPU architectures. This section discusses the nvcc compiler flags that \nare used within OpenMPF to tell the nvcc compiler what to include in the compiled output.\n\n\nThe nvcc compiler can generate two types of code: ELF code for a specific GPU architecture, and PTX code, which is the \nNVIDIA virtual machine and instruction set architecture that is generated in the first phase of nvcc compilation. \nYou can learn more about PTX \nhere\n. A fatbin may \nhave one or the other type of code, or both, for one or a set of different architectures. \n\n\nBy default, the OpenMPF components are built for maximum portability across NVIDIA GPU architectures. The nvcc flags \nto accomplish this are described in this \n\ntable\n. \nOpenMPF uses the \n-gencode\n flag, with the \n-arch=compute_30\n and \n-code=compute_30\n flags. This generates PTX code \nfor the minimum compute capability; at runtime, the NVIDIA driver will just-in-time compile the PTX code for the \narchitecture the code is running on.\n\n\nCustomizing the GPU Compile Flags\n\n\nOpenMPF currently has only one GPU component, the Darknet object detection and classification component. Our testing \nwith this component on a variety of NVIDIA GPU architectures has found an insignificant difference in the run time for \ndifferent architectures using this approach, and so we have opted to provide maximum runtime portability. For any new \ncomponents that may be developed, this may not be the case, and similar testing should be undertaken to determine the \ncorrect set of flags for that component. The nvcc compiler flags are configured by setting the \nCUDA_NVCC_FLAGS\n \nCMake variable in the individual component's CMakeLists.txt file. An example of setting \nCUDA_NVCC_FLAGS\n can be found \n\nhere\n.\n\n\n\n\nNOTE:\n OpenMPF GPU components are written so that they can run on the CPU only, as well as using GPU hardware. \nIf the component is built on a system that does not have the NVIDIA CUDA Toolkit installed, then the build will \ndefault to compiling for the CPU. It is recommended that developers of new GPU components make every attempt to \nfollow this model, so that other users are not burdened with installing the NVIDIA CUDA Toolkit when they have no \nplans to run on GPU hardware.", 
            "title": "GPU Support Guide"
        }, 
        {
            "location": "/GPU-Support-Guide/index.html#introduction", 
            "text": "A subset of OpenMPF components are capable of running on NVIDIA GPUs. GPU support is through the NVIDIA CUDA libraries \nand runtime. This guide provides information needed for new component developers that would like to use NVIDIA GPUs \nto accelerate their component processing, and for users of the existing components that provide GPU support.", 
            "title": "Introduction"
        }, 
        {
            "location": "/GPU-Support-Guide/index.html#building-a-component", 
            "text": "OpenMPF components that use GPUs are built with the NVIDIA nvcc compiler. Information about the nvcc compiler can be \nfound  here . The compiler accepts a number of \nflags to optimize the code generated, and the output of the compiler is called a \"fatbin\", since it may contain \nversions of the CUDA code compiled for multiple GPU architectures. This section discusses the nvcc compiler flags that \nare used within OpenMPF to tell the nvcc compiler what to include in the compiled output.  The nvcc compiler can generate two types of code: ELF code for a specific GPU architecture, and PTX code, which is the \nNVIDIA virtual machine and instruction set architecture that is generated in the first phase of nvcc compilation. \nYou can learn more about PTX  here . A fatbin may \nhave one or the other type of code, or both, for one or a set of different architectures.   By default, the OpenMPF components are built for maximum portability across NVIDIA GPU architectures. The nvcc flags \nto accomplish this are described in this  table . \nOpenMPF uses the  -gencode  flag, with the  -arch=compute_30  and  -code=compute_30  flags. This generates PTX code \nfor the minimum compute capability; at runtime, the NVIDIA driver will just-in-time compile the PTX code for the \narchitecture the code is running on.", 
            "title": "Building a Component"
        }, 
        {
            "location": "/GPU-Support-Guide/index.html#customizing-the-gpu-compile-flags", 
            "text": "OpenMPF currently has only one GPU component, the Darknet object detection and classification component. Our testing \nwith this component on a variety of NVIDIA GPU architectures has found an insignificant difference in the run time for \ndifferent architectures using this approach, and so we have opted to provide maximum runtime portability. For any new \ncomponents that may be developed, this may not be the case, and similar testing should be undertaken to determine the \ncorrect set of flags for that component. The nvcc compiler flags are configured by setting the  CUDA_NVCC_FLAGS  \nCMake variable in the individual component's CMakeLists.txt file. An example of setting  CUDA_NVCC_FLAGS  can be found  here .   NOTE:  OpenMPF GPU components are written so that they can run on the CPU only, as well as using GPU hardware. \nIf the component is built on a system that does not have the NVIDIA CUDA Toolkit installed, then the build will \ndefault to compiling for the CPU. It is recommended that developers of new GPU components make every attempt to \nfollow this model, so that other users are not burdened with installing the NVIDIA CUDA Toolkit when they have no \nplans to run on GPU hardware.", 
            "title": "Customizing the GPU Compile Flags"
        }, 
        {
            "location": "/Packaging-and-Registering-a-Component/index.html", 
            "text": "NOTICE:\n This software (or technical data) was produced for the U.S. Government under contract, and is subject to the Rights in Data-General Clause 52.227-14, Alt. IV (DEC 2007). Copyright 2019 The MITRE Corporation. All Rights Reserved.\n\n\n\n\nComponent Descriptor Overview\n\n\nIn order to be registered within OpenMPF, each component must provide a JavaScript Object Notation (JSON) descriptor file which provides contextual information about the component.\n\n\nThis file must be named \"descriptor.json\".\n\n\nFor an example, please see: \nHello World JSON Descriptor\n\n\nComponent Descriptor Data Elements\n\n\nContained within the JSON file should be the following elements:\n\n\ncomponentName\n\n\n\nRequired.\n\n\nContains the component\u2019s name. Should follow CamelCaseFormat.\n\n\nExample:\n\n\"componentName\" : \"SampleComponent\"\n\n\ncomponentVersion\n\n\n\nRequired.\n\n\nContains the component\u2019s version. Does not need to match the \ncomponentAPIVersion\n.\n\n\nExample:\n\n\"componentVersion\" : \"2.0.1\"\n\n\nmiddlewareVersion\n\n\n\nRequired.\n\n\nContains the version of the OpenMPF Component API that the component was built with.\n\n\nExample:\n\n\"middlewareVersion\" :  \"2.0.0\"\n\n\nsourceLanguage\n\n\n\nRequired.\n\n\nContains the language the component is coded in. Should be either \u201cjava\u201d or \u201cc++\u201d.\n\n\nExample:\n\n\"sourceLanguage\" : \"c++\"\n\n\nbatchLibrary\n\n\n\nOptional. At least one of \nbatchLibrary\n or \nstreamLibrary\n must be provided.\n\n\nFor C++ components, this contains the full path to the Component Logic shared object library used for batch processing once the component is deployed.\n\n\nFor Java components, this contains the name of the jar which contains the component implementation used for batch processing.\n\n\nFor setuptools-based Python components, this contains the component's distribution name, which is declared in the \ncomponent's \nsetup.py\n file. The distribution name is usually the same name as the component.\n\n\nFor basic Python components, this contains the full path to the Python file containing the component class.\n\n\nExample (C++):\n\n\"batchLibrary\" : \"${MPF_HOME}/plugins/SampleComponent/lib/libbatchSampleComponent.so\n\n\nExample (Java):\n\n\"batchLibrary\" : \"batch-sample-component-2.0.1.jar\"\n\n\nExample (setuptools-based Python):\n\n\"batchLibrary\" : \"SampleComponent\"\n\n\nExample (basic Python):\n\n\"batchLibrary\" : \"${MPF_HOME}/plugins/SampleComponent/sample_component.py\"\n\n\nstreamLibrary\n\n\n\nOptional. At least one of \nbatchLibrary\n or \nstreamLibrary\n must be provided.\n\n\nFor C++ components, this contains the full path to the Component Logic shared object library used for stream processing once the component is deployed.\n\n\nNote that Java components currently do not support stream processing, so this field should be omitted from Java component descriptor files.\n\n\n\n\n\nExample (C++):\n\n\"streamLibrary\" : \"${MPF_HOME}/plugins/SampleComponent/lib/libstreamSampleComponent.so\n\n\n\n\n\nenvironmentVariables\n\n\n\nRequired; can be empty.\n\n\nDefines a collection of environment variables that will be set when executing the OpenMPF Component Executable.\n\n\nContains the following sub-fields:\n\n\n\n\n\n\nname:\n\n  Name of the environment variable.\n\n\n\n\n\n\nvalue:\n\n  Value of the environment variable.\n  Note that value can be a list of values separated by \u201c:\u201d.\n\n\n\n\n\n\nsep:\n\n  The \nsep\n field (short for \u201cseparator\u201d) should be set to \u201cnull\u201d or \u201c:\u201d. When set to \u201cnull,\u201d the content of the environment variable specified by \nname\n is the content of \nvalue\n; for an existing variable, its former value will be replaced, otherwise, a new variable will be created and assigned this value. When set to \u201c:\u201d any prior value of the environment variable is retained and the content of \nvalue\n is simply appended to the end after a \u201c:\u201d character.\n\n\n\n\n\n\n\n\nIMPORTANT\n: For C++ components, the LD_LIBRARY_PATH needs to be set in order for the Component Executable to load the component\u2019s shared object library as well as any dependent libraries installed with the component. The usual form of the LD_LIBRARY_PATH variable should be \n${MPF_HOME}/plugins/\ncomponentName\n/lib/\n. Additional directories can be appended after a \u201c:\u201d delimiter.\n\n\n\n\nExample:\n\n\nenvironmentVariables\n: [\n    {\n      \nname\n: \nLD_LIBRARY_PATH\n,\n      \nvalue\n: \n${MPF_HOME}/plugins/SampleComponent/lib\n,\n      \nsep\n: \n:\n\n    }\n  ]\n\n\n\n\nalgorithm\n\n\n\nRequired.\n\n\nSpecifies information about the component\u2019s algorithm.\n\n\nContains the following sub-fields:\n\n\n\n\n\n\nname:\n\n  Required. Contains the algorithm\u2019s name. Should be unique and all CAPS.\n\n\n\n\n\n\ndescription:\n\n  Required. Contains a brief description of the algorithm.\n\n\n\n\n\n\ndetectionType:\n\n  Required. Defines the type of entity associated with the algorithm. For example: \nCLASS\n, \nFACE\n, \nMOTION\n, \nPERSON\n, \nSPEECH\n, or \nTEXT\n.\n\n\n\n\n\n\nactionType:\n\n  Required. Defines the type of processing that the algorithm performs. Must be set to \nDETECTION\n.\n\n\n\n\n\n\nrequiresCollection:\n\n  Required, can be empty. Contains the state(s) that must be produced by previous algorithms in the pipeline.\n  \nThis value should be empty \nunless\n the component depends on the results of another algorithm.\n\n\n\n\n\n\nprovidesCollection:\n\n  Contains the following sub-fields:\n\n\n\n\nstates:\n Required. Contains the state(s) that the algorithm provides.\n  Should contain the following values:\n\n\nDETECTION\n\n\nDETECTION_TYPE\n, where \nTYPE\n is the \nalgorithm.detectionType\n\n\nDETECTION_TYPE_ALGORITHM\n, where \nTYPE\n is the value of \nalgorithm.detectionType\n and \nALGORITHM\n is the value of \nalgorithm.name\n\nExample:\n\n\n\"states\": [\n  \"DETECTION\",\n  \"DETECTION_FACE\",\n  \"DETECTION_FACE_SAMPLECOMPONENT\"]\n\n\n\n\n\n\n\n\nproperties:\n\nRequired; can be empty. Declares a list of the configurable properties that the algorithm exposes.\nContains the following sub-fields:\n\n\nname:\n\n  Required.\n\n\ntype:\n\n  Required.\n  \nBOOLEAN\n, \nFLOAT\n, \nDOUBLE\n, \nINT\n, \nLONG\n, or \nSTRING\n.\n\n\ndefaultValue:\n\n  Required.\n  Must be provided in order to create a default action associated with the algorithm, where an action is a specific instance of an algorithm configured with a set of property values.\n\n\ndescription:\n\n  Required.\n  Description of the property. By convention, the default value for a property should be described in its description text.\n\n\n\n\n\n\n\n\n\n\n\n\nactions\n\n\n\nOptional.\n\n\nActions are used in the development of pipelines. Provides a list of custom actions that will be added during component registration.\n\n\n\n\nNOTE:\n For convenience, a default action will be created upon component registration if this element is not provided in the descriptor file.\n\n\n\n\nContains the following sub-fields:\n\n\n\n\n\n\nname:\n\n  Required. Contains the action\u2019s name. Must be unique among all actions, including those that already exist on the system and those specified in this descriptor.\n\n\n\n\n\n\ndescription:\n\n  Required. Contains a brief description of the action.\n\n\n\n\n\n\nalgorithm:\n\n  Required. Contains the name of the algorithm for this action. The algorithm must either already exist on the system or be defined in this descriptor.\n\n\n\n\n\n\nproperties:\n\n  Optional. List of properties that will be passed to the algorithm. Each property has an associated name and value sub-field, which are both required. Name must be one of the properties specified in the algorithm definition for this action.\n\n\n\n\n\n\nExample:\n\n\nactions\n: [\n  {\n    \nname\n: \nSAMPLE COMPONENT FACE DETECTION ACTION\n,\n    \ndescription\n: \nExecutes the sample component face detection algorithm using the default parameters.\n,\n    \nalgorithm\n: \nSAMPLECOMPONENT\n,\n    \nproperties\n: []\n  }\n]\n\n\n\n\ntasks\n\n\n\nOptional.\n\n\nA list of custom tasks that will be added during component registration.\n\n\n\n\nNOTE:\n For convenience, a default task will be created upon component registration if this element is not provided in the descriptor file.\n\n\n\n\nContains the following sub-fields:\n\n\n\n\n\n\nname:\n\n  Required. Contains the task's name. Must be unique among all tasks, including those that already exist on the system and those specified in this descriptor.\n\n\n\n\n\n\ndescription:\n\n  Required. Contains a brief description of the task.\n\n\n\n\n\n\nactions:\n\n  Required. Minimum length is 1. Contains the names of the actions that this task uses. Actions must either already exist on the system or be defined in this descriptor.\n\n\n\n\n\n\nExample:\n\n\ntasks\n: [\n  {\n    \nname\n: \nSAMPLE COMPONENT FACE DETECTION TASK\n,\n    \ndescription\n: \nPerforms sample component face detection.\n,\n    \nactions\n: [\n      \nSAMPLE COMPONENT FACE DETECTION ACTION\n\n    ]\n  }\n]\n\n\n\n\npipelines\n\n\n\nOptional.\n\n\nA list of custom pipelines that will be added during component registration.\n\n\n\n\nNOTE:\n For convenience, a default pipeline will be created upon component registration if this element is not provided in the descriptor file.\n\n\n\n\nContains the following sub-fields:\n\n\n\n\n\n\nname:\n\n  Required. Contains the pipeline's name. Must be unique among all pipelines, including those that already exist on the system and those specified in this descriptor.\n\n\n\n\n\n\ndescription:\n\n  Required. Contains a brief description of the pipeline.\n\n\n\n\n\n\ntasks:\n\n  Required. Minimum length is 1. Contains the names of the tasks that this pipeline uses. Tasks must either already exist on the system or be defined in this descriptor.\n\n\n\n\n\n\nExample:\n\n\npipelines\n: [\n  {\n    \nname\n: \nSAMPLE COMPONENT FACE DETECTION PIPELINE\n,\n    \ndescription\n: \nPerforms sample component face detection.\n,\n    \ntasks\n: [\n      \nSAMPLE COMPONENT FACE DETECTION TASK\n\n    ]\n  }\n]\n\n\n\n\nPackaging a Component\n\n\nOnce the descriptor file is complete, the next step is to compile your component source code, and finally, create a .tar.gz package containing the descriptor file, component library, and all other necessary files.\n\n\nThe package should contain a top-level directory with a unique name that will (hopefully) not conflict with existing component packages that have already been developed. The top-level directory name should be the same as the \ncomponentName\n.\n\n\nWithin the top-level directory there must be a directory named \u201cdescriptor\u201d with the descriptor JSON file in it. The name of the file must be \u201cdescriptor.json\u201d.\n\n\nExample:\n\n\n//sample-component-1.0.0-tar.gz contents\nSampleComponent/\n  config/\n  descriptor/\n    descriptor.json\n  lib/\n\n\n\n\nInstalling and registering a component\n\n\nThe Component Registration web page, located in the Admin section of the OpenMPF web user interface, can be used to upload and register the component.\n\n\nDrag and drop the .tar.gz file containing the component onto the dropzone area of that page. The component will automatically be uploaded and registered.\n\n\nUpon successful registration, the component will be available for deployment onto OpenMPF nodes via the Node Configuration web page and \n/rest/nodes/config\n end point.\n\n\nIf the descriptor contains custom actions, tasks, or pipelines, then they will be automatically added to the system upon registration.\n\n\n\n\nNOTE:\n If the descriptor does not contain custom actions, tasks, or pipelines, then a default action, task, and pipeline will be generated and added to the system.\n\n\nThe default action will use the component\u2019s algorithm with its default property value settings.\nThe default task will use the default action.\nThe default pipeline will use the default task. This will only be generated if the algorithm does not specify any \nrequiresCollection\n states.\n\n\n\n\nUnregistering a component\n\n\nA component can be unregistered by using the remove button on the Component Registration page.\n\n\nDuring unregistration, all services, algorithms, actions, tasks, and pipelines associated with the component are deleted. Additionally, all actions, tasks, and pipelines that depend on these elements are removed.", 
            "title": "Packaging and Registering a Component"
        }, 
        {
            "location": "/Packaging-and-Registering-a-Component/index.html#component-descriptor-overview", 
            "text": "In order to be registered within OpenMPF, each component must provide a JavaScript Object Notation (JSON) descriptor file which provides contextual information about the component.  This file must be named \"descriptor.json\".  For an example, please see:  Hello World JSON Descriptor", 
            "title": "Component Descriptor Overview"
        }, 
        {
            "location": "/Packaging-and-Registering-a-Component/index.html#component-descriptor-data-elements", 
            "text": "Contained within the JSON file should be the following elements:", 
            "title": "Component Descriptor Data Elements"
        }, 
        {
            "location": "/Packaging-and-Registering-a-Component/index.html#packaging-a-component", 
            "text": "Once the descriptor file is complete, the next step is to compile your component source code, and finally, create a .tar.gz package containing the descriptor file, component library, and all other necessary files.  The package should contain a top-level directory with a unique name that will (hopefully) not conflict with existing component packages that have already been developed. The top-level directory name should be the same as the  componentName .  Within the top-level directory there must be a directory named \u201cdescriptor\u201d with the descriptor JSON file in it. The name of the file must be \u201cdescriptor.json\u201d.  Example:  //sample-component-1.0.0-tar.gz contents\nSampleComponent/\n  config/\n  descriptor/\n    descriptor.json\n  lib/", 
            "title": "Packaging a Component"
        }, 
        {
            "location": "/Packaging-and-Registering-a-Component/index.html#installing-and-registering-a-component", 
            "text": "The Component Registration web page, located in the Admin section of the OpenMPF web user interface, can be used to upload and register the component.  Drag and drop the .tar.gz file containing the component onto the dropzone area of that page. The component will automatically be uploaded and registered.  Upon successful registration, the component will be available for deployment onto OpenMPF nodes via the Node Configuration web page and  /rest/nodes/config  end point.  If the descriptor contains custom actions, tasks, or pipelines, then they will be automatically added to the system upon registration.   NOTE:  If the descriptor does not contain custom actions, tasks, or pipelines, then a default action, task, and pipeline will be generated and added to the system.  The default action will use the component\u2019s algorithm with its default property value settings.\nThe default task will use the default action.\nThe default pipeline will use the default task. This will only be generated if the algorithm does not specify any  requiresCollection  states.", 
            "title": "Installing and registering a component"
        }, 
        {
            "location": "/Packaging-and-Registering-a-Component/index.html#unregistering-a-component", 
            "text": "A component can be unregistered by using the remove button on the Component Registration page.  During unregistration, all services, algorithms, actions, tasks, and pipelines associated with the component are deleted. Additionally, all actions, tasks, and pipelines that depend on these elements are removed.", 
            "title": "Unregistering a component"
        }, 
        {
            "location": "/Feed-Forward-Guide/index.html", 
            "text": "NOTICE:\n This software (or technical data) was produced for the U.S. Government under contract, and is subject to the Rights in Data-General Clause 52.227-14, Alt. IV (DEC 2007). Copyright 2019 The MITRE Corporation. All Rights Reserved.\n\n\n\n\nIntroduction\n\n\nFeed forward is an optional behavior of OpenMPF that allows tracks from one detection stage of the pipeline to be directly \u201cfed into\u201d the next stage. It differs from the default segmenting behavior in the following major ways:\n\n\n\n\n\n\nThe next stage will only look at the frames that had detections in the previous stage. The default segmenting behavior results in \u201cfilling the gaps\u201d so that the next stage looks at all the frames between the start and end frames of the feed forward track, regardless of whether a detection was actually found in those frames.\n\n\n\n\n\n\nThe next stage can be configured to only look at the detection regions for the frames in the feed forward track. The default segmenting behavior does not pass the detection region information to the next stage, so the next stage looks at the whole frame region for every frame in the segment.\n\n\n\n\n\n\nThe next stage will process one sub-job per track generated in the previous stage. If the previous stage generated more than one track in a frame, say 3 tracks, then the next stage will process that frame a total of 3 times. Feed forward can be configured such that only the detection regions for those tracks are processed. If they are non-overlapping then there is no duplication of work. The default segmenting behavior will result in one sub-job that captures the frame associated with all 3 tracks.\n\n\n\n\n\n\nMotivation\n\n\nConsider using feed forward for the following reasons:\n\n\n\n\n\n\nYou have an algorithm that isn\u2019t capable of breaking down a frame into regions of interest. For example, face detection can take a whole frame and generate a separate detection region for each face in the frame. On the other hand, performing classification with the OpenCV Deep Neural Network (DNN) component will take that whole frame and generate a single detection that\u2019s the size of the frame\u2019s width and height. The OpenCV DNN component will produce better results if it operates on smaller regions that only capture the desired object to be classified. Using feed forward, you can create a pipeline so that OpenCV DNN component only processes regions with motion in them.\n\n\n\n\n\n\nYou wish to reduce processing time by creating a pipeline in which algorithms are chained from fastest to slowest. For example, a pipeline that starts with motion detection will only feed regions with motion to the next stage, which may be a compute-intensive face detection algorithm. Reducing the amount of data that algorithm needs to process will speed up run times.\n\n\n\n\n\n\n\n\nNOTE:\n Enabling feed forward results in more sub-jobs and more message passing between the workflow manager and components than the default segmenting behavior. Generally speaking, the more feed forward tracks, the greater the overhead cost. The cost may be outweighed by how feed forward can \u201cfilter out\u201d pixel data that doesn\u2019t need to be processed. Often, the greater the media resolution, the more pixel data is filtered out, and the greater the benefit.\n\n\n\n\nThe output of a feed forward pipeline is the intersection of each stage's output. For example, running a feed forward pipeline that contains a motion detector and a face detector will ultimately output detections where motion was detected in the first stage and a face was detected in the second stage.\n\n\nFirst Stage and Combining Properties\n\n\nWhen feed forward is enabled on a job, there is no change in behavior for the first stage of the pipeline because there is no track to feed in. In other words, the first stage will process the media file as though feed forward was not enabled. The tracks generated by the first stage will be passed to the second stage which will then be able to take advantage of the feed forward behavior.\n\n\n\n\nNOTE:\n When \nFEED_FORWARD_TYPE\n is set to anything other than \nNONE\n, the following properties will be ignored: \nFRAME_INTERVAL\n, \nUSE_KEY_FRAMES\n, \nSEARCH_REGION_*\n.\n\n\n\n\nIf you wish to use the above properties, then you can configure them for the first stage of the pipeline, making sure that \nFEED_FORWARD_TYPE\n is set to \nNONE\n, or not specified, for the first stage. You can then configure each subsequent stage to use feed forward. Because only the frames with detections, and those detection regions, are passed forward from the first stage, the subsequent stages will inherit the effects of those properties set on the first stage.  \n\n\nFeed Forward Properties\n\n\nComponents that support feed forward have two algorithm properties that control the feed forward behavior: \nFEED_FORWARD_TYPE\n and \nFEED_FORWARD_TOP_CONFIDENCE_COUNT\n.\n\n\nFEED_FORWARD_TYPE\n can be set to the following values:\n\n\n\n\nNONE\n: Feed forward is disabled (default setting).\n\n\nFRAME\n: For each detection in the feed forward track, search the entire frame associated with that detection. The track's  detection regions are ignored.\n\n\nSUPERSET_REGION\n: Using the feed forward track, generate a superset region (minimum area rectangle) that captures all of the detection regions in that track across all of the frames in that track. Refer to the \nSuperset Region\n section for more details. For each detection in the feed forward track, search the superset region.\n\n\nREGION\n: For each detection in the feed forward track, search the exact detection region.\n\n\n\n\n\n\nNOTE:\n When using \nREGION\n, the location of the region within the frame, and the size of the region, may be different for each detection in the feed forward track. Thus, \nREGION\n should not be used by algorithms that perform region tracking and require a consistent coordinate space from detection to detection. For those algorithms, use \nSUPERSET_REGION\n instead. That will ensure that each detection region is relative to the upper right corner of the superset region for that track.\n\n\n\n\nFEED_FORWARD_TOP_CONFIDENCE_COUNT\n allows you to drop low confidence detections from feed forward tracks. Setting the property to a value less than or equal to 0 has no effect. In that case all detections in the feed forward track will be processed.\n\n\nWhen \nFEED_FORWARD_TOP_CONFIDENCE_COUNT\n is set to a number greater than 0, say 5, then the top 5 detections in the feed forward track (based on highest confidence) will be processed. If the track contains less than 5 detections then all of the detections in the track will be processed. If one or more detections have the same confidence value, then the detection(s) with the lower frame index take precedence.\n\n\nSuperset Region\n\n\nA \u201csuperset region\u201d is the smallest region of interest that contains all of the detections for all of the frames in a track. This is also known as a \u201cunion\u201d or \n\u201cminimum bounding rectangle\"\n.\n\n\n\n\nFor example, consider a track representing a person moving from the upper left to the lower right. The track consists of 3 frames that have the following detection regions:\n\n\n\n\nFrame 0: \n(x = 10, y = 10, width = 10, height = 10)\n\n\nFrame 1: \n(x = 15, y = 15, width = 10, height = 10)\n\n\nFrame 2: \n(x = 20, y = 20, width = 10, height = 10)\n\n\n\n\nEach detection region is drawn with a solid green line in the above diagram. The blue line represents the full frame region. The superset region for the track is \n(x = 10, y = 10, width = 20, height = 20)\n, and is drawn with a dotted red line.\n\n\nThe major advantage of using a superset region is constant size. Some algorithms require the search space in each frame to be a constant size in order to successfully track objects.\n\n\nA disadvantage is that the superset region will often be larger than any specific detection region, so the search space is not restricted to the smallest possible size in each frame; however, in many cases the search space will be significantly smaller than the whole frame.\n\n\nIn the worst case, a feed forward track might, for example, capture a person moving from the upper left corner of a video to the lower right corner. In that case the superset region will be the entire width and height of the frame, so \nSUPERSET_REGION\n devolves into \nFRAME\n.\n\n\nIn a more typical case, a feed forward track might capture a person moving in the upper left quadrant of a video. In that case \nSUPERSET_REGION\n is able to filter out 75% of the rest of the frame data. In the example shown in the above diagram, \nSUPERSET_REGION\n is able to filter out 83% of the rest of the frame data.\n\n\n\n  \n\n    \n\n    \nYour browser does not support the embedded video tag.\n\n    \nClick here to download the video.\n\n  \n\n\n\n\n\nThe above video shows three faces. For each face there is an inner bounding box that moves and an outer bounding box that does not. The inner bounding box represents the face detection in that frame, while the outer bounding box represents the superset region for the track associated with that face. Note that the bounding box for each face uses a different color. The colors are not related to those used in the above diagram.\n\n\nMPFVideoCapture and MPFImageReader Tools\n\n\nThe \nC++ Batch Component API\n and \nPython Batch Component API\n include utilities that make it easier to support feed forward in your components. They work similarly, but only the C++ tools will be discussed here. The \nMPFVideoCapture\n class is a wrapper around OpenCV's \ncv::VideoCapture\n class. \nMPFVideoCapture\n works very similarly to \ncv::VideoCapture\n, except that it might modify the video frames based on job properties. From the point of view of someone using \nMPFVideoCapture\n, these modifications are mostly transparent. \nMPFVideoCapture\n makes it look like you are reading the original video file.\n\n\nConceptually, consider generating a new video from a feed forward track. The new video would have fewer frames (unless there was a detection in every frame) and possibly a smaller frame size.\n\n\nFor example, the original video file might be 30 frames long with 640x480 resolution. If the feed forward track found detections in frames 4, 7, and 10, then \nMPFVideoCapture\n will make it look like the video only has those 3 frames. If the feed forward type is \nSUPERSET_REGION\n or \nREGION,\n and each detection is 30x50 pixels, then \nMPFVideoCapture\n will make it look like the video's original resolution was 30x50 pixels.\n\n\nOne issue with this approach is that the detection frame numbers and bounding box will be relative to the modified video, not the original. To make the detections relative to the original video the \nMPFVideoCapture::ReverseTransform(MPFVideoTrack \nvideoTrack)\n function must be used.\n\n\nThe general pattern for using \nMPFVideoCapture\n is as follows:\n\n\nMPFDetectionError OcvDnnDetection::GetDetections(\n    const MPFVideoJob \njob, std::vector\nMPFVideoTrack\n \ntracks) {\n\n    MPFVideoCapture video_cap(job);\n\n    cv::Mat frame;\n    while (video_cap.Read(frame)) {\n        // Process frames and detections to tracks vector\n    }\n\n    for (MPFVideoTrack \ntrack : tracks) {\n        video_cap.ReverseTransform(track);\n    }\n\n    return MPF_DETECTION_SUCCESS;\n}\n\n\n\n\nMPFVideoCapture\n makes it look like the user is processing the original video, when in reality they are processing a modified version. To avoid confusion, this means that \nMPFVideoCapture\n should always be returning frames that are the same size because most users expect each frame of a video to be the same size.\n\n\nWhen using \nSUPERSET_REGION\n this is not an issue, since one bounding box is used for the entire track. However, when using \nREGION\n, each detection can be a different size, so it is not possible for \nMPFVideoCapture\n to return frames that are always the same size. Since this is a deviation from the expected behavior, and breaks the transparency of \nMPFVideoCapture\n, \nSUPERSET_REGION\n should usually be preferred over \nREGION\n. The \nREGION\n setting should only be used with components that explicitly state they support it (e.g. OcvDnnDetection). Those components may not perform region tracking, so processing frames of various sizes is not a problem.\n\n\nThe \nMPFImageReader\n class is similar to \nMPFVideoCapture\n, but it works on images instead of videos. \nMPFImageReader\n makes it look like the user is processing an original image, when in reality they are processing a modified version where the frame region is generated based on a detection (\nMPFImageLocation\n) fed forward from the previous stage of a pipeline. Note that \nSUPERSET_REGION\n and \nREGION\n have the same effect when working with images. \nMPFImageReader\n also has a reverse transform function.\n\n\nOpenCV DNN Component Tracking\n\n\nThe OpenCV DNN component does not generate detection regions of its own when performing classification. Its tracking behavior depends on whether feed forward is enabled or not. When feed forward is disabled, the component will process the entire region of each frame of a video. If one or more consecutive frames has the same highest confidence classification, then a new track is generated that contains those frames.\n\n\nWhen feed forward is enabled, the OpenCV DNN component will process the region of each frame of feed forward track according to the \nFEED_FORWARD_TYPE\n. It will generate one track that contains the same frames as the feed forward track. If \nFEED_FORWARD_TYPE\n is set to \nREGION\n then the OpenCV DNN track will contain (inherit) the same detection regions as the feed forward track. In any case, the \ndetectionProperties\n map for the detections in the OpenCV DNN track will include the \nCLASSIFICATION\n entries and possibly other OpenCV DNN component properties.\n\n\nFeed Forward Pipeline Examples\n\n\nGoogLeNet Classification with MOG Motion Detection and Feed Forward Region\n\n\nFirst, create the following action:\n\n\nCAFFE GOOGLENET DETECTION (WITH FEED FORWARD REGION) ACTION\n+ Algorithm: DNNCV\n+ MODEL_NAME: googlenet\n+ SUBTRACT_BLUE_VALUE: 104.0\n+ SUBTRACT_GREEN_VALUE: 117.0\n+ SUBTRACT_RED_VALUE: 123.0\n+ FEED_FORWARD_TYPE: REGION\n\n\n\n\nThen create the following task:\n\n\nCAFFE GOOGLENET DETECTION (WITH FEED FORWARD REGION) TASK\n+ CAFFE GOOGLENET DETECTION (WITH FEED FORWARD REGION) ACTION\n\n\n\n\nThen create the following pipeline:\n\n\nCAFFE GOOGLENET DETECTION (WITH MOG MOTION TRACKING AND FEED FORWARD REGION) PIPELINE\n+ MOG MOTION DETECTION (WITH TRACKING) TASK\n+ CAFFE GOOGLENET DETECTION (WITH FEED FORWARD REGION) TASK\n\n\n\n\nRunning this pipeline will result in OpenCV DNN tracks that contain detections where there was MOG motion. Each detection in each track will have an OpenCV DNN \nCLASSIFICATION\n entry. Each track has a 1-to-1 correspondence with a MOG motion track.\n\n\nRefer to \nrunMogThenCaffeFeedForwardExactRegionTest()\n in the \nTestSystemOnDiff\n class for a system test that demonstrates this behavior. Refer to \nrunMogThenCaffeFeedForwardSupersetRegionTest()\n in that class for a system test that uses \nSUPERSET_REGION\n instead. Refer to \nrunMogThenCaffeFeedForwardFullFrameTest()\n for a system test that uses \nFRAME\n instead.\n\n\n\n\nNOTE:\n Short and/or spurious MOG motion tracks will result in more overhead work when performing feed forward. To mitigate this, consider setting the \nMERGE_TRACKS\n, \nMIN_GAP_BETWEEN_TRACKS\n, and \nMIN_TRACK_LENGTH\n properties to generate longer motion tracks and discard short and/or spurious motion tracks.\n\n\nNOTE:\n It doesn\u2019t make sense to use \nFEED_FORWARD_TOP_CONFIDENCE_COUNT\n on a pipeline stage that follows a MOG or SuBSENSE motion detection stage. That\u2019s because those motion detectors don\u2019t generate tracks with confidence values. Instead, \nFEED_FORWARD_TOP_CONFIDENCE_COUNT\n could potentially be used when feeding person tracks into a face detector, for example, if those person tracks have confidence values.\n\n\n\n\nOCV Face Detection with MOG Motion Detection and Feed Forward Superset Region\n\n\nFirst, create the following action:\n\n\nOCV FACE DETECTION (WITH FEED FORWARD SUPERSET REGION) ACTION\n+ Algorithm: FACECV\n+ FEED_FORWARD_TYPE: SUPERSET_REGION\n\n\n\n\nThen create the following task:\n\n\nOCV FACE DETECTION (WITH FEED FORWARD SUPERSET REGION) TASK\n+ OCV FACE DETECTION (WITH FEED FORWARD SUPERSET REGION) ACTION\n\n\n\n\nThen create the following pipeline:\n\n\nOCV FACE DETECTION (WITH MOG MOTION TRACKING AND FEED FORWARD SUPERSET REGION) PIPELINE\n+ MOG MOTION DETECTION (WITH TRACKING) TASK\n+ OCV FACE DETECTION (WITH FEED FORWARD SUPERSET REGION) TASK\n\n\n\n\nRunning this pipeline will result in OCV face tracks that contain detections where there was MOG motion. Each track has a 1-to-1 correspondence with a MOG motion track.\n\n\nRefer to \nrunMogThenOcvFaceFeedForwardRegionTest()\n in the \nTestSystemOnDiff\n class for a system test that demonstrates this behavior.", 
            "title": "Feed Forward Guide"
        }, 
        {
            "location": "/Feed-Forward-Guide/index.html#introduction", 
            "text": "Feed forward is an optional behavior of OpenMPF that allows tracks from one detection stage of the pipeline to be directly \u201cfed into\u201d the next stage. It differs from the default segmenting behavior in the following major ways:    The next stage will only look at the frames that had detections in the previous stage. The default segmenting behavior results in \u201cfilling the gaps\u201d so that the next stage looks at all the frames between the start and end frames of the feed forward track, regardless of whether a detection was actually found in those frames.    The next stage can be configured to only look at the detection regions for the frames in the feed forward track. The default segmenting behavior does not pass the detection region information to the next stage, so the next stage looks at the whole frame region for every frame in the segment.    The next stage will process one sub-job per track generated in the previous stage. If the previous stage generated more than one track in a frame, say 3 tracks, then the next stage will process that frame a total of 3 times. Feed forward can be configured such that only the detection regions for those tracks are processed. If they are non-overlapping then there is no duplication of work. The default segmenting behavior will result in one sub-job that captures the frame associated with all 3 tracks.", 
            "title": "Introduction"
        }, 
        {
            "location": "/Feed-Forward-Guide/index.html#motivation", 
            "text": "Consider using feed forward for the following reasons:    You have an algorithm that isn\u2019t capable of breaking down a frame into regions of interest. For example, face detection can take a whole frame and generate a separate detection region for each face in the frame. On the other hand, performing classification with the OpenCV Deep Neural Network (DNN) component will take that whole frame and generate a single detection that\u2019s the size of the frame\u2019s width and height. The OpenCV DNN component will produce better results if it operates on smaller regions that only capture the desired object to be classified. Using feed forward, you can create a pipeline so that OpenCV DNN component only processes regions with motion in them.    You wish to reduce processing time by creating a pipeline in which algorithms are chained from fastest to slowest. For example, a pipeline that starts with motion detection will only feed regions with motion to the next stage, which may be a compute-intensive face detection algorithm. Reducing the amount of data that algorithm needs to process will speed up run times.     NOTE:  Enabling feed forward results in more sub-jobs and more message passing between the workflow manager and components than the default segmenting behavior. Generally speaking, the more feed forward tracks, the greater the overhead cost. The cost may be outweighed by how feed forward can \u201cfilter out\u201d pixel data that doesn\u2019t need to be processed. Often, the greater the media resolution, the more pixel data is filtered out, and the greater the benefit.   The output of a feed forward pipeline is the intersection of each stage's output. For example, running a feed forward pipeline that contains a motion detector and a face detector will ultimately output detections where motion was detected in the first stage and a face was detected in the second stage.", 
            "title": "Motivation"
        }, 
        {
            "location": "/Feed-Forward-Guide/index.html#first-stage-and-combining-properties", 
            "text": "When feed forward is enabled on a job, there is no change in behavior for the first stage of the pipeline because there is no track to feed in. In other words, the first stage will process the media file as though feed forward was not enabled. The tracks generated by the first stage will be passed to the second stage which will then be able to take advantage of the feed forward behavior.   NOTE:  When  FEED_FORWARD_TYPE  is set to anything other than  NONE , the following properties will be ignored:  FRAME_INTERVAL ,  USE_KEY_FRAMES ,  SEARCH_REGION_* .   If you wish to use the above properties, then you can configure them for the first stage of the pipeline, making sure that  FEED_FORWARD_TYPE  is set to  NONE , or not specified, for the first stage. You can then configure each subsequent stage to use feed forward. Because only the frames with detections, and those detection regions, are passed forward from the first stage, the subsequent stages will inherit the effects of those properties set on the first stage.", 
            "title": "First Stage and Combining Properties"
        }, 
        {
            "location": "/Feed-Forward-Guide/index.html#feed-forward-properties", 
            "text": "Components that support feed forward have two algorithm properties that control the feed forward behavior:  FEED_FORWARD_TYPE  and  FEED_FORWARD_TOP_CONFIDENCE_COUNT .  FEED_FORWARD_TYPE  can be set to the following values:   NONE : Feed forward is disabled (default setting).  FRAME : For each detection in the feed forward track, search the entire frame associated with that detection. The track's  detection regions are ignored.  SUPERSET_REGION : Using the feed forward track, generate a superset region (minimum area rectangle) that captures all of the detection regions in that track across all of the frames in that track. Refer to the  Superset Region  section for more details. For each detection in the feed forward track, search the superset region.  REGION : For each detection in the feed forward track, search the exact detection region.    NOTE:  When using  REGION , the location of the region within the frame, and the size of the region, may be different for each detection in the feed forward track. Thus,  REGION  should not be used by algorithms that perform region tracking and require a consistent coordinate space from detection to detection. For those algorithms, use  SUPERSET_REGION  instead. That will ensure that each detection region is relative to the upper right corner of the superset region for that track.   FEED_FORWARD_TOP_CONFIDENCE_COUNT  allows you to drop low confidence detections from feed forward tracks. Setting the property to a value less than or equal to 0 has no effect. In that case all detections in the feed forward track will be processed.  When  FEED_FORWARD_TOP_CONFIDENCE_COUNT  is set to a number greater than 0, say 5, then the top 5 detections in the feed forward track (based on highest confidence) will be processed. If the track contains less than 5 detections then all of the detections in the track will be processed. If one or more detections have the same confidence value, then the detection(s) with the lower frame index take precedence.", 
            "title": "Feed Forward Properties"
        }, 
        {
            "location": "/Feed-Forward-Guide/index.html#superset-region", 
            "text": "A \u201csuperset region\u201d is the smallest region of interest that contains all of the detections for all of the frames in a track. This is also known as a \u201cunion\u201d or  \u201cminimum bounding rectangle\" .   For example, consider a track representing a person moving from the upper left to the lower right. The track consists of 3 frames that have the following detection regions:   Frame 0:  (x = 10, y = 10, width = 10, height = 10)  Frame 1:  (x = 15, y = 15, width = 10, height = 10)  Frame 2:  (x = 20, y = 20, width = 10, height = 10)   Each detection region is drawn with a solid green line in the above diagram. The blue line represents the full frame region. The superset region for the track is  (x = 10, y = 10, width = 20, height = 20) , and is drawn with a dotted red line.  The major advantage of using a superset region is constant size. Some algorithms require the search space in each frame to be a constant size in order to successfully track objects.  A disadvantage is that the superset region will often be larger than any specific detection region, so the search space is not restricted to the smallest possible size in each frame; however, in many cases the search space will be significantly smaller than the whole frame.  In the worst case, a feed forward track might, for example, capture a person moving from the upper left corner of a video to the lower right corner. In that case the superset region will be the entire width and height of the frame, so  SUPERSET_REGION  devolves into  FRAME .  In a more typical case, a feed forward track might capture a person moving in the upper left quadrant of a video. In that case  SUPERSET_REGION  is able to filter out 75% of the rest of the frame data. In the example shown in the above diagram,  SUPERSET_REGION  is able to filter out 83% of the rest of the frame data.  \n   \n     \n     Your browser does not support the embedded video tag. \n     Click here to download the video. \n     The above video shows three faces. For each face there is an inner bounding box that moves and an outer bounding box that does not. The inner bounding box represents the face detection in that frame, while the outer bounding box represents the superset region for the track associated with that face. Note that the bounding box for each face uses a different color. The colors are not related to those used in the above diagram.", 
            "title": "Superset Region"
        }, 
        {
            "location": "/Feed-Forward-Guide/index.html#mpfvideocapture-and-mpfimagereader-tools", 
            "text": "The  C++ Batch Component API  and  Python Batch Component API  include utilities that make it easier to support feed forward in your components. They work similarly, but only the C++ tools will be discussed here. The  MPFVideoCapture  class is a wrapper around OpenCV's  cv::VideoCapture  class.  MPFVideoCapture  works very similarly to  cv::VideoCapture , except that it might modify the video frames based on job properties. From the point of view of someone using  MPFVideoCapture , these modifications are mostly transparent.  MPFVideoCapture  makes it look like you are reading the original video file.  Conceptually, consider generating a new video from a feed forward track. The new video would have fewer frames (unless there was a detection in every frame) and possibly a smaller frame size.  For example, the original video file might be 30 frames long with 640x480 resolution. If the feed forward track found detections in frames 4, 7, and 10, then  MPFVideoCapture  will make it look like the video only has those 3 frames. If the feed forward type is  SUPERSET_REGION  or  REGION,  and each detection is 30x50 pixels, then  MPFVideoCapture  will make it look like the video's original resolution was 30x50 pixels.  One issue with this approach is that the detection frame numbers and bounding box will be relative to the modified video, not the original. To make the detections relative to the original video the  MPFVideoCapture::ReverseTransform(MPFVideoTrack  videoTrack)  function must be used.  The general pattern for using  MPFVideoCapture  is as follows:  MPFDetectionError OcvDnnDetection::GetDetections(\n    const MPFVideoJob  job, std::vector MPFVideoTrack   tracks) {\n\n    MPFVideoCapture video_cap(job);\n\n    cv::Mat frame;\n    while (video_cap.Read(frame)) {\n        // Process frames and detections to tracks vector\n    }\n\n    for (MPFVideoTrack  track : tracks) {\n        video_cap.ReverseTransform(track);\n    }\n\n    return MPF_DETECTION_SUCCESS;\n}  MPFVideoCapture  makes it look like the user is processing the original video, when in reality they are processing a modified version. To avoid confusion, this means that  MPFVideoCapture  should always be returning frames that are the same size because most users expect each frame of a video to be the same size.  When using  SUPERSET_REGION  this is not an issue, since one bounding box is used for the entire track. However, when using  REGION , each detection can be a different size, so it is not possible for  MPFVideoCapture  to return frames that are always the same size. Since this is a deviation from the expected behavior, and breaks the transparency of  MPFVideoCapture ,  SUPERSET_REGION  should usually be preferred over  REGION . The  REGION  setting should only be used with components that explicitly state they support it (e.g. OcvDnnDetection). Those components may not perform region tracking, so processing frames of various sizes is not a problem.  The  MPFImageReader  class is similar to  MPFVideoCapture , but it works on images instead of videos.  MPFImageReader  makes it look like the user is processing an original image, when in reality they are processing a modified version where the frame region is generated based on a detection ( MPFImageLocation ) fed forward from the previous stage of a pipeline. Note that  SUPERSET_REGION  and  REGION  have the same effect when working with images.  MPFImageReader  also has a reverse transform function.", 
            "title": "MPFVideoCapture and MPFImageReader Tools"
        }, 
        {
            "location": "/Feed-Forward-Guide/index.html#opencv-dnn-component-tracking", 
            "text": "The OpenCV DNN component does not generate detection regions of its own when performing classification. Its tracking behavior depends on whether feed forward is enabled or not. When feed forward is disabled, the component will process the entire region of each frame of a video. If one or more consecutive frames has the same highest confidence classification, then a new track is generated that contains those frames.  When feed forward is enabled, the OpenCV DNN component will process the region of each frame of feed forward track according to the  FEED_FORWARD_TYPE . It will generate one track that contains the same frames as the feed forward track. If  FEED_FORWARD_TYPE  is set to  REGION  then the OpenCV DNN track will contain (inherit) the same detection regions as the feed forward track. In any case, the  detectionProperties  map for the detections in the OpenCV DNN track will include the  CLASSIFICATION  entries and possibly other OpenCV DNN component properties.", 
            "title": "OpenCV DNN Component Tracking"
        }, 
        {
            "location": "/Feed-Forward-Guide/index.html#feed-forward-pipeline-examples", 
            "text": "GoogLeNet Classification with MOG Motion Detection and Feed Forward Region  First, create the following action:  CAFFE GOOGLENET DETECTION (WITH FEED FORWARD REGION) ACTION\n+ Algorithm: DNNCV\n+ MODEL_NAME: googlenet\n+ SUBTRACT_BLUE_VALUE: 104.0\n+ SUBTRACT_GREEN_VALUE: 117.0\n+ SUBTRACT_RED_VALUE: 123.0\n+ FEED_FORWARD_TYPE: REGION  Then create the following task:  CAFFE GOOGLENET DETECTION (WITH FEED FORWARD REGION) TASK\n+ CAFFE GOOGLENET DETECTION (WITH FEED FORWARD REGION) ACTION  Then create the following pipeline:  CAFFE GOOGLENET DETECTION (WITH MOG MOTION TRACKING AND FEED FORWARD REGION) PIPELINE\n+ MOG MOTION DETECTION (WITH TRACKING) TASK\n+ CAFFE GOOGLENET DETECTION (WITH FEED FORWARD REGION) TASK  Running this pipeline will result in OpenCV DNN tracks that contain detections where there was MOG motion. Each detection in each track will have an OpenCV DNN  CLASSIFICATION  entry. Each track has a 1-to-1 correspondence with a MOG motion track.  Refer to  runMogThenCaffeFeedForwardExactRegionTest()  in the  TestSystemOnDiff  class for a system test that demonstrates this behavior. Refer to  runMogThenCaffeFeedForwardSupersetRegionTest()  in that class for a system test that uses  SUPERSET_REGION  instead. Refer to  runMogThenCaffeFeedForwardFullFrameTest()  for a system test that uses  FRAME  instead.   NOTE:  Short and/or spurious MOG motion tracks will result in more overhead work when performing feed forward. To mitigate this, consider setting the  MERGE_TRACKS ,  MIN_GAP_BETWEEN_TRACKS , and  MIN_TRACK_LENGTH  properties to generate longer motion tracks and discard short and/or spurious motion tracks.  NOTE:  It doesn\u2019t make sense to use  FEED_FORWARD_TOP_CONFIDENCE_COUNT  on a pipeline stage that follows a MOG or SuBSENSE motion detection stage. That\u2019s because those motion detectors don\u2019t generate tracks with confidence values. Instead,  FEED_FORWARD_TOP_CONFIDENCE_COUNT  could potentially be used when feeding person tracks into a face detector, for example, if those person tracks have confidence values.   OCV Face Detection with MOG Motion Detection and Feed Forward Superset Region  First, create the following action:  OCV FACE DETECTION (WITH FEED FORWARD SUPERSET REGION) ACTION\n+ Algorithm: FACECV\n+ FEED_FORWARD_TYPE: SUPERSET_REGION  Then create the following task:  OCV FACE DETECTION (WITH FEED FORWARD SUPERSET REGION) TASK\n+ OCV FACE DETECTION (WITH FEED FORWARD SUPERSET REGION) ACTION  Then create the following pipeline:  OCV FACE DETECTION (WITH MOG MOTION TRACKING AND FEED FORWARD SUPERSET REGION) PIPELINE\n+ MOG MOTION DETECTION (WITH TRACKING) TASK\n+ OCV FACE DETECTION (WITH FEED FORWARD SUPERSET REGION) TASK  Running this pipeline will result in OCV face tracks that contain detections where there was MOG motion. Each track has a 1-to-1 correspondence with a MOG motion track.  Refer to  runMogThenOcvFaceFeedForwardRegionTest()  in the  TestSystemOnDiff  class for a system test that demonstrates this behavior.", 
            "title": "Feed Forward Pipeline Examples"
        }, 
        {
            "location": "/Workflow-Manager/index.html", 
            "text": "NOTICE:\n This software (or technical data) was produced for the U.S. Government under contract, and is subject to the Rights in Data-General Clause 52.227-14, Alt. IV (DEC 2007). Copyright 2019 The MITRE Corporation. All Rights Reserved.\n\n\nIMPORTANT:\n This document describes the Workflow Manager architecture for batch processing. There is a separate architecture for stream processing that uses many of the same elements and concepts.\n\n\n\n\nWorkflow Manager Overview\n\n\nThe OpenMPF consists of three major pieces:\n\n\n\n\nA collection of \nComponents\n which process media\n\n\nA \nNode Manager\n, which launches and monitors running components in the system\n\n\nThe \nWorkflow Manager\n (WFM), which allows for the creation of jobs and manages the flow through active components\n\n\n\n\nThese pieces are supported by a number of modules which provide shared functionality, as shown in the dependency diagram below:\n\n\n\n\nThere are three general functional areas in the WFM:\n\n\n\n\nThe \nControllers\n are the primary entry point, accepting REST requests which trigger actions by the WFM\n\n\nThe \nWFM Services\n, which handle administrative tasks such as pipeline creation, node management, and log retrieval\n\n\nJob Management\n, which uses Camel routes to pass a job through the levels of processing\n\n\n\n\nThere are two different databases used by the WFM:\n\n\n\n\nA \nSQL database\n stores persistent data about jobs. This data includes:\n\n\nThe job ID\n\n\nThe start and stop time of the job\n\n\nThe exit status of the job\n\n\nJob priority\n\n\nJob input/outputs\n\n\n\n\n\n\nA \nRedis database\n for storing transient data that is only necessary while the job is being run. Some of this data is used to generate the output which will later be persisted in long-term storage, and some is a temporary duplication of previously persisted data in active memory to avoid race conditions. This data includes:\n\n\nJob and media properties\n\n\nDetections\n\n\nTracks\n\n\nPipeline/Track/Action/Algorithm data duplicated from the system at job start time\n\n\nREST Callback information\n\n\n\n\n\n\n\n\nThe diagram below shows the functional areas of the WFM, the databases used by the WFM, and communication with components:\n\n\n\n\nControllers / Services\n\n\nThe controllers are all located  \nhere\n.\n\n\nEvery controller provides a collection of REST endpoints which allow access either to a WFM service or to the job management flow. Only the JobController enters the job management flow.\n\n\nBasic Controllers\n\n\nThe table below lists the basic controllers:\n\n\n\n\n\n\n\n\nController Class\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nAdminLogsController\n\n\nAccesses the log content via REST\n\n\n\n\n\n\nAdminErrorsController\n\n\nGets admin errors\n\n\n\n\n\n\nAdminPropertySettingsController\n\n\nAllows access and modification of system properties\n\n\n\n\n\n\nAdminStatisticsController\n\n\nGenerates job statistics\n\n\n\n\n\n\nAtmosphereController\n\n\nUses Atmosphere to manage server-side push\n\n\n\n\n\n\nBootoutController\n\n\nHandles bootouts when a second session is opened by the same user\n\n\n\n\n\n\nHomeController\n\n\nManages index page and version information\n\n\n\n\n\n\nLoginController\n\n\nManages login/logout and authentication\n\n\n\n\n\n\nServerMediaController\n\n\nEnables selection and deselection of files at a directory level\n\n\n\n\n\n\nSystemMessageController\n\n\nManages system level messages, such as notifying users that a server restart is needed\n\n\n\n\n\n\nTimeoutController\n\n\nManages session timeouts\n\n\n\n\n\n\n\n\nThe following sections list the rest of the constrollers in more detail.\n\n\nAdminComponentRegistrationController\n\n\nComponents in the OpenMPF are uploaded as tar.gz packages containing all necessary component data. For more information on components, read \nOpenMPF Component API Overview\n.\n\n\nThe \nAdminComponentRegistrationController\n provides endpoints which allow:\n\n\n\n\nAccess to current component information\n\n\nUpload of new components\n\n\nRegistration and unregistration of components (note that components must be registered to be included in pipelines)\n\n\nDeletion of components\n\n\n\n\nJobController\n\n\nA job is a specific pipeline's tasks and actions applied to a set of media. The \nJobController\n allows:\n\n\n\n\nAccess to information about jobs in the system\n\n\nCreation of new jobs\n\n\nCancellation of existing jobs\n\n\nDownload of job output data\n\n\nResubmission of jobs (regardless of initial job status)\n\n\n\n\nMarkupController\n\n\nMarkup files are copies of the initial media input to a job with detections visually highlighted in the image or video frames. The \nMarkupController\n can provide lists of available Markup files, or it can download a specific file.\n\n\nMediaController\n\n\nThe \nMediaController\n enables upload and organization of media files within the WFM. It provides endpoints for media upload, and also for creation of folders to organize media files in the system. At this time, there are no endpoints which allow for deletion or reorganization of media files, since all media is shared by all users.\n\n\nNodeController\n\n\nThe OpenMPF uses multiple hosts to enable scalability and parallel processing. The \nNodeController\n provides access to host information and allows components to be deployed on nodes. One or more components can be installed on a node. The same component can be installed on multiple nodes. Each node can manage one or more services for each component.\n\n\nThe \nNodeController\n provides host information and component service deployment status. It also provides an endpoint to deploy a service on a node and an endpoint to stop a service.\n\n\nFor more information on nodes, please read the \nNode Configuration and Status\n section in the Admin Guide.\n\n\nPipelineController\n\n\nThe \nPipeline Controller\n allows for the creation, retrieval, and deletion of pipelines or any of their constituent parts. While actions, tasks, and pipelines may not be directly modified, they may be deleted and recreated.\n\n\nFor more information on pipelines, please read the \nCreate Custom Pipelines\n section in the User Guide.\n\n\nJob Management\n\n\nThe request to create a job begins at the \nJobController\n. From there, it is transformed and passed through multiple flows on its way to the component services. These services process the job then return information to the WFM for JSON output generation.\n\n\nThe diagram below shows the sequence of WFM operations. It does not show the ActiveMQ messages that are sent to and from the component services.\n\n\n\n\nAfter the job request is validated and saved to the SQL database, it passes through multiple Apache Camel routes, each of which checks that the job is still valid (with no fatal errors or cancellations), and then invokes a series of transformations and processors specific to the route.\n\n\nApache Camel\n is an open-source framework that allows developers to build rule-based routing engines. Within the OpenMPF, we use a \nJava DSL\n to define the routes. Every route functions independently, and communication between the routes is URI-based. The OpenMPF uses ActiveMQ to handle its message traffic.\n\n\nJob Creator Route\n\n\nThe \nJob Creator Route\n sets up the job in memory. By the time this route is invoked, the job has been persisted in the permanent SQL database. The Job Creator Route sets up the transient objects in Redis that will be used for aggregating job data across pipeline stages. By the time this route exits, the particulars of the pipeline and job request will be stored in Redis.\n\n\nMedia Retriever Route\n\n\nThe \nMedia Retriever Route\n ensures that the media for the job can all be found and accessed. It stores the media information on the server to ensure continued access.\n\n\nMedia Inspection Route\n\n\nThe \nMedia Inspection Route\n splits a single job with multiple media inputs into separate messages, one for each piece of media. For each piece of media, it collects metadata about the media, including MIME type, duration, frame rate, and orientation data.\n\n\nJob Router Route\n\n\nThe \nJob Router Route\n uses the pipeline's flow to create the messages that are sent to the components. For large media files, it splits the job into smaller sub-jobs by logically breaking the media up into segments. Each segment has a start point and an end point (specified as a frame or time offset).\n\n\nThis route compiles properties for the job, media, and algorithm, and determines the next component that needs to be invoked. It then marshals the job into a serialized protobuf format and sends the message off to the component for processing.\n\n\nUnlike Job Creation, Media Retriever, and Media Inspection, this route may be invoked multiple times as future routes redirect back to the Job Router so that the job can be processed by the next component in the pipeline.\n\n\nDetection Response Route\n\n\nThe \nDetection Response Route\n is the re-entry point to the WFM. It unmarshals the protobuf responses, converts them into the Track and Detection objects used within the WFM, and stores them in the Redis database. Over multiple calls to the DetectionResponseProcessor, all data is eventually added into the transient Job object.\n\n\nStage Response Aggregation Route\n\n\nThe \nStage Response Aggregation Route\n is the exit point for the response processors. It waits until all the sub-job responses have been retrieved for the current stage of the pipeline, then it invokes the Job Router Route to see if any additional processing needs to be done.\n\n\nMarkup Response Route\n\n\nMarkup files are copies of the initial media input to a job with any detections visually highlighted in the image. The \nMarkup Response Route\n persists the locations of these markup files in the SQL database.\n\n\nJob Completed Route\n\n\nOnce the Job is completed, the \nJob Completed Route\n converts the aggregated transient data structure into a JSON output format. It then clears out any lingering transient objects, updates the final job status, and sends the output object to anything that needs to access it.", 
            "title": "Workflow Manager"
        }, 
        {
            "location": "/Workflow-Manager/index.html#workflow-manager-overview", 
            "text": "The OpenMPF consists of three major pieces:   A collection of  Components  which process media  A  Node Manager , which launches and monitors running components in the system  The  Workflow Manager  (WFM), which allows for the creation of jobs and manages the flow through active components   These pieces are supported by a number of modules which provide shared functionality, as shown in the dependency diagram below:   There are three general functional areas in the WFM:   The  Controllers  are the primary entry point, accepting REST requests which trigger actions by the WFM  The  WFM Services , which handle administrative tasks such as pipeline creation, node management, and log retrieval  Job Management , which uses Camel routes to pass a job through the levels of processing   There are two different databases used by the WFM:   A  SQL database  stores persistent data about jobs. This data includes:  The job ID  The start and stop time of the job  The exit status of the job  Job priority  Job input/outputs    A  Redis database  for storing transient data that is only necessary while the job is being run. Some of this data is used to generate the output which will later be persisted in long-term storage, and some is a temporary duplication of previously persisted data in active memory to avoid race conditions. This data includes:  Job and media properties  Detections  Tracks  Pipeline/Track/Action/Algorithm data duplicated from the system at job start time  REST Callback information     The diagram below shows the functional areas of the WFM, the databases used by the WFM, and communication with components:", 
            "title": "Workflow Manager Overview"
        }, 
        {
            "location": "/Workflow-Manager/index.html#controllers-services", 
            "text": "The controllers are all located   here .  Every controller provides a collection of REST endpoints which allow access either to a WFM service or to the job management flow. Only the JobController enters the job management flow.", 
            "title": "Controllers / Services"
        }, 
        {
            "location": "/Workflow-Manager/index.html#basic-controllers", 
            "text": "The table below lists the basic controllers:     Controller Class  Description      AdminLogsController  Accesses the log content via REST    AdminErrorsController  Gets admin errors    AdminPropertySettingsController  Allows access and modification of system properties    AdminStatisticsController  Generates job statistics    AtmosphereController  Uses Atmosphere to manage server-side push    BootoutController  Handles bootouts when a second session is opened by the same user    HomeController  Manages index page and version information    LoginController  Manages login/logout and authentication    ServerMediaController  Enables selection and deselection of files at a directory level    SystemMessageController  Manages system level messages, such as notifying users that a server restart is needed    TimeoutController  Manages session timeouts     The following sections list the rest of the constrollers in more detail.", 
            "title": "Basic Controllers"
        }, 
        {
            "location": "/Workflow-Manager/index.html#admincomponentregistrationcontroller", 
            "text": "Components in the OpenMPF are uploaded as tar.gz packages containing all necessary component data. For more information on components, read  OpenMPF Component API Overview .  The  AdminComponentRegistrationController  provides endpoints which allow:   Access to current component information  Upload of new components  Registration and unregistration of components (note that components must be registered to be included in pipelines)  Deletion of components", 
            "title": "AdminComponentRegistrationController"
        }, 
        {
            "location": "/Workflow-Manager/index.html#jobcontroller", 
            "text": "A job is a specific pipeline's tasks and actions applied to a set of media. The  JobController  allows:   Access to information about jobs in the system  Creation of new jobs  Cancellation of existing jobs  Download of job output data  Resubmission of jobs (regardless of initial job status)", 
            "title": "JobController"
        }, 
        {
            "location": "/Workflow-Manager/index.html#markupcontroller", 
            "text": "Markup files are copies of the initial media input to a job with detections visually highlighted in the image or video frames. The  MarkupController  can provide lists of available Markup files, or it can download a specific file.", 
            "title": "MarkupController"
        }, 
        {
            "location": "/Workflow-Manager/index.html#mediacontroller", 
            "text": "The  MediaController  enables upload and organization of media files within the WFM. It provides endpoints for media upload, and also for creation of folders to organize media files in the system. At this time, there are no endpoints which allow for deletion or reorganization of media files, since all media is shared by all users.", 
            "title": "MediaController"
        }, 
        {
            "location": "/Workflow-Manager/index.html#nodecontroller", 
            "text": "The OpenMPF uses multiple hosts to enable scalability and parallel processing. The  NodeController  provides access to host information and allows components to be deployed on nodes. One or more components can be installed on a node. The same component can be installed on multiple nodes. Each node can manage one or more services for each component.  The  NodeController  provides host information and component service deployment status. It also provides an endpoint to deploy a service on a node and an endpoint to stop a service.  For more information on nodes, please read the  Node Configuration and Status  section in the Admin Guide.", 
            "title": "NodeController"
        }, 
        {
            "location": "/Workflow-Manager/index.html#pipelinecontroller", 
            "text": "The  Pipeline Controller  allows for the creation, retrieval, and deletion of pipelines or any of their constituent parts. While actions, tasks, and pipelines may not be directly modified, they may be deleted and recreated.  For more information on pipelines, please read the  Create Custom Pipelines  section in the User Guide.", 
            "title": "PipelineController"
        }, 
        {
            "location": "/Workflow-Manager/index.html#job-management", 
            "text": "The request to create a job begins at the  JobController . From there, it is transformed and passed through multiple flows on its way to the component services. These services process the job then return information to the WFM for JSON output generation.  The diagram below shows the sequence of WFM operations. It does not show the ActiveMQ messages that are sent to and from the component services.   After the job request is validated and saved to the SQL database, it passes through multiple Apache Camel routes, each of which checks that the job is still valid (with no fatal errors or cancellations), and then invokes a series of transformations and processors specific to the route.  Apache Camel  is an open-source framework that allows developers to build rule-based routing engines. Within the OpenMPF, we use a  Java DSL  to define the routes. Every route functions independently, and communication between the routes is URI-based. The OpenMPF uses ActiveMQ to handle its message traffic.", 
            "title": "Job Management"
        }, 
        {
            "location": "/Workflow-Manager/index.html#job-creator-route", 
            "text": "The  Job Creator Route  sets up the job in memory. By the time this route is invoked, the job has been persisted in the permanent SQL database. The Job Creator Route sets up the transient objects in Redis that will be used for aggregating job data across pipeline stages. By the time this route exits, the particulars of the pipeline and job request will be stored in Redis.", 
            "title": "Job Creator Route"
        }, 
        {
            "location": "/Workflow-Manager/index.html#media-retriever-route", 
            "text": "The  Media Retriever Route  ensures that the media for the job can all be found and accessed. It stores the media information on the server to ensure continued access.", 
            "title": "Media Retriever Route"
        }, 
        {
            "location": "/Workflow-Manager/index.html#media-inspection-route", 
            "text": "The  Media Inspection Route  splits a single job with multiple media inputs into separate messages, one for each piece of media. For each piece of media, it collects metadata about the media, including MIME type, duration, frame rate, and orientation data.", 
            "title": "Media Inspection Route"
        }, 
        {
            "location": "/Workflow-Manager/index.html#job-router-route", 
            "text": "The  Job Router Route  uses the pipeline's flow to create the messages that are sent to the components. For large media files, it splits the job into smaller sub-jobs by logically breaking the media up into segments. Each segment has a start point and an end point (specified as a frame or time offset).  This route compiles properties for the job, media, and algorithm, and determines the next component that needs to be invoked. It then marshals the job into a serialized protobuf format and sends the message off to the component for processing.  Unlike Job Creation, Media Retriever, and Media Inspection, this route may be invoked multiple times as future routes redirect back to the Job Router so that the job can be processed by the next component in the pipeline.", 
            "title": "Job Router Route"
        }, 
        {
            "location": "/Workflow-Manager/index.html#detection-response-route", 
            "text": "The  Detection Response Route  is the re-entry point to the WFM. It unmarshals the protobuf responses, converts them into the Track and Detection objects used within the WFM, and stores them in the Redis database. Over multiple calls to the DetectionResponseProcessor, all data is eventually added into the transient Job object.", 
            "title": "Detection Response Route"
        }, 
        {
            "location": "/Workflow-Manager/index.html#stage-response-aggregation-route", 
            "text": "The  Stage Response Aggregation Route  is the exit point for the response processors. It waits until all the sub-job responses have been retrieved for the current stage of the pipeline, then it invokes the Job Router Route to see if any additional processing needs to be done.", 
            "title": "Stage Response Aggregation Route"
        }, 
        {
            "location": "/Workflow-Manager/index.html#markup-response-route", 
            "text": "Markup files are copies of the initial media input to a job with any detections visually highlighted in the image. The  Markup Response Route  persists the locations of these markup files in the SQL database.", 
            "title": "Markup Response Route"
        }, 
        {
            "location": "/Workflow-Manager/index.html#job-completed-route", 
            "text": "Once the Job is completed, the  Job Completed Route  converts the aggregated transient data structure into a JSON output format. It then clears out any lingering transient objects, updates the final job status, and sends the output object to anything that needs to access it.", 
            "title": "Job Completed Route"
        }, 
        {
            "location": "/REST-API/index.html", 
            "text": "The OpenMPF REST API is provided by Swagger and is available within the OpenMPF Workflow Manager web application. Swagger enables users to test the endpoints using the running instance of OpenMPF.\n\n\nClick \nhere\n for a generated version of the content.", 
            "title": "REST API"
        }
    ]
}